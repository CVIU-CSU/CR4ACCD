2023-02-03 02:14:11,242 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.11 (default, Jul 27 2021, 14:32:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3: GeForce RTX 2080 Ti
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.2, V10.2.89
GCC: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
PyTorch: 1.6.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.7.0
OpenCV: 4.5.4
MMCV: 1.3.17
MMCV Compiler: GCC 5.4
MMCV CUDA Compiler: 10.2
MMDetection: 2.18.0+
------------------------------------------------------------

2023-02-03 02:14:11,981 - mmdet - INFO - Distributed training: True
2023-02-03 02:14:12,761 - mmdet - INFO - Config:
dataset_type = 'CocoDataset'
data_root = '/root/commonfile/data/ComparisonDetectorDataset/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, load_pos_neg=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CocoDataset',
        ann_file='/root/commonfile/data/ComparisonDetectorDataset/train.json',
        img_prefix='/root/commonfile/data/ComparisonDetectorDataset/train/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ]),
    val=dict(
        type='CocoDataset',
        ann_file='/root/commonfile/data/ComparisonDetectorDataset/test.json',
        img_prefix='/root/commonfile/data/ComparisonDetectorDataset/test/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        ann_file='/root/commonfile/data/ComparisonDetectorDataset/test.json',
        img_prefix='/root/commonfile/data/ComparisonDetectorDataset/test/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(interval=1, metric='bbox')
optimizer = dict(type='AdamW', lr=2.5e-05, weight_decay=0.0001)
optimizer_config = dict(grad_clip=dict(max_norm=1, norm_type=2))
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
runner = dict(type='EpochBasedRunner', max_epochs=12)
checkpoint_config = dict(interval=2)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
num_stages = 6
num_proposals = 100
model = dict(
    type='SparseRCNN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_input',
        num_outs=4),
    rpn_head=dict(
        type='EmbeddingRPNHead',
        num_proposals=100,
        proposal_feature_channel=256),
    roi_head=dict(
        type='SparseRoIHead',
        num_stages=6,
        stage_loss_weights=[1, 1, 1, 1, 1, 1],
        proposal_feature_channel=256,
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=2),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=[
            dict(
                type='DIIHead',
                num_classes=11,
                num_ffn_fcs=2,
                num_heads=8,
                num_cls_fcs=1,
                num_reg_fcs=3,
                feedforward_channels=2048,
                in_channels=256,
                dropout=0.0,
                ffn_act_cfg=dict(type='ReLU', inplace=True),
                dynamic_conv_cfg=dict(
                    type='DynamicConv',
                    in_channels=256,
                    feat_channels=64,
                    out_channels=256,
                    input_feat_shape=7,
                    act_cfg=dict(type='ReLU', inplace=True),
                    norm_cfg=dict(type='LN')),
                loss_bbox=dict(type='L1Loss', loss_weight=5.0),
                loss_iou=dict(type='GIoULoss', loss_weight=2.0),
                loss_cls=dict(
                    type='FocalLoss',
                    use_sigmoid=True,
                    gamma=2.0,
                    alpha=0.25,
                    loss_weight=2.0),
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    clip_border=False,
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.5, 0.5, 1.0, 1.0])),
            dict(
                type='DIIHead',
                num_classes=11,
                num_ffn_fcs=2,
                num_heads=8,
                num_cls_fcs=1,
                num_reg_fcs=3,
                feedforward_channels=2048,
                in_channels=256,
                dropout=0.0,
                ffn_act_cfg=dict(type='ReLU', inplace=True),
                dynamic_conv_cfg=dict(
                    type='DynamicConv',
                    in_channels=256,
                    feat_channels=64,
                    out_channels=256,
                    input_feat_shape=7,
                    act_cfg=dict(type='ReLU', inplace=True),
                    norm_cfg=dict(type='LN')),
                loss_bbox=dict(type='L1Loss', loss_weight=5.0),
                loss_iou=dict(type='GIoULoss', loss_weight=2.0),
                loss_cls=dict(
                    type='FocalLoss',
                    use_sigmoid=True,
                    gamma=2.0,
                    alpha=0.25,
                    loss_weight=2.0),
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    clip_border=False,
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.5, 0.5, 1.0, 1.0])),
            dict(
                type='DIIHead',
                num_classes=11,
                num_ffn_fcs=2,
                num_heads=8,
                num_cls_fcs=1,
                num_reg_fcs=3,
                feedforward_channels=2048,
                in_channels=256,
                dropout=0.0,
                ffn_act_cfg=dict(type='ReLU', inplace=True),
                dynamic_conv_cfg=dict(
                    type='DynamicConv',
                    in_channels=256,
                    feat_channels=64,
                    out_channels=256,
                    input_feat_shape=7,
                    act_cfg=dict(type='ReLU', inplace=True),
                    norm_cfg=dict(type='LN')),
                loss_bbox=dict(type='L1Loss', loss_weight=5.0),
                loss_iou=dict(type='GIoULoss', loss_weight=2.0),
                loss_cls=dict(
                    type='FocalLoss',
                    use_sigmoid=True,
                    gamma=2.0,
                    alpha=0.25,
                    loss_weight=2.0),
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    clip_border=False,
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.5, 0.5, 1.0, 1.0])),
            dict(
                type='DIIHead',
                num_classes=11,
                num_ffn_fcs=2,
                num_heads=8,
                num_cls_fcs=1,
                num_reg_fcs=3,
                feedforward_channels=2048,
                in_channels=256,
                dropout=0.0,
                ffn_act_cfg=dict(type='ReLU', inplace=True),
                dynamic_conv_cfg=dict(
                    type='DynamicConv',
                    in_channels=256,
                    feat_channels=64,
                    out_channels=256,
                    input_feat_shape=7,
                    act_cfg=dict(type='ReLU', inplace=True),
                    norm_cfg=dict(type='LN')),
                loss_bbox=dict(type='L1Loss', loss_weight=5.0),
                loss_iou=dict(type='GIoULoss', loss_weight=2.0),
                loss_cls=dict(
                    type='FocalLoss',
                    use_sigmoid=True,
                    gamma=2.0,
                    alpha=0.25,
                    loss_weight=2.0),
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    clip_border=False,
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.5, 0.5, 1.0, 1.0])),
            dict(
                type='DIIHead',
                num_classes=11,
                num_ffn_fcs=2,
                num_heads=8,
                num_cls_fcs=1,
                num_reg_fcs=3,
                feedforward_channels=2048,
                in_channels=256,
                dropout=0.0,
                ffn_act_cfg=dict(type='ReLU', inplace=True),
                dynamic_conv_cfg=dict(
                    type='DynamicConv',
                    in_channels=256,
                    feat_channels=64,
                    out_channels=256,
                    input_feat_shape=7,
                    act_cfg=dict(type='ReLU', inplace=True),
                    norm_cfg=dict(type='LN')),
                loss_bbox=dict(type='L1Loss', loss_weight=5.0),
                loss_iou=dict(type='GIoULoss', loss_weight=2.0),
                loss_cls=dict(
                    type='FocalLoss',
                    use_sigmoid=True,
                    gamma=2.0,
                    alpha=0.25,
                    loss_weight=2.0),
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    clip_border=False,
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.5, 0.5, 1.0, 1.0])),
            dict(
                type='DIIHead',
                num_classes=11,
                num_ffn_fcs=2,
                num_heads=8,
                num_cls_fcs=1,
                num_reg_fcs=3,
                feedforward_channels=2048,
                in_channels=256,
                dropout=0.0,
                ffn_act_cfg=dict(type='ReLU', inplace=True),
                dynamic_conv_cfg=dict(
                    type='DynamicConv',
                    in_channels=256,
                    feat_channels=64,
                    out_channels=256,
                    input_feat_shape=7,
                    act_cfg=dict(type='ReLU', inplace=True),
                    norm_cfg=dict(type='LN')),
                loss_bbox=dict(type='L1Loss', loss_weight=5.0),
                loss_iou=dict(type='GIoULoss', loss_weight=2.0),
                loss_cls=dict(
                    type='FocalLoss',
                    use_sigmoid=True,
                    gamma=2.0,
                    alpha=0.25,
                    loss_weight=2.0),
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    clip_border=False,
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.5, 0.5, 1.0, 1.0]))
        ]),
    train_cfg=dict(
        rpn=None,
        rcnn=[
            dict(
                assigner=dict(
                    type='HungarianAssigner',
                    cls_cost=dict(type='FocalLossCost', weight=2.0),
                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),
                    iou_cost=dict(type='IoUCost', iou_mode='giou',
                                  weight=2.0)),
                sampler=dict(type='PseudoSampler'),
                pos_weight=1),
            dict(
                assigner=dict(
                    type='HungarianAssigner',
                    cls_cost=dict(type='FocalLossCost', weight=2.0),
                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),
                    iou_cost=dict(type='IoUCost', iou_mode='giou',
                                  weight=2.0)),
                sampler=dict(type='PseudoSampler'),
                pos_weight=1),
            dict(
                assigner=dict(
                    type='HungarianAssigner',
                    cls_cost=dict(type='FocalLossCost', weight=2.0),
                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),
                    iou_cost=dict(type='IoUCost', iou_mode='giou',
                                  weight=2.0)),
                sampler=dict(type='PseudoSampler'),
                pos_weight=1),
            dict(
                assigner=dict(
                    type='HungarianAssigner',
                    cls_cost=dict(type='FocalLossCost', weight=2.0),
                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),
                    iou_cost=dict(type='IoUCost', iou_mode='giou',
                                  weight=2.0)),
                sampler=dict(type='PseudoSampler'),
                pos_weight=1),
            dict(
                assigner=dict(
                    type='HungarianAssigner',
                    cls_cost=dict(type='FocalLossCost', weight=2.0),
                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),
                    iou_cost=dict(type='IoUCost', iou_mode='giou',
                                  weight=2.0)),
                sampler=dict(type='PseudoSampler'),
                pos_weight=1),
            dict(
                assigner=dict(
                    type='HungarianAssigner',
                    cls_cost=dict(type='FocalLossCost', weight=2.0),
                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),
                    iou_cost=dict(type='IoUCost', iou_mode='giou',
                                  weight=2.0)),
                sampler=dict(type='PseudoSampler'),
                pos_weight=1)
        ]),
    test_cfg=dict(rpn=None, rcnn=dict(max_per_img=100)))
opencv_num_threads = 0
mp_start_method = 'fork'
auto_scale_lr = dict(enable=False, base_batch_size=16)
work_dir = './work_dirs/sparse_rcnn_r50_fpn_1x_coco_comparison'
auto_resume = False
gpu_ids = range(0, 4)

2023-02-03 02:14:13,685 - mmdet - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'}
2023-02-03 02:14:17,339 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2023-02-03 02:14:17,370 - mmdet - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]
2023-02-03 02:14:17,481 - mmdet - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]
2023-02-03 02:14:17,594 - mmdet - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]
2023-02-03 02:14:17,706 - mmdet - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]
2023-02-03 02:14:17,818 - mmdet - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]
2023-02-03 02:14:17,929 - mmdet - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]
Name of parameter - Initialization information

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

rpn_head.init_proposal_bboxes.weight - torch.Size([100, 4]): 
Initialized by user-defined `init_weights` in EmbeddingRPNHead  

rpn_head.init_proposal_features.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.fc_cls.weight - torch.Size([11, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.fc_cls.bias - torch.Size([11]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.fc_reg.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.fc_reg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.001, bias=0 

roi_head.bbox_head.0.attention.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.attention.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.attention.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.attention.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.attention_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.attention_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.instance_interactive_conv.norm_in.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.instance_interactive_conv.norm_in.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.instance_interactive_conv.norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.instance_interactive_conv.norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.instance_interactive_conv.fc_layer.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.instance_interactive_conv.fc_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.instance_interactive_conv.fc_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.instance_interactive_conv_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.instance_interactive_conv_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.ffn_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.ffn_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.cls_fcs.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.cls_fcs.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.cls_fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.reg_fcs.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.reg_fcs.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.reg_fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.reg_fcs.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.reg_fcs.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.reg_fcs.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.reg_fcs.6.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.0.reg_fcs.7.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.0.reg_fcs.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.fc_cls.weight - torch.Size([11, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.fc_cls.bias - torch.Size([11]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.fc_reg.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.fc_reg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.001, bias=0 

roi_head.bbox_head.1.attention.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.attention.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.attention.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.attention.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.attention_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.attention_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.instance_interactive_conv.norm_in.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.instance_interactive_conv.norm_in.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.instance_interactive_conv.norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.instance_interactive_conv.norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.instance_interactive_conv.fc_layer.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.instance_interactive_conv.fc_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.instance_interactive_conv.fc_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.instance_interactive_conv_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.instance_interactive_conv_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.ffn_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.ffn_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.cls_fcs.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.cls_fcs.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.cls_fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.reg_fcs.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.reg_fcs.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.reg_fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.reg_fcs.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.reg_fcs.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.reg_fcs.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.reg_fcs.6.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.1.reg_fcs.7.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.1.reg_fcs.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.fc_cls.weight - torch.Size([11, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.fc_cls.bias - torch.Size([11]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.fc_reg.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.fc_reg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.001, bias=0 

roi_head.bbox_head.2.attention.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.attention.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.attention.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.attention.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.attention_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.attention_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.instance_interactive_conv.norm_in.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.instance_interactive_conv.norm_in.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.instance_interactive_conv.norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.instance_interactive_conv.norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.instance_interactive_conv.fc_layer.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.instance_interactive_conv.fc_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.instance_interactive_conv.fc_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.instance_interactive_conv_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.instance_interactive_conv_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.ffn_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.ffn_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.cls_fcs.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.cls_fcs.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.cls_fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.reg_fcs.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.reg_fcs.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.reg_fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.reg_fcs.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.reg_fcs.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.reg_fcs.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.reg_fcs.6.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.2.reg_fcs.7.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.2.reg_fcs.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.fc_cls.weight - torch.Size([11, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.fc_cls.bias - torch.Size([11]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.fc_reg.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.fc_reg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.001, bias=0 

roi_head.bbox_head.3.attention.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.attention.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.attention.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.attention.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.attention_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.attention_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.instance_interactive_conv.norm_in.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.instance_interactive_conv.norm_in.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.instance_interactive_conv.norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.instance_interactive_conv.norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.instance_interactive_conv.fc_layer.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.instance_interactive_conv.fc_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.instance_interactive_conv.fc_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.instance_interactive_conv_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.instance_interactive_conv_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.ffn_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.ffn_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.cls_fcs.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.cls_fcs.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.cls_fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.reg_fcs.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.reg_fcs.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.reg_fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.reg_fcs.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.reg_fcs.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.reg_fcs.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.reg_fcs.6.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.3.reg_fcs.7.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.3.reg_fcs.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.fc_cls.weight - torch.Size([11, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.fc_cls.bias - torch.Size([11]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.fc_reg.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.fc_reg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.001, bias=0 

roi_head.bbox_head.4.attention.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.attention.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.attention.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.attention.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.attention_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.attention_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.instance_interactive_conv.norm_in.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.instance_interactive_conv.norm_in.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.instance_interactive_conv.norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.instance_interactive_conv.norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.instance_interactive_conv.fc_layer.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.instance_interactive_conv.fc_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.instance_interactive_conv.fc_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.instance_interactive_conv_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.instance_interactive_conv_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.ffn_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.ffn_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.cls_fcs.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.cls_fcs.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.cls_fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.reg_fcs.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.reg_fcs.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.reg_fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.reg_fcs.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.reg_fcs.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.reg_fcs.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.reg_fcs.6.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.4.reg_fcs.7.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.4.reg_fcs.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.fc_cls.weight - torch.Size([11, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.fc_cls.bias - torch.Size([11]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.fc_reg.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.fc_reg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.001, bias=0 

roi_head.bbox_head.5.attention.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.attention.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.attention.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.attention.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.attention_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.attention_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.instance_interactive_conv.norm_in.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.instance_interactive_conv.norm_in.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.instance_interactive_conv.norm_out.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.instance_interactive_conv.norm_out.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.instance_interactive_conv.fc_layer.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.instance_interactive_conv.fc_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.instance_interactive_conv.fc_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.instance_interactive_conv_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.instance_interactive_conv_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.ffn_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.ffn_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.cls_fcs.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.cls_fcs.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.cls_fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.reg_fcs.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.reg_fcs.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.reg_fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.reg_fcs.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.reg_fcs.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.reg_fcs.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.reg_fcs.6.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in DIIHead  

roi_head.bbox_head.5.reg_fcs.7.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  

roi_head.bbox_head.5.reg_fcs.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseRCNN  
2023-02-03 02:14:19,331 - mmdet - INFO - Start running, host: root@045187aa37f7, work_dir: /root/userfolder/mmdet-2.18/mmdetection/work_dirs/sparse_rcnn_r50_fpn_1x_coco_comparison
2023-02-03 02:14:19,332 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) DistSamplerSeedHook                
(NORMAL      ) NumClassCheckHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(NORMAL      ) NumClassCheckHook                  
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-02-03 02:14:19,332 - mmdet - INFO - workflow: [('train', 1)], max: 12 epochs
2023-02-03 02:14:19,332 - mmdet - INFO - Checkpoints will be saved to /root/userfolder/mmdet-2.18/mmdetection/work_dirs/sparse_rcnn_r50_fpn_1x_coco_comparison by HardDiskBackend.
2023-02-03 02:15:06,872 - mmdet - INFO - Epoch [1][50/834]	lr: 2.473e-06, eta: 2:37:46, time: 0.951, data_time: 0.284, memory: 5033, stage0_loss_cls: 2.1178, stage0_pos_acc: 14.0500, stage0_loss_bbox: 4.8701, stage0_loss_iou: 2.2290, stage1_loss_cls: 2.1987, stage1_pos_acc: 10.6939, stage1_loss_bbox: 3.4884, stage1_loss_iou: 2.1194, stage2_loss_cls: 2.0587, stage2_pos_acc: 12.5966, stage2_loss_bbox: 3.9794, stage2_loss_iou: 2.2457, stage3_loss_cls: 2.1525, stage3_pos_acc: 11.3238, stage3_loss_bbox: 3.5492, stage3_loss_iou: 2.3890, stage4_loss_cls: 2.2036, stage4_pos_acc: 9.7335, stage4_loss_bbox: 3.9625, stage4_loss_iou: 2.6525, stage5_loss_cls: 2.5433, stage5_pos_acc: 4.4337, stage5_loss_bbox: 4.0157, stage5_loss_iou: 2.7217, loss: 51.4972, grad_norm: 11720.5860
2023-02-03 02:15:40,308 - mmdet - INFO - Epoch [1][100/834]	lr: 4.970e-06, eta: 2:13:42, time: 0.669, data_time: 0.008, memory: 5033, stage0_loss_cls: 1.9988, stage0_pos_acc: 17.3869, stage0_loss_bbox: 2.2959, stage0_loss_iou: 1.8726, stage1_loss_cls: 1.9756, stage1_pos_acc: 21.2265, stage1_loss_bbox: 1.6524, stage1_loss_iou: 1.9190, stage2_loss_cls: 1.8020, stage2_pos_acc: 26.3933, stage2_loss_bbox: 1.5792, stage2_loss_iou: 2.0331, stage3_loss_cls: 1.7820, stage3_pos_acc: 21.6388, stage3_loss_bbox: 1.4646, stage3_loss_iou: 2.1982, stage4_loss_cls: 1.5884, stage4_pos_acc: 30.0413, stage4_loss_bbox: 1.4472, stage4_loss_iou: 2.2487, stage5_loss_cls: 1.7102, stage5_pos_acc: 28.2412, stage5_loss_bbox: 1.4453, stage5_loss_iou: 2.2095, loss: 33.2229, grad_norm: 3702.2805
2023-02-03 02:16:13,037 - mmdet - INFO - Epoch [1][150/834]	lr: 7.468e-06, eta: 2:04:32, time: 0.655, data_time: 0.008, memory: 5033, stage0_loss_cls: 1.7010, stage0_pos_acc: 25.3884, stage0_loss_bbox: 1.3938, stage0_loss_iou: 1.7488, stage1_loss_cls: 1.5048, stage1_pos_acc: 47.8467, stage1_loss_bbox: 1.2072, stage1_loss_iou: 1.8927, stage2_loss_cls: 1.2486, stage2_pos_acc: 48.9009, stage2_loss_bbox: 1.2090, stage2_loss_iou: 1.9427, stage3_loss_cls: 1.2178, stage3_pos_acc: 46.8356, stage3_loss_bbox: 1.2450, stage3_loss_iou: 1.9564, stage4_loss_cls: 1.1488, stage4_pos_acc: 47.0782, stage4_loss_bbox: 1.2950, stage4_loss_iou: 1.9178, stage5_loss_cls: 1.0926, stage5_pos_acc: 47.9777, stage5_loss_bbox: 1.3011, stage5_loss_iou: 1.9530, loss: 26.9761, grad_norm: 1178.3039
2023-02-03 02:16:45,782 - mmdet - INFO - Epoch [1][200/834]	lr: 9.965e-06, eta: 1:59:41, time: 0.655, data_time: 0.008, memory: 5033, stage0_loss_cls: 1.3200, stage0_pos_acc: 46.3812, stage0_loss_bbox: 1.1855, stage0_loss_iou: 1.6869, stage1_loss_cls: 1.3135, stage1_pos_acc: 49.5673, stage1_loss_bbox: 1.1295, stage1_loss_iou: 1.7373, stage2_loss_cls: 1.1528, stage2_pos_acc: 46.9457, stage2_loss_bbox: 1.1402, stage2_loss_iou: 1.7230, stage3_loss_cls: 1.0722, stage3_pos_acc: 47.4668, stage3_loss_bbox: 1.1318, stage3_loss_iou: 1.7175, stage4_loss_cls: 1.0556, stage4_pos_acc: 48.3322, stage4_loss_bbox: 1.1454, stage4_loss_iou: 1.7363, stage5_loss_cls: 1.0706, stage5_pos_acc: 48.0697, stage5_loss_bbox: 1.2044, stage5_loss_iou: 1.7736, loss: 24.2961, grad_norm: 446.3506
2023-02-03 02:17:18,677 - mmdet - INFO - Epoch [1][250/834]	lr: 1.246e-05, eta: 1:56:39, time: 0.658, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.1989, stage0_pos_acc: 45.4315, stage0_loss_bbox: 1.0986, stage0_loss_iou: 1.7052, stage1_loss_cls: 1.1411, stage1_pos_acc: 45.5675, stage1_loss_bbox: 1.0989, stage1_loss_iou: 1.7008, stage2_loss_cls: 1.0586, stage2_pos_acc: 45.8104, stage2_loss_bbox: 1.0550, stage2_loss_iou: 1.6792, stage3_loss_cls: 1.0006, stage3_pos_acc: 45.5307, stage3_loss_bbox: 1.0186, stage3_loss_iou: 1.6466, stage4_loss_cls: 0.9879, stage4_pos_acc: 46.5006, stage4_loss_bbox: 1.0061, stage4_loss_iou: 1.6480, stage5_loss_cls: 0.9969, stage5_pos_acc: 46.9428, stage5_loss_bbox: 1.0627, stage5_loss_iou: 1.7109, loss: 22.8147, grad_norm: 227.2660
2023-02-03 02:17:51,616 - mmdet - INFO - Epoch [1][300/834]	lr: 1.496e-05, eta: 1:54:28, time: 0.659, data_time: 0.008, memory: 5033, stage0_loss_cls: 1.1634, stage0_pos_acc: 42.3921, stage0_loss_bbox: 1.0917, stage0_loss_iou: 1.6737, stage1_loss_cls: 1.0603, stage1_pos_acc: 42.4033, stage1_loss_bbox: 1.0737, stage1_loss_iou: 1.6621, stage2_loss_cls: 1.0547, stage2_pos_acc: 41.3137, stage2_loss_bbox: 1.0005, stage2_loss_iou: 1.6179, stage3_loss_cls: 0.9998, stage3_pos_acc: 44.3354, stage3_loss_bbox: 0.9062, stage3_loss_iou: 1.5649, stage4_loss_cls: 0.9772, stage4_pos_acc: 43.1406, stage4_loss_bbox: 0.9445, stage4_loss_iou: 1.6204, stage5_loss_cls: 0.9847, stage5_pos_acc: 43.4897, stage5_loss_bbox: 1.0021, stage5_loss_iou: 1.7240, loss: 22.1217, grad_norm: 156.6090
2023-02-03 02:18:24,782 - mmdet - INFO - Epoch [1][350/834]	lr: 1.746e-05, eta: 1:52:52, time: 0.663, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.1018, stage0_pos_acc: 44.6402, stage0_loss_bbox: 1.0184, stage0_loss_iou: 1.6637, stage1_loss_cls: 0.9714, stage1_pos_acc: 47.1341, stage1_loss_bbox: 0.9591, stage1_loss_iou: 1.6174, stage2_loss_cls: 0.9639, stage2_pos_acc: 48.4320, stage2_loss_bbox: 0.8318, stage2_loss_iou: 1.5086, stage3_loss_cls: 0.9281, stage3_pos_acc: 48.7912, stage3_loss_bbox: 0.7816, stage3_loss_iou: 1.4984, stage4_loss_cls: 0.9134, stage4_pos_acc: 50.9503, stage4_loss_bbox: 0.8112, stage4_loss_iou: 1.5190, stage5_loss_cls: 0.9253, stage5_pos_acc: 50.5686, stage5_loss_bbox: 0.8611, stage5_loss_iou: 1.5635, loss: 20.4377, grad_norm: 131.6294
2023-02-03 02:18:57,829 - mmdet - INFO - Epoch [1][400/834]	lr: 1.996e-05, eta: 1:51:28, time: 0.661, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.0638, stage0_pos_acc: 45.9496, stage0_loss_bbox: 1.0185, stage0_loss_iou: 1.6364, stage1_loss_cls: 0.9244, stage1_pos_acc: 51.2317, stage1_loss_bbox: 0.8337, stage1_loss_iou: 1.4895, stage2_loss_cls: 0.9058, stage2_pos_acc: 52.9728, stage2_loss_bbox: 0.7088, stage2_loss_iou: 1.3912, stage3_loss_cls: 0.8885, stage3_pos_acc: 54.5398, stage3_loss_bbox: 0.6850, stage3_loss_iou: 1.3532, stage4_loss_cls: 0.8772, stage4_pos_acc: 55.2378, stage4_loss_bbox: 0.7006, stage4_loss_iou: 1.3669, stage5_loss_cls: 0.8845, stage5_pos_acc: 54.0444, stage5_loss_bbox: 0.7350, stage5_loss_iou: 1.3991, loss: 18.8620, grad_norm: 88.9841
2023-02-03 02:19:30,876 - mmdet - INFO - Epoch [1][450/834]	lr: 2.245e-05, eta: 1:50:16, time: 0.662, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.0842, stage0_pos_acc: 44.8082, stage0_loss_bbox: 1.0416, stage0_loss_iou: 1.6222, stage1_loss_cls: 0.9374, stage1_pos_acc: 48.2676, stage1_loss_bbox: 0.7916, stage1_loss_iou: 1.4141, stage2_loss_cls: 0.9112, stage2_pos_acc: 52.7175, stage2_loss_bbox: 0.6992, stage2_loss_iou: 1.3154, stage3_loss_cls: 0.8960, stage3_pos_acc: 54.4059, stage3_loss_bbox: 0.6912, stage3_loss_iou: 1.3079, stage4_loss_cls: 0.8871, stage4_pos_acc: 54.4265, stage4_loss_bbox: 0.7215, stage4_loss_iou: 1.3478, stage5_loss_cls: 0.8962, stage5_pos_acc: 54.8152, stage5_loss_bbox: 0.7486, stage5_loss_iou: 1.3827, loss: 18.6960, grad_norm: 81.2408
2023-02-03 02:20:03,876 - mmdet - INFO - Epoch [1][500/834]	lr: 2.495e-05, eta: 1:49:11, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.0341, stage0_pos_acc: 49.9511, stage0_loss_bbox: 0.9900, stage0_loss_iou: 1.5867, stage1_loss_cls: 0.8782, stage1_pos_acc: 53.1009, stage1_loss_bbox: 0.7210, stage1_loss_iou: 1.3592, stage2_loss_cls: 0.8730, stage2_pos_acc: 56.6523, stage2_loss_bbox: 0.6274, stage2_loss_iou: 1.2476, stage3_loss_cls: 0.8816, stage3_pos_acc: 54.8807, stage3_loss_bbox: 0.6127, stage3_loss_iou: 1.2089, stage4_loss_cls: 0.8839, stage4_pos_acc: 56.6514, stage4_loss_bbox: 0.5997, stage4_loss_iou: 1.1812, stage5_loss_cls: 0.8832, stage5_pos_acc: 55.5981, stage5_loss_bbox: 0.6334, stage5_loss_iou: 1.2394, loss: 17.4411, grad_norm: 71.4026
2023-02-03 02:20:36,753 - mmdet - INFO - Epoch [1][550/834]	lr: 2.500e-05, eta: 1:48:10, time: 0.658, data_time: 0.008, memory: 5033, stage0_loss_cls: 1.0314, stage0_pos_acc: 47.6353, stage0_loss_bbox: 0.9428, stage0_loss_iou: 1.5553, stage1_loss_cls: 0.8563, stage1_pos_acc: 55.2403, stage1_loss_bbox: 0.6480, stage1_loss_iou: 1.2781, stage2_loss_cls: 0.8420, stage2_pos_acc: 58.8838, stage2_loss_bbox: 0.5700, stage2_loss_iou: 1.1567, stage3_loss_cls: 0.8449, stage3_pos_acc: 58.6701, stage3_loss_bbox: 0.5529, stage3_loss_iou: 1.1313, stage4_loss_cls: 0.8415, stage4_pos_acc: 59.3095, stage4_loss_bbox: 0.5736, stage4_loss_iou: 1.1379, stage5_loss_cls: 0.8527, stage5_pos_acc: 59.2986, stage5_loss_bbox: 0.5931, stage5_loss_iou: 1.1730, loss: 16.5815, grad_norm: 68.0544
2023-02-03 02:21:09,891 - mmdet - INFO - Epoch [1][600/834]	lr: 2.500e-05, eta: 1:47:17, time: 0.663, data_time: 0.008, memory: 5033, stage0_loss_cls: 1.0073, stage0_pos_acc: 48.6087, stage0_loss_bbox: 0.9006, stage0_loss_iou: 1.5463, stage1_loss_cls: 0.8449, stage1_pos_acc: 57.3554, stage1_loss_bbox: 0.5949, stage1_loss_iou: 1.2450, stage2_loss_cls: 0.8265, stage2_pos_acc: 60.0892, stage2_loss_bbox: 0.5140, stage2_loss_iou: 1.1090, stage3_loss_cls: 0.8251, stage3_pos_acc: 60.1785, stage3_loss_bbox: 0.5135, stage3_loss_iou: 1.0990, stage4_loss_cls: 0.8253, stage4_pos_acc: 60.6123, stage4_loss_bbox: 0.5267, stage4_loss_iou: 1.1188, stage5_loss_cls: 0.8375, stage5_pos_acc: 60.7611, stage5_loss_bbox: 0.5482, stage5_loss_iou: 1.1540, loss: 16.0367, grad_norm: 65.9696
2023-02-03 02:21:42,884 - mmdet - INFO - Epoch [1][650/834]	lr: 2.500e-05, eta: 1:46:25, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.0001, stage0_pos_acc: 53.3770, stage0_loss_bbox: 0.9164, stage0_loss_iou: 1.5837, stage1_loss_cls: 0.8089, stage1_pos_acc: 60.2131, stage1_loss_bbox: 0.6171, stage1_loss_iou: 1.2839, stage2_loss_cls: 0.7972, stage2_pos_acc: 62.7929, stage2_loss_bbox: 0.5364, stage2_loss_iou: 1.1563, stage3_loss_cls: 0.7975, stage3_pos_acc: 62.5497, stage3_loss_bbox: 0.5270, stage3_loss_iou: 1.1452, stage4_loss_cls: 0.7947, stage4_pos_acc: 63.7875, stage4_loss_bbox: 0.5266, stage4_loss_iou: 1.1355, stage5_loss_cls: 0.8026, stage5_pos_acc: 62.5620, stage5_loss_bbox: 0.5477, stage5_loss_iou: 1.1583, loss: 16.1348, grad_norm: 67.4446
2023-02-03 02:22:16,053 - mmdet - INFO - Epoch [1][700/834]	lr: 2.500e-05, eta: 1:45:38, time: 0.663, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.0098, stage0_pos_acc: 46.0710, stage0_loss_bbox: 0.8909, stage0_loss_iou: 1.5650, stage1_loss_cls: 0.8080, stage1_pos_acc: 57.1315, stage1_loss_bbox: 0.5697, stage1_loss_iou: 1.2408, stage2_loss_cls: 0.7893, stage2_pos_acc: 60.0710, stage2_loss_bbox: 0.4970, stage2_loss_iou: 1.1136, stage3_loss_cls: 0.7912, stage3_pos_acc: 58.6095, stage3_loss_bbox: 0.4971, stage3_loss_iou: 1.1048, stage4_loss_cls: 0.7955, stage4_pos_acc: 59.8888, stage4_loss_bbox: 0.4970, stage4_loss_iou: 1.1084, stage5_loss_cls: 0.8025, stage5_pos_acc: 59.7560, stage5_loss_bbox: 0.5161, stage5_loss_iou: 1.1341, loss: 15.7309, grad_norm: 63.4760
2023-02-03 02:22:48,977 - mmdet - INFO - Epoch [1][750/834]	lr: 2.500e-05, eta: 1:44:50, time: 0.658, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.0520, stage0_pos_acc: 48.1122, stage0_loss_bbox: 0.9183, stage0_loss_iou: 1.5163, stage1_loss_cls: 0.8463, stage1_pos_acc: 55.8574, stage1_loss_bbox: 0.5533, stage1_loss_iou: 1.1483, stage2_loss_cls: 0.8282, stage2_pos_acc: 59.1369, stage2_loss_bbox: 0.4739, stage2_loss_iou: 1.0131, stage3_loss_cls: 0.8370, stage3_pos_acc: 57.8299, stage3_loss_bbox: 0.4642, stage3_loss_iou: 0.9829, stage4_loss_cls: 0.8316, stage4_pos_acc: 60.2823, stage4_loss_bbox: 0.4617, stage4_loss_iou: 0.9697, stage5_loss_cls: 0.8358, stage5_pos_acc: 60.4038, stage5_loss_bbox: 0.4850, stage5_loss_iou: 0.9919, loss: 15.2094, grad_norm: 64.6238
2023-02-03 02:23:21,988 - mmdet - INFO - Epoch [1][800/834]	lr: 2.500e-05, eta: 1:44:05, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.0212, stage0_pos_acc: 46.9969, stage0_loss_bbox: 0.8905, stage0_loss_iou: 1.5216, stage1_loss_cls: 0.8396, stage1_pos_acc: 53.9152, stage1_loss_bbox: 0.5235, stage1_loss_iou: 1.1407, stage2_loss_cls: 0.8164, stage2_pos_acc: 56.8982, stage2_loss_bbox: 0.4524, stage2_loss_iou: 1.0017, stage3_loss_cls: 0.8159, stage3_pos_acc: 57.6322, stage3_loss_bbox: 0.4520, stage3_loss_iou: 0.9914, stage4_loss_cls: 0.8218, stage4_pos_acc: 58.8545, stage4_loss_bbox: 0.4543, stage4_loss_iou: 0.9816, stage5_loss_cls: 0.8209, stage5_pos_acc: 58.8156, stage5_loss_bbox: 0.4620, stage5_loss_iou: 0.9899, loss: 14.9974, grad_norm: 62.1710
2023-02-03 02:24:13,338 - mmdet - INFO - Evaluating bbox...
2023-02-03 02:24:24,233 - mmdet - INFO - Exp name: sparse_rcnn_r50_fpn_1x_coco_comparison.py
2023-02-03 02:24:24,234 - mmdet - INFO - Epoch(val) [1][186]	bbox_mAP: 0.0140, bbox_mAP_50: 0.0390, bbox_mAP_75: 0.0070, bbox_mAP_s: 0.0000, bbox_mAP_m: 0.0120, bbox_mAP_l: 0.0140, bbox_mAP_copypaste: 0.014 0.039 0.007 0.000 0.012 0.014
2023-02-03 02:25:10,468 - mmdet - INFO - Epoch [2][50/834]	lr: 2.500e-05, eta: 1:41:17, time: 0.924, data_time: 0.267, memory: 5033, stage0_loss_cls: 1.0084, stage0_pos_acc: 50.9727, stage0_loss_bbox: 0.8750, stage0_loss_iou: 1.5133, stage1_loss_cls: 0.7957, stage1_pos_acc: 58.9246, stage1_loss_bbox: 0.5010, stage1_loss_iou: 1.1029, stage2_loss_cls: 0.7782, stage2_pos_acc: 58.8753, stage2_loss_bbox: 0.4217, stage2_loss_iou: 0.9526, stage3_loss_cls: 0.7756, stage3_pos_acc: 61.7717, stage3_loss_bbox: 0.4098, stage3_loss_iou: 0.9207, stage4_loss_cls: 0.7846, stage4_pos_acc: 61.0316, stage4_loss_bbox: 0.4081, stage4_loss_iou: 0.9108, stage5_loss_cls: 0.7925, stage5_pos_acc: 60.6476, stage5_loss_bbox: 0.4093, stage5_loss_iou: 0.9111, loss: 14.2716, grad_norm: 61.0890
2023-02-03 02:25:43,963 - mmdet - INFO - Epoch [2][100/834]	lr: 2.500e-05, eta: 1:40:46, time: 0.669, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9887, stage0_pos_acc: 51.1284, stage0_loss_bbox: 0.8941, stage0_loss_iou: 1.5379, stage1_loss_cls: 0.7581, stage1_pos_acc: 61.5106, stage1_loss_bbox: 0.5115, stage1_loss_iou: 1.1343, stage2_loss_cls: 0.7413, stage2_pos_acc: 65.4870, stage2_loss_bbox: 0.4303, stage2_loss_iou: 0.9680, stage3_loss_cls: 0.7449, stage3_pos_acc: 64.3655, stage3_loss_bbox: 0.4189, stage3_loss_iou: 0.9374, stage4_loss_cls: 0.7512, stage4_pos_acc: 66.4998, stage4_loss_bbox: 0.4127, stage4_loss_iou: 0.9254, stage5_loss_cls: 0.7537, stage5_pos_acc: 65.9520, stage5_loss_bbox: 0.4181, stage5_loss_iou: 0.9297, loss: 14.2563, grad_norm: 56.9054
2023-02-03 02:26:17,322 - mmdet - INFO - Epoch [2][150/834]	lr: 2.500e-05, eta: 1:40:13, time: 0.667, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.0476, stage0_pos_acc: 43.2531, stage0_loss_bbox: 0.9131, stage0_loss_iou: 1.5157, stage1_loss_cls: 0.8191, stage1_pos_acc: 55.7660, stage1_loss_bbox: 0.5306, stage1_loss_iou: 1.1193, stage2_loss_cls: 0.7986, stage2_pos_acc: 57.9972, stage2_loss_bbox: 0.4459, stage2_loss_iou: 0.9809, stage3_loss_cls: 0.7941, stage3_pos_acc: 61.9910, stage3_loss_bbox: 0.4342, stage3_loss_iou: 0.9516, stage4_loss_cls: 0.7987, stage4_pos_acc: 59.6006, stage4_loss_bbox: 0.4365, stage4_loss_iou: 0.9464, stage5_loss_cls: 0.8022, stage5_pos_acc: 61.0314, stage5_loss_bbox: 0.4360, stage5_loss_iou: 0.9470, loss: 14.7175, grad_norm: 55.5395
2023-02-03 02:26:50,293 - mmdet - INFO - Epoch [2][200/834]	lr: 2.500e-05, eta: 1:39:36, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.0063, stage0_pos_acc: 48.7138, stage0_loss_bbox: 0.8687, stage0_loss_iou: 1.4875, stage1_loss_cls: 0.7705, stage1_pos_acc: 58.3767, stage1_loss_bbox: 0.4924, stage1_loss_iou: 1.0552, stage2_loss_cls: 0.7508, stage2_pos_acc: 64.7951, stage2_loss_bbox: 0.4155, stage2_loss_iou: 0.9095, stage3_loss_cls: 0.7573, stage3_pos_acc: 63.5573, stage3_loss_bbox: 0.3983, stage3_loss_iou: 0.8784, stage4_loss_cls: 0.7635, stage4_pos_acc: 63.2442, stage4_loss_bbox: 0.3976, stage4_loss_iou: 0.8680, stage5_loss_cls: 0.7674, stage5_pos_acc: 64.2199, stage5_loss_bbox: 0.4029, stage5_loss_iou: 0.8704, loss: 13.8601, grad_norm: 56.9159
2023-02-03 02:27:23,226 - mmdet - INFO - Epoch [2][250/834]	lr: 2.500e-05, eta: 1:39:00, time: 0.659, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.0169, stage0_pos_acc: 47.6841, stage0_loss_bbox: 0.8545, stage0_loss_iou: 1.5285, stage1_loss_cls: 0.7721, stage1_pos_acc: 59.9002, stage1_loss_bbox: 0.4698, stage1_loss_iou: 1.0742, stage2_loss_cls: 0.7406, stage2_pos_acc: 64.3416, stage2_loss_bbox: 0.4019, stage2_loss_iou: 0.9265, stage3_loss_cls: 0.7348, stage3_pos_acc: 65.7283, stage3_loss_bbox: 0.3836, stage3_loss_iou: 0.8866, stage4_loss_cls: 0.7467, stage4_pos_acc: 64.1538, stage4_loss_bbox: 0.3786, stage4_loss_iou: 0.8787, stage5_loss_cls: 0.7521, stage5_pos_acc: 64.2858, stage5_loss_bbox: 0.3761, stage5_loss_iou: 0.8728, loss: 13.7949, grad_norm: 57.7384
2023-02-03 02:27:56,073 - mmdet - INFO - Epoch [2][300/834]	lr: 2.500e-05, eta: 1:38:23, time: 0.657, data_time: 0.009, memory: 5033, stage0_loss_cls: 1.0097, stage0_pos_acc: 46.8374, stage0_loss_bbox: 0.8699, stage0_loss_iou: 1.4817, stage1_loss_cls: 0.7729, stage1_pos_acc: 57.8767, stage1_loss_bbox: 0.4791, stage1_loss_iou: 1.0365, stage2_loss_cls: 0.7447, stage2_pos_acc: 62.0354, stage2_loss_bbox: 0.3979, stage2_loss_iou: 0.8803, stage3_loss_cls: 0.7514, stage3_pos_acc: 62.8159, stage3_loss_bbox: 0.3867, stage3_loss_iou: 0.8481, stage4_loss_cls: 0.7632, stage4_pos_acc: 61.3880, stage4_loss_bbox: 0.3838, stage4_loss_iou: 0.8412, stage5_loss_cls: 0.7688, stage5_pos_acc: 61.5868, stage5_loss_bbox: 0.3885, stage5_loss_iou: 0.8457, loss: 13.6501, grad_norm: 56.3624
2023-02-03 02:28:28,768 - mmdet - INFO - Epoch [2][350/834]	lr: 2.500e-05, eta: 1:37:46, time: 0.654, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9302, stage0_pos_acc: 56.0415, stage0_loss_bbox: 0.8336, stage0_loss_iou: 1.5301, stage1_loss_cls: 0.7088, stage1_pos_acc: 66.0479, stage1_loss_bbox: 0.4436, stage1_loss_iou: 1.0762, stage2_loss_cls: 0.6783, stage2_pos_acc: 69.7309, stage2_loss_bbox: 0.3750, stage2_loss_iou: 0.9191, stage3_loss_cls: 0.6807, stage3_pos_acc: 71.3127, stage3_loss_bbox: 0.3605, stage3_loss_iou: 0.8823, stage4_loss_cls: 0.6901, stage4_pos_acc: 71.1593, stage4_loss_bbox: 0.3588, stage4_loss_iou: 0.8742, stage5_loss_cls: 0.6942, stage5_pos_acc: 71.8646, stage5_loss_bbox: 0.3602, stage5_loss_iou: 0.8766, loss: 13.2727, grad_norm: 51.0379
2023-02-03 02:29:01,758 - mmdet - INFO - Epoch [2][400/834]	lr: 2.500e-05, eta: 1:37:11, time: 0.660, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9759, stage0_pos_acc: 51.0486, stage0_loss_bbox: 0.8286, stage0_loss_iou: 1.4775, stage1_loss_cls: 0.7467, stage1_pos_acc: 63.3856, stage1_loss_bbox: 0.4462, stage1_loss_iou: 0.9892, stage2_loss_cls: 0.7262, stage2_pos_acc: 67.2013, stage2_loss_bbox: 0.3722, stage2_loss_iou: 0.8361, stage3_loss_cls: 0.7338, stage3_pos_acc: 67.2338, stage3_loss_bbox: 0.3594, stage3_loss_iou: 0.8149, stage4_loss_cls: 0.7479, stage4_pos_acc: 66.6437, stage4_loss_bbox: 0.3592, stage4_loss_iou: 0.8096, stage5_loss_cls: 0.7542, stage5_pos_acc: 67.4057, stage5_loss_bbox: 0.3635, stage5_loss_iou: 0.8162, loss: 13.1574, grad_norm: 57.0999
2023-02-03 02:29:34,757 - mmdet - INFO - Epoch [2][450/834]	lr: 2.500e-05, eta: 1:36:36, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9802, stage0_pos_acc: 50.9317, stage0_loss_bbox: 0.8387, stage0_loss_iou: 1.4864, stage1_loss_cls: 0.7339, stage1_pos_acc: 65.1527, stage1_loss_bbox: 0.4473, stage1_loss_iou: 1.0294, stage2_loss_cls: 0.7069, stage2_pos_acc: 70.2006, stage2_loss_bbox: 0.3779, stage2_loss_iou: 0.8892, stage3_loss_cls: 0.7135, stage3_pos_acc: 69.6134, stage3_loss_bbox: 0.3682, stage3_loss_iou: 0.8590, stage4_loss_cls: 0.7235, stage4_pos_acc: 68.5372, stage4_loss_bbox: 0.3608, stage4_loss_iou: 0.8416, stage5_loss_cls: 0.7296, stage5_pos_acc: 69.8208, stage5_loss_bbox: 0.3647, stage5_loss_iou: 0.8447, loss: 13.2955, grad_norm: 55.5790
2023-02-03 02:30:07,902 - mmdet - INFO - Epoch [2][500/834]	lr: 2.500e-05, eta: 1:36:02, time: 0.663, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9539, stage0_pos_acc: 49.4740, stage0_loss_bbox: 0.8549, stage0_loss_iou: 1.5105, stage1_loss_cls: 0.6923, stage1_pos_acc: 67.4587, stage1_loss_bbox: 0.4293, stage1_loss_iou: 1.0121, stage2_loss_cls: 0.6688, stage2_pos_acc: 70.5811, stage2_loss_bbox: 0.3546, stage2_loss_iou: 0.8548, stage3_loss_cls: 0.6748, stage3_pos_acc: 71.1205, stage3_loss_bbox: 0.3407, stage3_loss_iou: 0.8234, stage4_loss_cls: 0.6864, stage4_pos_acc: 71.4393, stage4_loss_bbox: 0.3379, stage4_loss_iou: 0.8111, stage5_loss_cls: 0.6954, stage5_pos_acc: 72.3839, stage5_loss_bbox: 0.3365, stage5_loss_iou: 0.8050, loss: 12.8424, grad_norm: 56.2531
2023-02-03 02:30:40,930 - mmdet - INFO - Epoch [2][550/834]	lr: 2.500e-05, eta: 1:35:28, time: 0.661, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9905, stage0_pos_acc: 46.0581, stage0_loss_bbox: 0.8389, stage0_loss_iou: 1.4941, stage1_loss_cls: 0.7266, stage1_pos_acc: 64.3123, stage1_loss_bbox: 0.4391, stage1_loss_iou: 0.9925, stage2_loss_cls: 0.7025, stage2_pos_acc: 68.9313, stage2_loss_bbox: 0.3685, stage2_loss_iou: 0.8503, stage3_loss_cls: 0.7194, stage3_pos_acc: 67.0682, stage3_loss_bbox: 0.3545, stage3_loss_iou: 0.8127, stage4_loss_cls: 0.7266, stage4_pos_acc: 68.4289, stage4_loss_bbox: 0.3515, stage4_loss_iou: 0.8040, stage5_loss_cls: 0.7329, stage5_pos_acc: 68.2339, stage5_loss_bbox: 0.3519, stage5_loss_iou: 0.8047, loss: 13.0613, grad_norm: 57.7027
2023-02-03 02:31:13,803 - mmdet - INFO - Epoch [2][600/834]	lr: 2.500e-05, eta: 1:34:53, time: 0.657, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9797, stage0_pos_acc: 48.4022, stage0_loss_bbox: 0.8365, stage0_loss_iou: 1.4710, stage1_loss_cls: 0.7133, stage1_pos_acc: 65.0499, stage1_loss_bbox: 0.4388, stage1_loss_iou: 0.9875, stage2_loss_cls: 0.6883, stage2_pos_acc: 68.7814, stage2_loss_bbox: 0.3681, stage2_loss_iou: 0.8414, stage3_loss_cls: 0.7032, stage3_pos_acc: 68.2282, stage3_loss_bbox: 0.3512, stage3_loss_iou: 0.8042, stage4_loss_cls: 0.7112, stage4_pos_acc: 68.3137, stage4_loss_bbox: 0.3510, stage4_loss_iou: 0.7988, stage5_loss_cls: 0.7207, stage5_pos_acc: 67.8704, stage5_loss_bbox: 0.3495, stage5_loss_iou: 0.7983, loss: 12.9127, grad_norm: 54.3092
2023-02-03 02:31:46,725 - mmdet - INFO - Epoch [2][650/834]	lr: 2.500e-05, eta: 1:34:18, time: 0.658, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9781, stage0_pos_acc: 48.5644, stage0_loss_bbox: 0.8539, stage0_loss_iou: 1.5010, stage1_loss_cls: 0.7299, stage1_pos_acc: 66.1374, stage1_loss_bbox: 0.4407, stage1_loss_iou: 1.0265, stage2_loss_cls: 0.7074, stage2_pos_acc: 68.7406, stage2_loss_bbox: 0.3715, stage2_loss_iou: 0.8724, stage3_loss_cls: 0.7136, stage3_pos_acc: 68.6949, stage3_loss_bbox: 0.3539, stage3_loss_iou: 0.8318, stage4_loss_cls: 0.7303, stage4_pos_acc: 68.3409, stage4_loss_bbox: 0.3457, stage4_loss_iou: 0.8157, stage5_loss_cls: 0.7402, stage5_pos_acc: 68.3690, stage5_loss_bbox: 0.3442, stage5_loss_iou: 0.8139, loss: 13.1705, grad_norm: 59.6550
2023-02-03 02:32:19,938 - mmdet - INFO - Epoch [2][700/834]	lr: 2.500e-05, eta: 1:33:45, time: 0.664, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9710, stage0_pos_acc: 49.1757, stage0_loss_bbox: 0.8717, stage0_loss_iou: 1.5210, stage1_loss_cls: 0.7148, stage1_pos_acc: 68.3420, stage1_loss_bbox: 0.4562, stage1_loss_iou: 1.0198, stage2_loss_cls: 0.6868, stage2_pos_acc: 71.8550, stage2_loss_bbox: 0.3802, stage2_loss_iou: 0.8630, stage3_loss_cls: 0.6930, stage3_pos_acc: 71.7008, stage3_loss_bbox: 0.3660, stage3_loss_iou: 0.8316, stage4_loss_cls: 0.7030, stage4_pos_acc: 71.5514, stage4_loss_bbox: 0.3624, stage4_loss_iou: 0.8177, stage5_loss_cls: 0.7082, stage5_pos_acc: 72.8882, stage5_loss_bbox: 0.3614, stage5_loss_iou: 0.8136, loss: 13.1415, grad_norm: 55.8243
2023-02-03 02:32:52,805 - mmdet - INFO - Epoch [2][750/834]	lr: 2.500e-05, eta: 1:33:10, time: 0.657, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9459, stage0_pos_acc: 52.6269, stage0_loss_bbox: 0.8132, stage0_loss_iou: 1.4758, stage1_loss_cls: 0.7113, stage1_pos_acc: 67.4093, stage1_loss_bbox: 0.4293, stage1_loss_iou: 0.9900, stage2_loss_cls: 0.6945, stage2_pos_acc: 68.9739, stage2_loss_bbox: 0.3603, stage2_loss_iou: 0.8477, stage3_loss_cls: 0.7060, stage3_pos_acc: 69.4464, stage3_loss_bbox: 0.3488, stage3_loss_iou: 0.8207, stage4_loss_cls: 0.7187, stage4_pos_acc: 69.1045, stage4_loss_bbox: 0.3490, stage4_loss_iou: 0.8182, stage5_loss_cls: 0.7290, stage5_pos_acc: 69.0889, stage5_loss_bbox: 0.3506, stage5_loss_iou: 0.8206, loss: 12.9296, grad_norm: 55.4268
2023-02-03 02:33:25,738 - mmdet - INFO - Epoch [2][800/834]	lr: 2.500e-05, eta: 1:32:35, time: 0.659, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9728, stage0_pos_acc: 51.5852, stage0_loss_bbox: 0.8081, stage0_loss_iou: 1.4425, stage1_loss_cls: 0.7200, stage1_pos_acc: 67.5134, stage1_loss_bbox: 0.4162, stage1_loss_iou: 0.9211, stage2_loss_cls: 0.7078, stage2_pos_acc: 69.1348, stage2_loss_bbox: 0.3512, stage2_loss_iou: 0.7770, stage3_loss_cls: 0.7212, stage3_pos_acc: 68.7941, stage3_loss_bbox: 0.3396, stage3_loss_iou: 0.7533, stage4_loss_cls: 0.7291, stage4_pos_acc: 69.9169, stage4_loss_bbox: 0.3342, stage4_loss_iou: 0.7408, stage5_loss_cls: 0.7416, stage5_pos_acc: 69.0306, stage5_loss_bbox: 0.3367, stage5_loss_iou: 0.7418, loss: 12.5549, grad_norm: 57.5690
2023-02-03 02:33:49,011 - mmdet - INFO - Saving checkpoint at 2 epochs
2023-02-03 02:34:20,314 - mmdet - INFO - Evaluating bbox...
2023-02-03 02:34:32,346 - mmdet - INFO - Exp name: sparse_rcnn_r50_fpn_1x_coco_comparison.py
2023-02-03 02:34:32,347 - mmdet - INFO - Epoch(val) [2][186]	bbox_mAP: 0.0340, bbox_mAP_50: 0.0740, bbox_mAP_75: 0.0260, bbox_mAP_s: 0.0030, bbox_mAP_m: 0.0360, bbox_mAP_l: 0.0360, bbox_mAP_copypaste: 0.034 0.074 0.026 0.003 0.036 0.036
2023-02-03 02:35:18,529 - mmdet - INFO - Epoch [3][50/834]	lr: 2.500e-05, eta: 1:30:54, time: 0.923, data_time: 0.272, memory: 5033, stage0_loss_cls: 0.9708, stage0_pos_acc: 50.2067, stage0_loss_bbox: 0.8400, stage0_loss_iou: 1.5109, stage1_loss_cls: 0.7006, stage1_pos_acc: 68.2198, stage1_loss_bbox: 0.4159, stage1_loss_iou: 0.9777, stage2_loss_cls: 0.6887, stage2_pos_acc: 70.0350, stage2_loss_bbox: 0.3516, stage2_loss_iou: 0.8239, stage3_loss_cls: 0.6944, stage3_pos_acc: 71.5422, stage3_loss_bbox: 0.3351, stage3_loss_iou: 0.7877, stage4_loss_cls: 0.7072, stage4_pos_acc: 72.1984, stage4_loss_bbox: 0.3318, stage4_loss_iou: 0.7815, stage5_loss_cls: 0.7205, stage5_pos_acc: 72.0238, stage5_loss_bbox: 0.3291, stage5_loss_iou: 0.7734, loss: 12.7407, grad_norm: 56.0061
2023-02-03 02:35:51,458 - mmdet - INFO - Epoch [3][100/834]	lr: 2.500e-05, eta: 1:30:21, time: 0.659, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9748, stage0_pos_acc: 48.4189, stage0_loss_bbox: 0.8917, stage0_loss_iou: 1.5309, stage1_loss_cls: 0.7083, stage1_pos_acc: 66.5060, stage1_loss_bbox: 0.4388, stage1_loss_iou: 1.0166, stage2_loss_cls: 0.6841, stage2_pos_acc: 70.4048, stage2_loss_bbox: 0.3636, stage2_loss_iou: 0.8541, stage3_loss_cls: 0.6834, stage3_pos_acc: 71.2693, stage3_loss_bbox: 0.3531, stage3_loss_iou: 0.8272, stage4_loss_cls: 0.6946, stage4_pos_acc: 70.5408, stage4_loss_bbox: 0.3466, stage4_loss_iou: 0.8163, stage5_loss_cls: 0.7062, stage5_pos_acc: 71.3824, stage5_loss_bbox: 0.3452, stage5_loss_iou: 0.8121, loss: 13.0477, grad_norm: 53.8305
2023-02-03 02:36:24,504 - mmdet - INFO - Epoch [3][150/834]	lr: 2.500e-05, eta: 1:29:49, time: 0.661, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9319, stage0_pos_acc: 50.4478, stage0_loss_bbox: 0.8328, stage0_loss_iou: 1.4787, stage1_loss_cls: 0.7030, stage1_pos_acc: 68.6826, stage1_loss_bbox: 0.4269, stage1_loss_iou: 0.9659, stage2_loss_cls: 0.6781, stage2_pos_acc: 69.7328, stage2_loss_bbox: 0.3528, stage2_loss_iou: 0.8143, stage3_loss_cls: 0.6781, stage3_pos_acc: 70.5269, stage3_loss_bbox: 0.3363, stage3_loss_iou: 0.7821, stage4_loss_cls: 0.6970, stage4_pos_acc: 69.6483, stage4_loss_bbox: 0.3288, stage4_loss_iou: 0.7688, stage5_loss_cls: 0.7123, stage5_pos_acc: 70.1327, stage5_loss_bbox: 0.3260, stage5_loss_iou: 0.7663, loss: 12.5800, grad_norm: 55.2661
2023-02-03 02:36:57,449 - mmdet - INFO - Epoch [3][200/834]	lr: 2.500e-05, eta: 1:29:16, time: 0.659, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9204, stage0_pos_acc: 51.1725, stage0_loss_bbox: 0.8310, stage0_loss_iou: 1.4926, stage1_loss_cls: 0.7047, stage1_pos_acc: 68.0189, stage1_loss_bbox: 0.4137, stage1_loss_iou: 0.9699, stage2_loss_cls: 0.6901, stage2_pos_acc: 69.5910, stage2_loss_bbox: 0.3491, stage2_loss_iou: 0.8246, stage3_loss_cls: 0.6905, stage3_pos_acc: 69.9135, stage3_loss_bbox: 0.3346, stage3_loss_iou: 0.7917, stage4_loss_cls: 0.7073, stage4_pos_acc: 70.5120, stage4_loss_bbox: 0.3341, stage4_loss_iou: 0.7919, stage5_loss_cls: 0.7146, stage5_pos_acc: 70.7930, stage5_loss_bbox: 0.3328, stage5_loss_iou: 0.7853, loss: 12.6791, grad_norm: 51.9161
2023-02-03 02:37:30,622 - mmdet - INFO - Epoch [3][250/834]	lr: 2.500e-05, eta: 1:28:44, time: 0.663, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9269, stage0_pos_acc: 51.7194, stage0_loss_bbox: 0.8088, stage0_loss_iou: 1.4842, stage1_loss_cls: 0.7031, stage1_pos_acc: 68.8665, stage1_loss_bbox: 0.4076, stage1_loss_iou: 0.9644, stage2_loss_cls: 0.6816, stage2_pos_acc: 71.7829, stage2_loss_bbox: 0.3465, stage2_loss_iou: 0.8114, stage3_loss_cls: 0.6871, stage3_pos_acc: 70.5713, stage3_loss_bbox: 0.3352, stage3_loss_iou: 0.7861, stage4_loss_cls: 0.7016, stage4_pos_acc: 70.1106, stage4_loss_bbox: 0.3308, stage4_loss_iou: 0.7742, stage5_loss_cls: 0.7085, stage5_pos_acc: 71.2458, stage5_loss_bbox: 0.3305, stage5_loss_iou: 0.7713, loss: 12.5598, grad_norm: 50.7597
2023-02-03 02:38:03,536 - mmdet - INFO - Epoch [3][300/834]	lr: 2.500e-05, eta: 1:28:11, time: 0.658, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9574, stage0_pos_acc: 50.7775, stage0_loss_bbox: 0.8239, stage0_loss_iou: 1.4456, stage1_loss_cls: 0.7305, stage1_pos_acc: 66.1164, stage1_loss_bbox: 0.4018, stage1_loss_iou: 0.9107, stage2_loss_cls: 0.7100, stage2_pos_acc: 68.5020, stage2_loss_bbox: 0.3343, stage2_loss_iou: 0.7710, stage3_loss_cls: 0.7174, stage3_pos_acc: 68.4697, stage3_loss_bbox: 0.3205, stage3_loss_iou: 0.7434, stage4_loss_cls: 0.7326, stage4_pos_acc: 68.0520, stage4_loss_bbox: 0.3172, stage4_loss_iou: 0.7335, stage5_loss_cls: 0.7476, stage5_pos_acc: 69.6553, stage5_loss_bbox: 0.3135, stage5_loss_iou: 0.7297, loss: 12.4404, grad_norm: 51.9319
2023-02-03 02:38:36,321 - mmdet - INFO - Epoch [3][350/834]	lr: 2.500e-05, eta: 1:27:38, time: 0.656, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9304, stage0_pos_acc: 52.5623, stage0_loss_bbox: 0.8264, stage0_loss_iou: 1.4498, stage1_loss_cls: 0.6904, stage1_pos_acc: 68.3549, stage1_loss_bbox: 0.4093, stage1_loss_iou: 0.9377, stage2_loss_cls: 0.6737, stage2_pos_acc: 70.9592, stage2_loss_bbox: 0.3463, stage2_loss_iou: 0.7929, stage3_loss_cls: 0.6817, stage3_pos_acc: 70.6148, stage3_loss_bbox: 0.3319, stage3_loss_iou: 0.7609, stage4_loss_cls: 0.6979, stage4_pos_acc: 71.1623, stage4_loss_bbox: 0.3271, stage4_loss_iou: 0.7493, stage5_loss_cls: 0.7097, stage5_pos_acc: 70.6274, stage5_loss_bbox: 0.3283, stage5_loss_iou: 0.7514, loss: 12.3950, grad_norm: 53.3910
2023-02-03 02:39:09,261 - mmdet - INFO - Epoch [3][400/834]	lr: 2.500e-05, eta: 1:27:05, time: 0.659, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9544, stage0_pos_acc: 50.0382, stage0_loss_bbox: 0.7949, stage0_loss_iou: 1.4654, stage1_loss_cls: 0.6971, stage1_pos_acc: 66.4079, stage1_loss_bbox: 0.3895, stage1_loss_iou: 0.9066, stage2_loss_cls: 0.6795, stage2_pos_acc: 68.4804, stage2_loss_bbox: 0.3271, stage2_loss_iou: 0.7648, stage3_loss_cls: 0.6830, stage3_pos_acc: 70.5791, stage3_loss_bbox: 0.3153, stage3_loss_iou: 0.7407, stage4_loss_cls: 0.6989, stage4_pos_acc: 70.0726, stage4_loss_bbox: 0.3129, stage4_loss_iou: 0.7327, stage5_loss_cls: 0.7137, stage5_pos_acc: 69.0137, stage5_loss_bbox: 0.3106, stage5_loss_iou: 0.7257, loss: 12.2125, grad_norm: 54.8338
2023-02-03 02:39:42,101 - mmdet - INFO - Epoch [3][450/834]	lr: 2.500e-05, eta: 1:26:32, time: 0.657, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9275, stage0_pos_acc: 53.2807, stage0_loss_bbox: 0.7931, stage0_loss_iou: 1.4617, stage1_loss_cls: 0.6955, stage1_pos_acc: 67.5414, stage1_loss_bbox: 0.4092, stage1_loss_iou: 0.9684, stage2_loss_cls: 0.6658, stage2_pos_acc: 71.6049, stage2_loss_bbox: 0.3479, stage2_loss_iou: 0.8297, stage3_loss_cls: 0.6767, stage3_pos_acc: 71.0244, stage3_loss_bbox: 0.3361, stage3_loss_iou: 0.8002, stage4_loss_cls: 0.6858, stage4_pos_acc: 71.5396, stage4_loss_bbox: 0.3361, stage4_loss_iou: 0.7936, stage5_loss_cls: 0.6999, stage5_pos_acc: 71.2742, stage5_loss_bbox: 0.3357, stage5_loss_iou: 0.7924, loss: 12.5553, grad_norm: 53.9736
2023-02-03 02:40:15,029 - mmdet - INFO - Epoch [3][500/834]	lr: 2.500e-05, eta: 1:25:59, time: 0.659, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9387, stage0_pos_acc: 52.3402, stage0_loss_bbox: 0.8219, stage0_loss_iou: 1.4603, stage1_loss_cls: 0.6983, stage1_pos_acc: 68.1362, stage1_loss_bbox: 0.4007, stage1_loss_iou: 0.9278, stage2_loss_cls: 0.6767, stage2_pos_acc: 70.6195, stage2_loss_bbox: 0.3346, stage2_loss_iou: 0.7882, stage3_loss_cls: 0.6841, stage3_pos_acc: 71.5723, stage3_loss_bbox: 0.3249, stage3_loss_iou: 0.7659, stage4_loss_cls: 0.6883, stage4_pos_acc: 71.8810, stage4_loss_bbox: 0.3218, stage4_loss_iou: 0.7594, stage5_loss_cls: 0.7035, stage5_pos_acc: 71.9035, stage5_loss_bbox: 0.3223, stage5_loss_iou: 0.7559, loss: 12.3734, grad_norm: 52.8775
2023-02-03 02:40:47,927 - mmdet - INFO - Epoch [3][550/834]	lr: 2.500e-05, eta: 1:25:26, time: 0.658, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9438, stage0_pos_acc: 53.4115, stage0_loss_bbox: 0.8308, stage0_loss_iou: 1.4547, stage1_loss_cls: 0.7055, stage1_pos_acc: 67.9046, stage1_loss_bbox: 0.4032, stage1_loss_iou: 0.9164, stage2_loss_cls: 0.6843, stage2_pos_acc: 70.6251, stage2_loss_bbox: 0.3347, stage2_loss_iou: 0.7705, stage3_loss_cls: 0.6832, stage3_pos_acc: 69.7727, stage3_loss_bbox: 0.3255, stage3_loss_iou: 0.7446, stage4_loss_cls: 0.6954, stage4_pos_acc: 70.6397, stage4_loss_bbox: 0.3212, stage4_loss_iou: 0.7394, stage5_loss_cls: 0.7131, stage5_pos_acc: 71.1669, stage5_loss_bbox: 0.3210, stage5_loss_iou: 0.7356, loss: 12.3229, grad_norm: 52.5814
2023-02-03 02:41:20,868 - mmdet - INFO - Epoch [3][600/834]	lr: 2.500e-05, eta: 1:24:53, time: 0.659, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9339, stage0_pos_acc: 54.9792, stage0_loss_bbox: 0.8100, stage0_loss_iou: 1.4696, stage1_loss_cls: 0.6846, stage1_pos_acc: 71.8742, stage1_loss_bbox: 0.3930, stage1_loss_iou: 0.9191, stage2_loss_cls: 0.6660, stage2_pos_acc: 72.5691, stage2_loss_bbox: 0.3342, stage2_loss_iou: 0.7837, stage3_loss_cls: 0.6679, stage3_pos_acc: 72.5334, stage3_loss_bbox: 0.3232, stage3_loss_iou: 0.7628, stage4_loss_cls: 0.6824, stage4_pos_acc: 71.9418, stage4_loss_bbox: 0.3187, stage4_loss_iou: 0.7548, stage5_loss_cls: 0.7008, stage5_pos_acc: 72.1255, stage5_loss_bbox: 0.3136, stage5_loss_iou: 0.7469, loss: 12.2652, grad_norm: 52.9935
2023-02-03 02:41:53,639 - mmdet - INFO - Epoch [3][650/834]	lr: 2.500e-05, eta: 1:24:20, time: 0.655, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9133, stage0_pos_acc: 55.6016, stage0_loss_bbox: 0.8342, stage0_loss_iou: 1.5180, stage1_loss_cls: 0.6868, stage1_pos_acc: 72.0591, stage1_loss_bbox: 0.3969, stage1_loss_iou: 0.9848, stage2_loss_cls: 0.6657, stage2_pos_acc: 73.6393, stage2_loss_bbox: 0.3391, stage2_loss_iou: 0.8314, stage3_loss_cls: 0.6652, stage3_pos_acc: 74.2072, stage3_loss_bbox: 0.3238, stage3_loss_iou: 0.7993, stage4_loss_cls: 0.6703, stage4_pos_acc: 75.3525, stage4_loss_bbox: 0.3188, stage4_loss_iou: 0.7882, stage5_loss_cls: 0.6839, stage5_pos_acc: 75.5147, stage5_loss_bbox: 0.3131, stage5_loss_iou: 0.7780, loss: 12.5110, grad_norm: 49.8622
2023-02-03 02:42:26,640 - mmdet - INFO - Epoch [3][700/834]	lr: 2.500e-05, eta: 1:23:47, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9274, stage0_pos_acc: 56.0789, stage0_loss_bbox: 0.8026, stage0_loss_iou: 1.4680, stage1_loss_cls: 0.6965, stage1_pos_acc: 69.6619, stage1_loss_bbox: 0.3819, stage1_loss_iou: 0.9177, stage2_loss_cls: 0.6723, stage2_pos_acc: 71.1041, stage2_loss_bbox: 0.3232, stage2_loss_iou: 0.7768, stage3_loss_cls: 0.6785, stage3_pos_acc: 70.9498, stage3_loss_bbox: 0.3149, stage3_loss_iou: 0.7523, stage4_loss_cls: 0.6943, stage4_pos_acc: 71.0329, stage4_loss_bbox: 0.3128, stage4_loss_iou: 0.7474, stage5_loss_cls: 0.7097, stage5_pos_acc: 70.8576, stage5_loss_bbox: 0.3060, stage5_loss_iou: 0.7367, loss: 12.2191, grad_norm: 52.8685
2023-02-03 02:42:59,511 - mmdet - INFO - Epoch [3][750/834]	lr: 2.500e-05, eta: 1:23:14, time: 0.657, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9071, stage0_pos_acc: 56.7937, stage0_loss_bbox: 0.7938, stage0_loss_iou: 1.4809, stage1_loss_cls: 0.6713, stage1_pos_acc: 71.6147, stage1_loss_bbox: 0.3967, stage1_loss_iou: 0.9283, stage2_loss_cls: 0.6474, stage2_pos_acc: 73.3221, stage2_loss_bbox: 0.3311, stage2_loss_iou: 0.7870, stage3_loss_cls: 0.6530, stage3_pos_acc: 73.9943, stage3_loss_bbox: 0.3223, stage3_loss_iou: 0.7679, stage4_loss_cls: 0.6616, stage4_pos_acc: 74.1612, stage4_loss_bbox: 0.3199, stage4_loss_iou: 0.7635, stage5_loss_cls: 0.6843, stage5_pos_acc: 73.8735, stage5_loss_bbox: 0.3187, stage5_loss_iou: 0.7570, loss: 12.1919, grad_norm: 54.3955
2023-02-03 02:43:32,423 - mmdet - INFO - Epoch [3][800/834]	lr: 2.500e-05, eta: 1:22:42, time: 0.659, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9056, stage0_pos_acc: 60.5850, stage0_loss_bbox: 0.7940, stage0_loss_iou: 1.4723, stage1_loss_cls: 0.6693, stage1_pos_acc: 71.7013, stage1_loss_bbox: 0.3731, stage1_loss_iou: 0.8958, stage2_loss_cls: 0.6419, stage2_pos_acc: 73.9980, stage2_loss_bbox: 0.3149, stage2_loss_iou: 0.7563, stage3_loss_cls: 0.6388, stage3_pos_acc: 73.7852, stage3_loss_bbox: 0.3010, stage3_loss_iou: 0.7307, stage4_loss_cls: 0.6463, stage4_pos_acc: 76.2039, stage4_loss_bbox: 0.3005, stage4_loss_iou: 0.7259, stage5_loss_cls: 0.6723, stage5_pos_acc: 76.2054, stage5_loss_bbox: 0.2980, stage5_loss_iou: 0.7240, loss: 11.8606, grad_norm: 52.7569
2023-02-03 02:44:23,593 - mmdet - INFO - Evaluating bbox...
2023-02-03 02:44:35,398 - mmdet - INFO - Exp name: sparse_rcnn_r50_fpn_1x_coco_comparison.py
2023-02-03 02:44:35,399 - mmdet - INFO - Epoch(val) [3][186]	bbox_mAP: 0.0570, bbox_mAP_50: 0.1330, bbox_mAP_75: 0.0410, bbox_mAP_s: 0.0020, bbox_mAP_m: 0.0350, bbox_mAP_l: 0.0610, bbox_mAP_copypaste: 0.057 0.133 0.041 0.002 0.035 0.061
2023-02-03 02:45:21,841 - mmdet - INFO - Epoch [4][50/834]	lr: 2.500e-05, eta: 1:21:20, time: 0.929, data_time: 0.270, memory: 5033, stage0_loss_cls: 0.9421, stage0_pos_acc: 56.6981, stage0_loss_bbox: 0.8047, stage0_loss_iou: 1.4627, stage1_loss_cls: 0.7044, stage1_pos_acc: 68.4082, stage1_loss_bbox: 0.3793, stage1_loss_iou: 0.8934, stage2_loss_cls: 0.6707, stage2_pos_acc: 71.6117, stage2_loss_bbox: 0.3246, stage2_loss_iou: 0.7607, stage3_loss_cls: 0.6731, stage3_pos_acc: 72.0933, stage3_loss_bbox: 0.3160, stage3_loss_iou: 0.7388, stage4_loss_cls: 0.6773, stage4_pos_acc: 73.2477, stage4_loss_bbox: 0.3110, stage4_loss_iou: 0.7274, stage5_loss_cls: 0.6920, stage5_pos_acc: 73.0220, stage5_loss_bbox: 0.3098, stage5_loss_iou: 0.7219, loss: 12.1098, grad_norm: 53.9533
2023-02-03 02:45:54,967 - mmdet - INFO - Epoch [4][100/834]	lr: 2.500e-05, eta: 1:20:49, time: 0.663, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8802, stage0_pos_acc: 58.8200, stage0_loss_bbox: 0.7977, stage0_loss_iou: 1.4952, stage1_loss_cls: 0.6471, stage1_pos_acc: 73.4748, stage1_loss_bbox: 0.3874, stage1_loss_iou: 0.9404, stage2_loss_cls: 0.6336, stage2_pos_acc: 76.8256, stage2_loss_bbox: 0.3223, stage2_loss_iou: 0.7963, stage3_loss_cls: 0.6339, stage3_pos_acc: 77.1024, stage3_loss_bbox: 0.3091, stage3_loss_iou: 0.7691, stage4_loss_cls: 0.6409, stage4_pos_acc: 77.2795, stage4_loss_bbox: 0.3064, stage4_loss_iou: 0.7609, stage5_loss_cls: 0.6600, stage5_pos_acc: 76.7961, stage5_loss_bbox: 0.3049, stage5_loss_iou: 0.7578, loss: 12.0433, grad_norm: 52.1875
2023-02-03 02:46:27,820 - mmdet - INFO - Epoch [4][150/834]	lr: 2.500e-05, eta: 1:20:16, time: 0.657, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9196, stage0_pos_acc: 57.0592, stage0_loss_bbox: 0.8244, stage0_loss_iou: 1.4637, stage1_loss_cls: 0.6744, stage1_pos_acc: 69.5503, stage1_loss_bbox: 0.3992, stage1_loss_iou: 0.9124, stage2_loss_cls: 0.6426, stage2_pos_acc: 74.8361, stage2_loss_bbox: 0.3329, stage2_loss_iou: 0.7608, stage3_loss_cls: 0.6402, stage3_pos_acc: 74.6260, stage3_loss_bbox: 0.3223, stage3_loss_iou: 0.7349, stage4_loss_cls: 0.6460, stage4_pos_acc: 75.9786, stage4_loss_bbox: 0.3149, stage4_loss_iou: 0.7204, stage5_loss_cls: 0.6606, stage5_pos_acc: 76.7824, stage5_loss_bbox: 0.3116, stage5_loss_iou: 0.7140, loss: 11.9947, grad_norm: 50.2711
2023-02-03 02:47:00,609 - mmdet - INFO - Epoch [4][200/834]	lr: 2.500e-05, eta: 1:19:44, time: 0.656, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9414, stage0_pos_acc: 56.4268, stage0_loss_bbox: 0.8101, stage0_loss_iou: 1.4371, stage1_loss_cls: 0.7041, stage1_pos_acc: 71.5237, stage1_loss_bbox: 0.3918, stage1_loss_iou: 0.8884, stage2_loss_cls: 0.6752, stage2_pos_acc: 73.1928, stage2_loss_bbox: 0.3326, stage2_loss_iou: 0.7541, stage3_loss_cls: 0.6699, stage3_pos_acc: 73.0082, stage3_loss_bbox: 0.3192, stage3_loss_iou: 0.7313, stage4_loss_cls: 0.6712, stage4_pos_acc: 74.1584, stage4_loss_bbox: 0.3175, stage4_loss_iou: 0.7250, stage5_loss_cls: 0.6907, stage5_pos_acc: 72.9635, stage5_loss_bbox: 0.3147, stage5_loss_iou: 0.7201, loss: 12.0944, grad_norm: 57.8928
2023-02-03 02:47:33,555 - mmdet - INFO - Epoch [4][250/834]	lr: 2.500e-05, eta: 1:19:12, time: 0.659, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9266, stage0_pos_acc: 55.9100, stage0_loss_bbox: 0.7999, stage0_loss_iou: 1.4329, stage1_loss_cls: 0.6788, stage1_pos_acc: 71.6329, stage1_loss_bbox: 0.3879, stage1_loss_iou: 0.8928, stage2_loss_cls: 0.6595, stage2_pos_acc: 72.8442, stage2_loss_bbox: 0.3275, stage2_loss_iou: 0.7475, stage3_loss_cls: 0.6536, stage3_pos_acc: 73.9455, stage3_loss_bbox: 0.3195, stage3_loss_iou: 0.7272, stage4_loss_cls: 0.6597, stage4_pos_acc: 73.9541, stage4_loss_bbox: 0.3146, stage4_loss_iou: 0.7207, stage5_loss_cls: 0.6759, stage5_pos_acc: 74.1409, stage5_loss_bbox: 0.3106, stage5_loss_iou: 0.7111, loss: 11.9460, grad_norm: 52.5098
2023-02-03 02:48:06,759 - mmdet - INFO - Epoch [4][300/834]	lr: 2.500e-05, eta: 1:18:40, time: 0.664, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8984, stage0_pos_acc: 56.0175, stage0_loss_bbox: 0.8031, stage0_loss_iou: 1.4923, stage1_loss_cls: 0.6733, stage1_pos_acc: 70.2130, stage1_loss_bbox: 0.3969, stage1_loss_iou: 0.9557, stage2_loss_cls: 0.6498, stage2_pos_acc: 72.1407, stage2_loss_bbox: 0.3414, stage2_loss_iou: 0.8177, stage3_loss_cls: 0.6494, stage3_pos_acc: 72.8854, stage3_loss_bbox: 0.3359, stage3_loss_iou: 0.7993, stage4_loss_cls: 0.6511, stage4_pos_acc: 74.0766, stage4_loss_bbox: 0.3297, stage4_loss_iou: 0.7876, stage5_loss_cls: 0.6648, stage5_pos_acc: 72.5686, stage5_loss_bbox: 0.3262, stage5_loss_iou: 0.7798, loss: 12.3525, grad_norm: 49.3037
2023-02-03 02:48:39,991 - mmdet - INFO - Epoch [4][350/834]	lr: 2.500e-05, eta: 1:18:08, time: 0.665, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.9183, stage0_pos_acc: 58.3665, stage0_loss_bbox: 0.8078, stage0_loss_iou: 1.4416, stage1_loss_cls: 0.6791, stage1_pos_acc: 70.8368, stage1_loss_bbox: 0.3855, stage1_loss_iou: 0.9003, stage2_loss_cls: 0.6499, stage2_pos_acc: 73.6747, stage2_loss_bbox: 0.3217, stage2_loss_iou: 0.7606, stage3_loss_cls: 0.6463, stage3_pos_acc: 74.2177, stage3_loss_bbox: 0.3147, stage3_loss_iou: 0.7408, stage4_loss_cls: 0.6564, stage4_pos_acc: 73.5085, stage4_loss_bbox: 0.3148, stage4_loss_iou: 0.7372, stage5_loss_cls: 0.6696, stage5_pos_acc: 74.9648, stage5_loss_bbox: 0.3173, stage5_loss_iou: 0.7398, loss: 12.0016, grad_norm: 54.0256
2023-02-03 02:49:12,772 - mmdet - INFO - Epoch [4][400/834]	lr: 2.500e-05, eta: 1:17:36, time: 0.656, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8797, stage0_pos_acc: 60.9574, stage0_loss_bbox: 0.8116, stage0_loss_iou: 1.4583, stage1_loss_cls: 0.6625, stage1_pos_acc: 72.3396, stage1_loss_bbox: 0.4077, stage1_loss_iou: 0.9188, stage2_loss_cls: 0.6387, stage2_pos_acc: 76.4358, stage2_loss_bbox: 0.3469, stage2_loss_iou: 0.7893, stage3_loss_cls: 0.6404, stage3_pos_acc: 74.9319, stage3_loss_bbox: 0.3400, stage3_loss_iou: 0.7654, stage4_loss_cls: 0.6424, stage4_pos_acc: 75.9369, stage4_loss_bbox: 0.3403, stage4_loss_iou: 0.7647, stage5_loss_cls: 0.6557, stage5_pos_acc: 74.7755, stage5_loss_bbox: 0.3367, stage5_loss_iou: 0.7604, loss: 12.1596, grad_norm: 49.8483
2023-02-03 02:49:45,640 - mmdet - INFO - Epoch [4][450/834]	lr: 2.500e-05, eta: 1:17:03, time: 0.657, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8752, stage0_pos_acc: 62.7487, stage0_loss_bbox: 0.8036, stage0_loss_iou: 1.5065, stage1_loss_cls: 0.6576, stage1_pos_acc: 75.2337, stage1_loss_bbox: 0.3866, stage1_loss_iou: 0.9207, stage2_loss_cls: 0.6385, stage2_pos_acc: 77.0491, stage2_loss_bbox: 0.3297, stage2_loss_iou: 0.7833, stage3_loss_cls: 0.6300, stage3_pos_acc: 77.7415, stage3_loss_bbox: 0.3216, stage3_loss_iou: 0.7692, stage4_loss_cls: 0.6326, stage4_pos_acc: 78.4020, stage4_loss_bbox: 0.3216, stage4_loss_iou: 0.7657, stage5_loss_cls: 0.6416, stage5_pos_acc: 78.0407, stage5_loss_bbox: 0.3188, stage5_loss_iou: 0.7590, loss: 12.0618, grad_norm: 51.4820
2023-02-03 02:50:18,573 - mmdet - INFO - Epoch [4][500/834]	lr: 2.500e-05, eta: 1:16:31, time: 0.659, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.9024, stage0_pos_acc: 56.8884, stage0_loss_bbox: 0.7993, stage0_loss_iou: 1.4681, stage1_loss_cls: 0.6729, stage1_pos_acc: 66.9711, stage1_loss_bbox: 0.3828, stage1_loss_iou: 0.8964, stage2_loss_cls: 0.6477, stage2_pos_acc: 73.5729, stage2_loss_bbox: 0.3266, stage2_loss_iou: 0.7545, stage3_loss_cls: 0.6480, stage3_pos_acc: 73.3911, stage3_loss_bbox: 0.3162, stage3_loss_iou: 0.7347, stage4_loss_cls: 0.6517, stage4_pos_acc: 73.5781, stage4_loss_bbox: 0.3188, stage4_loss_iou: 0.7336, stage5_loss_cls: 0.6618, stage5_pos_acc: 73.2916, stage5_loss_bbox: 0.3165, stage5_loss_iou: 0.7279, loss: 11.9600, grad_norm: 51.8244
2023-02-03 02:50:51,858 - mmdet - INFO - Epoch [4][550/834]	lr: 2.500e-05, eta: 1:15:59, time: 0.666, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8592, stage0_pos_acc: 60.9696, stage0_loss_bbox: 0.7857, stage0_loss_iou: 1.4966, stage1_loss_cls: 0.6223, stage1_pos_acc: 73.2884, stage1_loss_bbox: 0.3627, stage1_loss_iou: 0.9073, stage2_loss_cls: 0.5954, stage2_pos_acc: 74.6189, stage2_loss_bbox: 0.3058, stage2_loss_iou: 0.7603, stage3_loss_cls: 0.5943, stage3_pos_acc: 75.8444, stage3_loss_bbox: 0.2937, stage3_loss_iou: 0.7366, stage4_loss_cls: 0.5935, stage4_pos_acc: 76.0205, stage4_loss_bbox: 0.2940, stage4_loss_iou: 0.7348, stage5_loss_cls: 0.6058, stage5_pos_acc: 76.1107, stage5_loss_bbox: 0.2926, stage5_loss_iou: 0.7323, loss: 11.5729, grad_norm: 52.9761
2023-02-03 02:51:24,775 - mmdet - INFO - Epoch [4][600/834]	lr: 2.500e-05, eta: 1:15:27, time: 0.658, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8395, stage0_pos_acc: 64.2608, stage0_loss_bbox: 0.8102, stage0_loss_iou: 1.4944, stage1_loss_cls: 0.6317, stage1_pos_acc: 74.2074, stage1_loss_bbox: 0.3931, stage1_loss_iou: 0.9298, stage2_loss_cls: 0.6153, stage2_pos_acc: 76.0283, stage2_loss_bbox: 0.3364, stage2_loss_iou: 0.7931, stage3_loss_cls: 0.6148, stage3_pos_acc: 77.7793, stage3_loss_bbox: 0.3274, stage3_loss_iou: 0.7744, stage4_loss_cls: 0.6198, stage4_pos_acc: 76.8462, stage4_loss_bbox: 0.3261, stage4_loss_iou: 0.7693, stage5_loss_cls: 0.6244, stage5_pos_acc: 76.1186, stage5_loss_bbox: 0.3245, stage5_loss_iou: 0.7659, loss: 11.9901, grad_norm: 50.7629
2023-02-03 02:51:57,660 - mmdet - INFO - Epoch [4][650/834]	lr: 2.500e-05, eta: 1:14:54, time: 0.658, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8447, stage0_pos_acc: 61.8261, stage0_loss_bbox: 0.7886, stage0_loss_iou: 1.4891, stage1_loss_cls: 0.6284, stage1_pos_acc: 72.8044, stage1_loss_bbox: 0.3767, stage1_loss_iou: 0.9269, stage2_loss_cls: 0.6153, stage2_pos_acc: 75.3884, stage2_loss_bbox: 0.3198, stage2_loss_iou: 0.7867, stage3_loss_cls: 0.6108, stage3_pos_acc: 77.3895, stage3_loss_bbox: 0.3111, stage3_loss_iou: 0.7615, stage4_loss_cls: 0.6117, stage4_pos_acc: 77.4655, stage4_loss_bbox: 0.3120, stage4_loss_iou: 0.7594, stage5_loss_cls: 0.6251, stage5_pos_acc: 77.0061, stage5_loss_bbox: 0.3077, stage5_loss_iou: 0.7537, loss: 11.8290, grad_norm: 49.9039
2023-02-03 02:52:30,497 - mmdet - INFO - Epoch [4][700/834]	lr: 2.500e-05, eta: 1:14:22, time: 0.657, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8944, stage0_pos_acc: 57.8826, stage0_loss_bbox: 0.7665, stage0_loss_iou: 1.4302, stage1_loss_cls: 0.6547, stage1_pos_acc: 68.3045, stage1_loss_bbox: 0.3583, stage1_loss_iou: 0.8559, stage2_loss_cls: 0.6332, stage2_pos_acc: 70.4210, stage2_loss_bbox: 0.3032, stage2_loss_iou: 0.7305, stage3_loss_cls: 0.6302, stage3_pos_acc: 71.4718, stage3_loss_bbox: 0.2946, stage3_loss_iou: 0.7092, stage4_loss_cls: 0.6324, stage4_pos_acc: 72.3149, stage4_loss_bbox: 0.2930, stage4_loss_iou: 0.7033, stage5_loss_cls: 0.6431, stage5_pos_acc: 71.8050, stage5_loss_bbox: 0.2934, stage5_loss_iou: 0.7032, loss: 11.5293, grad_norm: 50.3785
2023-02-03 02:53:03,170 - mmdet - INFO - Epoch [4][750/834]	lr: 2.500e-05, eta: 1:13:49, time: 0.653, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8698, stage0_pos_acc: 64.5254, stage0_loss_bbox: 0.7774, stage0_loss_iou: 1.4430, stage1_loss_cls: 0.6376, stage1_pos_acc: 73.1719, stage1_loss_bbox: 0.3550, stage1_loss_iou: 0.8512, stage2_loss_cls: 0.6105, stage2_pos_acc: 76.8942, stage2_loss_bbox: 0.3013, stage2_loss_iou: 0.7213, stage3_loss_cls: 0.5987, stage3_pos_acc: 78.4849, stage3_loss_bbox: 0.2919, stage3_loss_iou: 0.7012, stage4_loss_cls: 0.6028, stage4_pos_acc: 78.3427, stage4_loss_bbox: 0.2922, stage4_loss_iou: 0.7010, stage5_loss_cls: 0.6124, stage5_pos_acc: 78.2715, stage5_loss_bbox: 0.2859, stage5_loss_iou: 0.6953, loss: 11.3486, grad_norm: 49.5651
2023-02-03 02:53:35,880 - mmdet - INFO - Epoch [4][800/834]	lr: 2.500e-05, eta: 1:13:16, time: 0.654, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8837, stage0_pos_acc: 63.9339, stage0_loss_bbox: 0.7931, stage0_loss_iou: 1.4529, stage1_loss_cls: 0.6414, stage1_pos_acc: 74.0184, stage1_loss_bbox: 0.3666, stage1_loss_iou: 0.8806, stage2_loss_cls: 0.6133, stage2_pos_acc: 76.8405, stage2_loss_bbox: 0.3130, stage2_loss_iou: 0.7458, stage3_loss_cls: 0.6061, stage3_pos_acc: 77.3458, stage3_loss_bbox: 0.3020, stage3_loss_iou: 0.7282, stage4_loss_cls: 0.6069, stage4_pos_acc: 78.6365, stage4_loss_bbox: 0.3001, stage4_loss_iou: 0.7216, stage5_loss_cls: 0.6129, stage5_pos_acc: 78.2823, stage5_loss_bbox: 0.2972, stage5_loss_iou: 0.7171, loss: 11.5824, grad_norm: 51.3721
2023-02-03 02:53:58,797 - mmdet - INFO - Saving checkpoint at 4 epochs
2023-02-03 02:54:30,082 - mmdet - INFO - Evaluating bbox...
2023-02-03 02:54:42,677 - mmdet - INFO - Exp name: sparse_rcnn_r50_fpn_1x_coco_comparison.py
2023-02-03 02:54:42,678 - mmdet - INFO - Epoch(val) [4][186]	bbox_mAP: 0.0770, bbox_mAP_50: 0.1820, bbox_mAP_75: 0.0560, bbox_mAP_s: 0.0010, bbox_mAP_m: 0.0570, bbox_mAP_l: 0.0820, bbox_mAP_copypaste: 0.077 0.182 0.056 0.001 0.057 0.082
2023-02-03 02:55:28,960 - mmdet - INFO - Epoch [5][50/834]	lr: 2.500e-05, eta: 1:12:03, time: 0.925, data_time: 0.269, memory: 5033, stage0_loss_cls: 0.8641, stage0_pos_acc: 63.9115, stage0_loss_bbox: 0.7774, stage0_loss_iou: 1.4734, stage1_loss_cls: 0.6284, stage1_pos_acc: 75.7416, stage1_loss_bbox: 0.3593, stage1_loss_iou: 0.8679, stage2_loss_cls: 0.6102, stage2_pos_acc: 76.1637, stage2_loss_bbox: 0.3013, stage2_loss_iou: 0.7336, stage3_loss_cls: 0.6092, stage3_pos_acc: 77.2033, stage3_loss_bbox: 0.2934, stage3_loss_iou: 0.7111, stage4_loss_cls: 0.6090, stage4_pos_acc: 77.8989, stage4_loss_bbox: 0.2942, stage4_loss_iou: 0.7115, stage5_loss_cls: 0.6180, stage5_pos_acc: 77.1083, stage5_loss_bbox: 0.2916, stage5_loss_iou: 0.7051, loss: 11.4589, grad_norm: 53.9209
2023-02-03 02:56:01,943 - mmdet - INFO - Epoch [5][100/834]	lr: 2.500e-05, eta: 1:11:31, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8979, stage0_pos_acc: 59.7967, stage0_loss_bbox: 0.8092, stage0_loss_iou: 1.4429, stage1_loss_cls: 0.6601, stage1_pos_acc: 71.3333, stage1_loss_bbox: 0.3880, stage1_loss_iou: 0.8614, stage2_loss_cls: 0.6308, stage2_pos_acc: 74.9349, stage2_loss_bbox: 0.3292, stage2_loss_iou: 0.7354, stage3_loss_cls: 0.6228, stage3_pos_acc: 74.9553, stage3_loss_bbox: 0.3202, stage3_loss_iou: 0.7151, stage4_loss_cls: 0.6248, stage4_pos_acc: 75.6248, stage4_loss_bbox: 0.3183, stage4_loss_iou: 0.7109, stage5_loss_cls: 0.6340, stage5_pos_acc: 75.3254, stage5_loss_bbox: 0.3178, stage5_loss_iou: 0.7111, loss: 11.7299, grad_norm: 50.8181
2023-02-03 02:56:34,722 - mmdet - INFO - Epoch [5][150/834]	lr: 2.500e-05, eta: 1:10:59, time: 0.656, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8541, stage0_pos_acc: 64.9631, stage0_loss_bbox: 0.7883, stage0_loss_iou: 1.4711, stage1_loss_cls: 0.6399, stage1_pos_acc: 75.2004, stage1_loss_bbox: 0.3668, stage1_loss_iou: 0.8868, stage2_loss_cls: 0.6171, stage2_pos_acc: 77.7276, stage2_loss_bbox: 0.3161, stage2_loss_iou: 0.7649, stage3_loss_cls: 0.6095, stage3_pos_acc: 78.8124, stage3_loss_bbox: 0.3084, stage3_loss_iou: 0.7454, stage4_loss_cls: 0.6176, stage4_pos_acc: 78.8410, stage4_loss_bbox: 0.3074, stage4_loss_iou: 0.7419, stage5_loss_cls: 0.6317, stage5_pos_acc: 78.8015, stage5_loss_bbox: 0.3053, stage5_loss_iou: 0.7392, loss: 11.7114, grad_norm: 48.8312
2023-02-03 02:57:07,651 - mmdet - INFO - Epoch [5][200/834]	lr: 2.500e-05, eta: 1:10:27, time: 0.659, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8827, stage0_pos_acc: 62.5527, stage0_loss_bbox: 0.7755, stage0_loss_iou: 1.4263, stage1_loss_cls: 0.6352, stage1_pos_acc: 73.1107, stage1_loss_bbox: 0.3720, stage1_loss_iou: 0.8566, stage2_loss_cls: 0.6116, stage2_pos_acc: 76.3932, stage2_loss_bbox: 0.3098, stage2_loss_iou: 0.7171, stage3_loss_cls: 0.6034, stage3_pos_acc: 78.1573, stage3_loss_bbox: 0.3019, stage3_loss_iou: 0.6968, stage4_loss_cls: 0.6049, stage4_pos_acc: 78.1878, stage4_loss_bbox: 0.2991, stage4_loss_iou: 0.6913, stage5_loss_cls: 0.6132, stage5_pos_acc: 77.3830, stage5_loss_bbox: 0.2971, stage5_loss_iou: 0.6886, loss: 11.3831, grad_norm: 48.7976
2023-02-03 02:57:41,150 - mmdet - INFO - Epoch [5][250/834]	lr: 2.500e-05, eta: 1:09:56, time: 0.670, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8790, stage0_pos_acc: 63.4103, stage0_loss_bbox: 0.7801, stage0_loss_iou: 1.4283, stage1_loss_cls: 0.6359, stage1_pos_acc: 75.2946, stage1_loss_bbox: 0.3597, stage1_loss_iou: 0.8479, stage2_loss_cls: 0.6143, stage2_pos_acc: 78.8972, stage2_loss_bbox: 0.2991, stage2_loss_iou: 0.7094, stage3_loss_cls: 0.6104, stage3_pos_acc: 79.6348, stage3_loss_bbox: 0.2907, stage3_loss_iou: 0.6888, stage4_loss_cls: 0.6071, stage4_pos_acc: 80.5463, stage4_loss_bbox: 0.2870, stage4_loss_iou: 0.6827, stage5_loss_cls: 0.6142, stage5_pos_acc: 79.7570, stage5_loss_bbox: 0.2858, stage5_loss_iou: 0.6829, loss: 11.3033, grad_norm: 49.6810
2023-02-03 02:58:14,166 - mmdet - INFO - Epoch [5][300/834]	lr: 2.500e-05, eta: 1:09:24, time: 0.660, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8747, stage0_pos_acc: 60.3387, stage0_loss_bbox: 0.7944, stage0_loss_iou: 1.4821, stage1_loss_cls: 0.6307, stage1_pos_acc: 75.0557, stage1_loss_bbox: 0.3696, stage1_loss_iou: 0.8913, stage2_loss_cls: 0.5986, stage2_pos_acc: 77.9175, stage2_loss_bbox: 0.3173, stage2_loss_iou: 0.7603, stage3_loss_cls: 0.5920, stage3_pos_acc: 79.0411, stage3_loss_bbox: 0.3072, stage3_loss_iou: 0.7373, stage4_loss_cls: 0.5910, stage4_pos_acc: 80.1955, stage4_loss_bbox: 0.3067, stage4_loss_iou: 0.7334, stage5_loss_cls: 0.5983, stage5_pos_acc: 79.5258, stage5_loss_bbox: 0.3044, stage5_loss_iou: 0.7302, loss: 11.6194, grad_norm: 53.2209
2023-02-03 02:58:47,132 - mmdet - INFO - Epoch [5][350/834]	lr: 2.500e-05, eta: 1:08:51, time: 0.659, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8971, stage0_pos_acc: 60.9192, stage0_loss_bbox: 0.7831, stage0_loss_iou: 1.4286, stage1_loss_cls: 0.6346, stage1_pos_acc: 72.0779, stage1_loss_bbox: 0.3569, stage1_loss_iou: 0.8385, stage2_loss_cls: 0.6109, stage2_pos_acc: 75.0248, stage2_loss_bbox: 0.3003, stage2_loss_iou: 0.7112, stage3_loss_cls: 0.6063, stage3_pos_acc: 77.1048, stage3_loss_bbox: 0.2908, stage3_loss_iou: 0.6938, stage4_loss_cls: 0.6019, stage4_pos_acc: 77.3779, stage4_loss_bbox: 0.2911, stage4_loss_iou: 0.6906, stage5_loss_cls: 0.6100, stage5_pos_acc: 77.5696, stage5_loss_bbox: 0.2892, stage5_loss_iou: 0.6826, loss: 11.3175, grad_norm: 52.0531
2023-02-03 02:59:20,437 - mmdet - INFO - Epoch [5][400/834]	lr: 2.500e-05, eta: 1:08:20, time: 0.666, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8470, stage0_pos_acc: 62.1179, stage0_loss_bbox: 0.7841, stage0_loss_iou: 1.4999, stage1_loss_cls: 0.6067, stage1_pos_acc: 74.8230, stage1_loss_bbox: 0.3762, stage1_loss_iou: 0.9017, stage2_loss_cls: 0.5835, stage2_pos_acc: 78.3218, stage2_loss_bbox: 0.3257, stage2_loss_iou: 0.7717, stage3_loss_cls: 0.5743, stage3_pos_acc: 79.6651, stage3_loss_bbox: 0.3202, stage3_loss_iou: 0.7550, stage4_loss_cls: 0.5734, stage4_pos_acc: 79.3656, stage4_loss_bbox: 0.3218, stage4_loss_iou: 0.7516, stage5_loss_cls: 0.5779, stage5_pos_acc: 79.3742, stage5_loss_bbox: 0.3202, stage5_loss_iou: 0.7496, loss: 11.6406, grad_norm: 50.6876
2023-02-03 02:59:53,349 - mmdet - INFO - Epoch [5][450/834]	lr: 2.500e-05, eta: 1:07:47, time: 0.658, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8559, stage0_pos_acc: 61.9848, stage0_loss_bbox: 0.7785, stage0_loss_iou: 1.4793, stage1_loss_cls: 0.6234, stage1_pos_acc: 73.6787, stage1_loss_bbox: 0.3664, stage1_loss_iou: 0.9034, stage2_loss_cls: 0.5958, stage2_pos_acc: 75.2335, stage2_loss_bbox: 0.3094, stage2_loss_iou: 0.7611, stage3_loss_cls: 0.5890, stage3_pos_acc: 76.6904, stage3_loss_bbox: 0.3026, stage3_loss_iou: 0.7432, stage4_loss_cls: 0.5870, stage4_pos_acc: 76.6299, stage4_loss_bbox: 0.3035, stage4_loss_iou: 0.7416, stage5_loss_cls: 0.5926, stage5_pos_acc: 76.5903, stage5_loss_bbox: 0.2991, stage5_loss_iou: 0.7354, loss: 11.5673, grad_norm: 53.7045
2023-02-03 03:00:26,266 - mmdet - INFO - Epoch [5][500/834]	lr: 2.500e-05, eta: 1:07:15, time: 0.658, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8661, stage0_pos_acc: 63.5535, stage0_loss_bbox: 0.7881, stage0_loss_iou: 1.4476, stage1_loss_cls: 0.6278, stage1_pos_acc: 73.5880, stage1_loss_bbox: 0.3655, stage1_loss_iou: 0.8574, stage2_loss_cls: 0.6139, stage2_pos_acc: 76.7927, stage2_loss_bbox: 0.3022, stage2_loss_iou: 0.7183, stage3_loss_cls: 0.6030, stage3_pos_acc: 77.4592, stage3_loss_bbox: 0.2940, stage3_loss_iou: 0.7012, stage4_loss_cls: 0.6007, stage4_pos_acc: 76.5910, stage4_loss_bbox: 0.2928, stage4_loss_iou: 0.6949, stage5_loss_cls: 0.6064, stage5_pos_acc: 77.4565, stage5_loss_bbox: 0.2921, stage5_loss_iou: 0.6902, loss: 11.3621, grad_norm: 48.7723
2023-02-03 03:00:59,337 - mmdet - INFO - Epoch [5][550/834]	lr: 2.500e-05, eta: 1:06:43, time: 0.661, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8755, stage0_pos_acc: 62.7815, stage0_loss_bbox: 0.7854, stage0_loss_iou: 1.4280, stage1_loss_cls: 0.6154, stage1_pos_acc: 75.9407, stage1_loss_bbox: 0.3598, stage1_loss_iou: 0.8306, stage2_loss_cls: 0.5840, stage2_pos_acc: 79.4548, stage2_loss_bbox: 0.3023, stage2_loss_iou: 0.7052, stage3_loss_cls: 0.5793, stage3_pos_acc: 78.8525, stage3_loss_bbox: 0.2949, stage3_loss_iou: 0.6886, stage4_loss_cls: 0.5745, stage4_pos_acc: 80.0306, stage4_loss_bbox: 0.2949, stage4_loss_iou: 0.6851, stage5_loss_cls: 0.5824, stage5_pos_acc: 79.5740, stage5_loss_bbox: 0.2916, stage5_loss_iou: 0.6826, loss: 11.1600, grad_norm: 52.7741
2023-02-03 03:01:32,465 - mmdet - INFO - Epoch [5][600/834]	lr: 2.500e-05, eta: 1:06:11, time: 0.663, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8623, stage0_pos_acc: 63.2650, stage0_loss_bbox: 0.7795, stage0_loss_iou: 1.4497, stage1_loss_cls: 0.6265, stage1_pos_acc: 74.9576, stage1_loss_bbox: 0.3597, stage1_loss_iou: 0.8778, stage2_loss_cls: 0.6035, stage2_pos_acc: 76.8745, stage2_loss_bbox: 0.3115, stage2_loss_iou: 0.7606, stage3_loss_cls: 0.5906, stage3_pos_acc: 77.6775, stage3_loss_bbox: 0.3080, stage3_loss_iou: 0.7480, stage4_loss_cls: 0.5958, stage4_pos_acc: 78.1347, stage4_loss_bbox: 0.3055, stage4_loss_iou: 0.7438, stage5_loss_cls: 0.5957, stage5_pos_acc: 77.7579, stage5_loss_bbox: 0.3037, stage5_loss_iou: 0.7397, loss: 11.5619, grad_norm: 48.6175
2023-02-03 03:02:05,836 - mmdet - INFO - Epoch [5][650/834]	lr: 2.500e-05, eta: 1:05:39, time: 0.667, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8640, stage0_pos_acc: 61.6280, stage0_loss_bbox: 0.7759, stage0_loss_iou: 1.4601, stage1_loss_cls: 0.6238, stage1_pos_acc: 73.1545, stage1_loss_bbox: 0.3428, stage1_loss_iou: 0.8492, stage2_loss_cls: 0.5956, stage2_pos_acc: 76.5821, stage2_loss_bbox: 0.2919, stage2_loss_iou: 0.7241, stage3_loss_cls: 0.5919, stage3_pos_acc: 76.8370, stage3_loss_bbox: 0.2865, stage3_loss_iou: 0.7107, stage4_loss_cls: 0.5872, stage4_pos_acc: 76.8405, stage4_loss_bbox: 0.2865, stage4_loss_iou: 0.7093, stage5_loss_cls: 0.5944, stage5_pos_acc: 77.5815, stage5_loss_bbox: 0.2851, stage5_loss_iou: 0.7050, loss: 11.2838, grad_norm: 50.3571
2023-02-03 03:02:38,878 - mmdet - INFO - Epoch [5][700/834]	lr: 2.500e-05, eta: 1:05:07, time: 0.661, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8669, stage0_pos_acc: 64.5136, stage0_loss_bbox: 0.7540, stage0_loss_iou: 1.4552, stage1_loss_cls: 0.6350, stage1_pos_acc: 74.4697, stage1_loss_bbox: 0.3330, stage1_loss_iou: 0.8387, stage2_loss_cls: 0.6092, stage2_pos_acc: 76.3263, stage2_loss_bbox: 0.2783, stage2_loss_iou: 0.7084, stage3_loss_cls: 0.6000, stage3_pos_acc: 76.9792, stage3_loss_bbox: 0.2733, stage3_loss_iou: 0.6933, stage4_loss_cls: 0.5961, stage4_pos_acc: 77.1414, stage4_loss_bbox: 0.2700, stage4_loss_iou: 0.6889, stage5_loss_cls: 0.6008, stage5_pos_acc: 77.0347, stage5_loss_bbox: 0.2694, stage5_loss_iou: 0.6892, loss: 11.1599, grad_norm: 51.8566
2023-02-03 03:03:11,966 - mmdet - INFO - Epoch [5][750/834]	lr: 2.500e-05, eta: 1:04:35, time: 0.662, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8600, stage0_pos_acc: 62.3862, stage0_loss_bbox: 0.7776, stage0_loss_iou: 1.4376, stage1_loss_cls: 0.6204, stage1_pos_acc: 76.3698, stage1_loss_bbox: 0.3626, stage1_loss_iou: 0.8581, stage2_loss_cls: 0.5889, stage2_pos_acc: 78.6428, stage2_loss_bbox: 0.3115, stage2_loss_iou: 0.7356, stage3_loss_cls: 0.5839, stage3_pos_acc: 78.8914, stage3_loss_bbox: 0.3035, stage3_loss_iou: 0.7166, stage4_loss_cls: 0.5795, stage4_pos_acc: 79.8653, stage4_loss_bbox: 0.3036, stage4_loss_iou: 0.7140, stage5_loss_cls: 0.5871, stage5_pos_acc: 79.2697, stage5_loss_bbox: 0.3012, stage5_loss_iou: 0.7105, loss: 11.3523, grad_norm: 49.7217
2023-02-03 03:03:45,024 - mmdet - INFO - Epoch [5][800/834]	lr: 2.500e-05, eta: 1:04:03, time: 0.661, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8613, stage0_pos_acc: 65.4868, stage0_loss_bbox: 0.7751, stage0_loss_iou: 1.4635, stage1_loss_cls: 0.6112, stage1_pos_acc: 77.2128, stage1_loss_bbox: 0.3513, stage1_loss_iou: 0.8600, stage2_loss_cls: 0.5877, stage2_pos_acc: 79.1221, stage2_loss_bbox: 0.2973, stage2_loss_iou: 0.7262, stage3_loss_cls: 0.5764, stage3_pos_acc: 79.7329, stage3_loss_bbox: 0.2871, stage3_loss_iou: 0.7058, stage4_loss_cls: 0.5687, stage4_pos_acc: 81.5025, stage4_loss_bbox: 0.2851, stage4_loss_iou: 0.7018, stage5_loss_cls: 0.5766, stage5_pos_acc: 80.7998, stage5_loss_bbox: 0.2825, stage5_loss_iou: 0.6990, loss: 11.2165, grad_norm: 48.6948
2023-02-03 03:04:36,646 - mmdet - INFO - Evaluating bbox...
2023-02-03 03:04:49,326 - mmdet - INFO - Exp name: sparse_rcnn_r50_fpn_1x_coco_comparison.py
2023-02-03 03:04:49,327 - mmdet - INFO - Epoch(val) [5][186]	bbox_mAP: 0.1220, bbox_mAP_50: 0.2720, bbox_mAP_75: 0.0880, bbox_mAP_s: 0.0050, bbox_mAP_m: 0.0750, bbox_mAP_l: 0.1280, bbox_mAP_copypaste: 0.122 0.272 0.088 0.005 0.075 0.128
2023-02-03 03:05:35,383 - mmdet - INFO - Epoch [6][50/834]	lr: 2.500e-05, eta: 1:02:55, time: 0.921, data_time: 0.270, memory: 5033, stage0_loss_cls: 0.8695, stage0_pos_acc: 63.3158, stage0_loss_bbox: 0.7898, stage0_loss_iou: 1.4520, stage1_loss_cls: 0.6212, stage1_pos_acc: 73.8195, stage1_loss_bbox: 0.3559, stage1_loss_iou: 0.8639, stage2_loss_cls: 0.6012, stage2_pos_acc: 75.4733, stage2_loss_bbox: 0.2938, stage2_loss_iou: 0.7228, stage3_loss_cls: 0.5880, stage3_pos_acc: 77.6349, stage3_loss_bbox: 0.2858, stage3_loss_iou: 0.7083, stage4_loss_cls: 0.5850, stage4_pos_acc: 76.6784, stage4_loss_bbox: 0.2834, stage4_loss_iou: 0.7031, stage5_loss_cls: 0.5881, stage5_pos_acc: 77.7292, stage5_loss_bbox: 0.2818, stage5_loss_iou: 0.6998, loss: 11.2934, grad_norm: 50.4824
2023-02-03 03:06:08,354 - mmdet - INFO - Epoch [6][100/834]	lr: 2.500e-05, eta: 1:02:23, time: 0.659, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8582, stage0_pos_acc: 63.2306, stage0_loss_bbox: 0.7526, stage0_loss_iou: 1.4404, stage1_loss_cls: 0.6145, stage1_pos_acc: 74.9150, stage1_loss_bbox: 0.3398, stage1_loss_iou: 0.8320, stage2_loss_cls: 0.5887, stage2_pos_acc: 78.1105, stage2_loss_bbox: 0.2879, stage2_loss_iou: 0.7079, stage3_loss_cls: 0.5788, stage3_pos_acc: 78.3935, stage3_loss_bbox: 0.2813, stage3_loss_iou: 0.6928, stage4_loss_cls: 0.5732, stage4_pos_acc: 77.9850, stage4_loss_bbox: 0.2794, stage4_loss_iou: 0.6881, stage5_loss_cls: 0.5774, stage5_pos_acc: 78.1728, stage5_loss_bbox: 0.2795, stage5_loss_iou: 0.6862, loss: 11.0586, grad_norm: 49.3869
2023-02-03 03:06:41,520 - mmdet - INFO - Epoch [6][150/834]	lr: 2.500e-05, eta: 1:01:51, time: 0.663, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8864, stage0_pos_acc: 62.5780, stage0_loss_bbox: 0.7566, stage0_loss_iou: 1.4165, stage1_loss_cls: 0.6180, stage1_pos_acc: 75.5718, stage1_loss_bbox: 0.3345, stage1_loss_iou: 0.8005, stage2_loss_cls: 0.5790, stage2_pos_acc: 80.0133, stage2_loss_bbox: 0.2859, stage2_loss_iou: 0.6836, stage3_loss_cls: 0.5698, stage3_pos_acc: 80.2016, stage3_loss_bbox: 0.2758, stage3_loss_iou: 0.6669, stage4_loss_cls: 0.5660, stage4_pos_acc: 81.0078, stage4_loss_bbox: 0.2759, stage4_loss_iou: 0.6653, stage5_loss_cls: 0.5684, stage5_pos_acc: 81.5114, stage5_loss_bbox: 0.2755, stage5_loss_iou: 0.6640, loss: 10.8884, grad_norm: 53.4517
2023-02-03 03:07:14,437 - mmdet - INFO - Epoch [6][200/834]	lr: 2.500e-05, eta: 1:01:19, time: 0.658, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8618, stage0_pos_acc: 63.8491, stage0_loss_bbox: 0.7927, stage0_loss_iou: 1.4729, stage1_loss_cls: 0.6131, stage1_pos_acc: 74.8391, stage1_loss_bbox: 0.3634, stage1_loss_iou: 0.8722, stage2_loss_cls: 0.5871, stage2_pos_acc: 76.0669, stage2_loss_bbox: 0.3162, stage2_loss_iou: 0.7460, stage3_loss_cls: 0.5781, stage3_pos_acc: 77.8041, stage3_loss_bbox: 0.3099, stage3_loss_iou: 0.7295, stage4_loss_cls: 0.5754, stage4_pos_acc: 76.8040, stage4_loss_bbox: 0.3066, stage4_loss_iou: 0.7230, stage5_loss_cls: 0.5799, stage5_pos_acc: 76.6460, stage5_loss_bbox: 0.3059, stage5_loss_iou: 0.7221, loss: 11.4557, grad_norm: 51.0103
2023-02-03 03:07:47,353 - mmdet - INFO - Epoch [6][250/834]	lr: 2.500e-05, eta: 1:00:47, time: 0.658, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8573, stage0_pos_acc: 63.3272, stage0_loss_bbox: 0.7413, stage0_loss_iou: 1.4153, stage1_loss_cls: 0.5945, stage1_pos_acc: 77.0731, stage1_loss_bbox: 0.3358, stage1_loss_iou: 0.8066, stage2_loss_cls: 0.5659, stage2_pos_acc: 78.4037, stage2_loss_bbox: 0.2874, stage2_loss_iou: 0.6897, stage3_loss_cls: 0.5534, stage3_pos_acc: 79.3834, stage3_loss_bbox: 0.2826, stage3_loss_iou: 0.6731, stage4_loss_cls: 0.5582, stage4_pos_acc: 79.9631, stage4_loss_bbox: 0.2802, stage4_loss_iou: 0.6680, stage5_loss_cls: 0.5585, stage5_pos_acc: 78.9406, stage5_loss_bbox: 0.2807, stage5_loss_iou: 0.6667, loss: 10.8151, grad_norm: 50.5065
2023-02-03 03:08:20,367 - mmdet - INFO - Epoch [6][300/834]	lr: 2.500e-05, eta: 1:00:15, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8593, stage0_pos_acc: 65.3866, stage0_loss_bbox: 0.7557, stage0_loss_iou: 1.4233, stage1_loss_cls: 0.6121, stage1_pos_acc: 76.1800, stage1_loss_bbox: 0.3360, stage1_loss_iou: 0.8223, stage2_loss_cls: 0.5925, stage2_pos_acc: 78.4249, stage2_loss_bbox: 0.2869, stage2_loss_iou: 0.7034, stage3_loss_cls: 0.5797, stage3_pos_acc: 80.0199, stage3_loss_bbox: 0.2788, stage3_loss_iou: 0.6858, stage4_loss_cls: 0.5814, stage4_pos_acc: 80.9172, stage4_loss_bbox: 0.2761, stage4_loss_iou: 0.6783, stage5_loss_cls: 0.5848, stage5_pos_acc: 80.5334, stage5_loss_bbox: 0.2767, stage5_loss_iou: 0.6789, loss: 11.0120, grad_norm: 50.6279
2023-02-03 03:08:53,557 - mmdet - INFO - Epoch [6][350/834]	lr: 2.500e-05, eta: 0:59:43, time: 0.664, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8479, stage0_pos_acc: 64.8819, stage0_loss_bbox: 0.7593, stage0_loss_iou: 1.4369, stage1_loss_cls: 0.6005, stage1_pos_acc: 75.9456, stage1_loss_bbox: 0.3369, stage1_loss_iou: 0.8353, stage2_loss_cls: 0.5727, stage2_pos_acc: 79.1120, stage2_loss_bbox: 0.2836, stage2_loss_iou: 0.7077, stage3_loss_cls: 0.5605, stage3_pos_acc: 78.9851, stage3_loss_bbox: 0.2719, stage3_loss_iou: 0.6851, stage4_loss_cls: 0.5627, stage4_pos_acc: 79.9120, stage4_loss_bbox: 0.2715, stage4_loss_iou: 0.6829, stage5_loss_cls: 0.5673, stage5_pos_acc: 79.4259, stage5_loss_bbox: 0.2699, stage5_loss_iou: 0.6784, loss: 10.9311, grad_norm: 49.3067
2023-02-03 03:09:26,758 - mmdet - INFO - Epoch [6][400/834]	lr: 2.500e-05, eta: 0:59:11, time: 0.664, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8476, stage0_pos_acc: 64.6434, stage0_loss_bbox: 0.7591, stage0_loss_iou: 1.4591, stage1_loss_cls: 0.5978, stage1_pos_acc: 77.8860, stage1_loss_bbox: 0.3376, stage1_loss_iou: 0.8436, stage2_loss_cls: 0.5716, stage2_pos_acc: 80.8211, stage2_loss_bbox: 0.2832, stage2_loss_iou: 0.7114, stage3_loss_cls: 0.5602, stage3_pos_acc: 82.0465, stage3_loss_bbox: 0.2780, stage3_loss_iou: 0.6913, stage4_loss_cls: 0.5536, stage4_pos_acc: 82.6529, stage4_loss_bbox: 0.2752, stage4_loss_iou: 0.6869, stage5_loss_cls: 0.5597, stage5_pos_acc: 82.1148, stage5_loss_bbox: 0.2755, stage5_loss_iou: 0.6866, loss: 10.9782, grad_norm: 52.0905
2023-02-03 03:09:59,856 - mmdet - INFO - Epoch [6][450/834]	lr: 2.500e-05, eta: 0:58:39, time: 0.662, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8653, stage0_pos_acc: 65.1633, stage0_loss_bbox: 0.7626, stage0_loss_iou: 1.4311, stage1_loss_cls: 0.6190, stage1_pos_acc: 79.4913, stage1_loss_bbox: 0.3362, stage1_loss_iou: 0.8034, stage2_loss_cls: 0.5867, stage2_pos_acc: 80.9965, stage2_loss_bbox: 0.2806, stage2_loss_iou: 0.6808, stage3_loss_cls: 0.5779, stage3_pos_acc: 81.3137, stage3_loss_bbox: 0.2773, stage3_loss_iou: 0.6700, stage4_loss_cls: 0.5781, stage4_pos_acc: 81.7606, stage4_loss_bbox: 0.2773, stage4_loss_iou: 0.6675, stage5_loss_cls: 0.5808, stage5_pos_acc: 81.4703, stage5_loss_bbox: 0.2755, stage5_loss_iou: 0.6651, loss: 10.9353, grad_norm: 51.3322
2023-02-03 03:10:33,224 - mmdet - INFO - Epoch [6][500/834]	lr: 2.500e-05, eta: 0:58:07, time: 0.667, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8286, stage0_pos_acc: 66.4700, stage0_loss_bbox: 0.7686, stage0_loss_iou: 1.4737, stage1_loss_cls: 0.5914, stage1_pos_acc: 78.0522, stage1_loss_bbox: 0.3432, stage1_loss_iou: 0.8587, stage2_loss_cls: 0.5689, stage2_pos_acc: 80.4705, stage2_loss_bbox: 0.2947, stage2_loss_iou: 0.7392, stage3_loss_cls: 0.5567, stage3_pos_acc: 80.9355, stage3_loss_bbox: 0.2925, stage3_loss_iou: 0.7292, stage4_loss_cls: 0.5551, stage4_pos_acc: 80.5492, stage4_loss_bbox: 0.2911, stage4_loss_iou: 0.7268, stage5_loss_cls: 0.5619, stage5_pos_acc: 81.3076, stage5_loss_bbox: 0.2906, stage5_loss_iou: 0.7257, loss: 11.1965, grad_norm: 47.0239
2023-02-03 03:11:06,457 - mmdet - INFO - Epoch [6][550/834]	lr: 2.500e-05, eta: 0:57:35, time: 0.664, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8360, stage0_pos_acc: 67.3893, stage0_loss_bbox: 0.7589, stage0_loss_iou: 1.4366, stage1_loss_cls: 0.5994, stage1_pos_acc: 76.8661, stage1_loss_bbox: 0.3495, stage1_loss_iou: 0.8379, stage2_loss_cls: 0.5718, stage2_pos_acc: 79.5138, stage2_loss_bbox: 0.3020, stage2_loss_iou: 0.7219, stage3_loss_cls: 0.5692, stage3_pos_acc: 79.6512, stage3_loss_bbox: 0.2921, stage3_loss_iou: 0.7025, stage4_loss_cls: 0.5617, stage4_pos_acc: 79.3464, stage4_loss_bbox: 0.2924, stage4_loss_iou: 0.6984, stage5_loss_cls: 0.5612, stage5_pos_acc: 80.1601, stage5_loss_bbox: 0.2920, stage5_loss_iou: 0.6986, loss: 11.0822, grad_norm: 46.4015
2023-02-03 03:11:39,587 - mmdet - INFO - Epoch [6][600/834]	lr: 2.500e-05, eta: 0:57:03, time: 0.663, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8773, stage0_pos_acc: 63.4535, stage0_loss_bbox: 0.7584, stage0_loss_iou: 1.4271, stage1_loss_cls: 0.6265, stage1_pos_acc: 75.3552, stage1_loss_bbox: 0.3338, stage1_loss_iou: 0.7995, stage2_loss_cls: 0.6073, stage2_pos_acc: 77.3088, stage2_loss_bbox: 0.2812, stage2_loss_iou: 0.6764, stage3_loss_cls: 0.5935, stage3_pos_acc: 78.4711, stage3_loss_bbox: 0.2709, stage3_loss_iou: 0.6590, stage4_loss_cls: 0.5943, stage4_pos_acc: 78.1676, stage4_loss_bbox: 0.2692, stage4_loss_iou: 0.6585, stage5_loss_cls: 0.5941, stage5_pos_acc: 77.6557, stage5_loss_bbox: 0.2680, stage5_loss_iou: 0.6562, loss: 10.9513, grad_norm: 50.4634
2023-02-03 03:12:12,964 - mmdet - INFO - Epoch [6][650/834]	lr: 2.500e-05, eta: 0:56:31, time: 0.667, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8488, stage0_pos_acc: 68.1883, stage0_loss_bbox: 0.7405, stage0_loss_iou: 1.4497, stage1_loss_cls: 0.5936, stage1_pos_acc: 77.8554, stage1_loss_bbox: 0.3210, stage1_loss_iou: 0.8292, stage2_loss_cls: 0.5704, stage2_pos_acc: 80.7634, stage2_loss_bbox: 0.2710, stage2_loss_iou: 0.7001, stage3_loss_cls: 0.5604, stage3_pos_acc: 81.6568, stage3_loss_bbox: 0.2613, stage3_loss_iou: 0.6832, stage4_loss_cls: 0.5570, stage4_pos_acc: 80.8119, stage4_loss_bbox: 0.2607, stage4_loss_iou: 0.6814, stage5_loss_cls: 0.5560, stage5_pos_acc: 80.7420, stage5_loss_bbox: 0.2633, stage5_loss_iou: 0.6844, loss: 10.8322, grad_norm: 48.5317
2023-02-03 03:12:45,826 - mmdet - INFO - Epoch [6][700/834]	lr: 2.500e-05, eta: 0:55:58, time: 0.658, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8560, stage0_pos_acc: 71.4805, stage0_loss_bbox: 0.7548, stage0_loss_iou: 1.4405, stage1_loss_cls: 0.6018, stage1_pos_acc: 79.3254, stage1_loss_bbox: 0.3386, stage1_loss_iou: 0.8215, stage2_loss_cls: 0.5764, stage2_pos_acc: 81.7605, stage2_loss_bbox: 0.2840, stage2_loss_iou: 0.6938, stage3_loss_cls: 0.5700, stage3_pos_acc: 82.0769, stage3_loss_bbox: 0.2772, stage3_loss_iou: 0.6780, stage4_loss_cls: 0.5662, stage4_pos_acc: 82.3095, stage4_loss_bbox: 0.2755, stage4_loss_iou: 0.6736, stage5_loss_cls: 0.5662, stage5_pos_acc: 82.4136, stage5_loss_bbox: 0.2770, stage5_loss_iou: 0.6736, loss: 10.9248, grad_norm: 51.4714
2023-02-03 03:13:18,928 - mmdet - INFO - Epoch [6][750/834]	lr: 2.500e-05, eta: 0:55:26, time: 0.662, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8374, stage0_pos_acc: 67.8218, stage0_loss_bbox: 0.7633, stage0_loss_iou: 1.4573, stage1_loss_cls: 0.5988, stage1_pos_acc: 77.6679, stage1_loss_bbox: 0.3423, stage1_loss_iou: 0.8394, stage2_loss_cls: 0.5754, stage2_pos_acc: 80.7419, stage2_loss_bbox: 0.2868, stage2_loss_iou: 0.7107, stage3_loss_cls: 0.5684, stage3_pos_acc: 81.6955, stage3_loss_bbox: 0.2820, stage3_loss_iou: 0.6924, stage4_loss_cls: 0.5646, stage4_pos_acc: 81.0586, stage4_loss_bbox: 0.2804, stage4_loss_iou: 0.6883, stage5_loss_cls: 0.5707, stage5_pos_acc: 81.1146, stage5_loss_bbox: 0.2771, stage5_loss_iou: 0.6828, loss: 11.0180, grad_norm: 48.4051
2023-02-03 03:13:51,877 - mmdet - INFO - Epoch [6][800/834]	lr: 2.500e-05, eta: 0:54:53, time: 0.659, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.8572, stage0_pos_acc: 67.8875, stage0_loss_bbox: 0.7742, stage0_loss_iou: 1.4229, stage1_loss_cls: 0.6162, stage1_pos_acc: 79.5598, stage1_loss_bbox: 0.3525, stage1_loss_iou: 0.8150, stage2_loss_cls: 0.5966, stage2_pos_acc: 80.2900, stage2_loss_bbox: 0.3064, stage2_loss_iou: 0.7013, stage3_loss_cls: 0.5829, stage3_pos_acc: 83.0365, stage3_loss_bbox: 0.2966, stage3_loss_iou: 0.6819, stage4_loss_cls: 0.5832, stage4_pos_acc: 82.0313, stage4_loss_bbox: 0.2986, stage4_loss_iou: 0.6794, stage5_loss_cls: 0.5867, stage5_pos_acc: 81.2037, stage5_loss_bbox: 0.2973, stage5_loss_iou: 0.6803, loss: 11.1293, grad_norm: 52.1514
2023-02-03 03:14:14,739 - mmdet - INFO - Saving checkpoint at 6 epochs
2023-02-03 03:14:45,502 - mmdet - INFO - Evaluating bbox...
2023-02-03 03:14:58,465 - mmdet - INFO - Exp name: sparse_rcnn_r50_fpn_1x_coco_comparison.py
2023-02-03 03:14:58,466 - mmdet - INFO - Epoch(val) [6][186]	bbox_mAP: 0.1550, bbox_mAP_50: 0.3180, bbox_mAP_75: 0.1430, bbox_mAP_s: 0.0050, bbox_mAP_m: 0.0860, bbox_mAP_l: 0.1570, bbox_mAP_copypaste: 0.155 0.318 0.143 0.005 0.086 0.157
2023-02-03 03:15:44,873 - mmdet - INFO - Epoch [7][50/834]	lr: 2.500e-05, eta: 0:53:50, time: 0.928, data_time: 0.276, memory: 5033, stage0_loss_cls: 0.8788, stage0_pos_acc: 63.8659, stage0_loss_bbox: 0.7672, stage0_loss_iou: 1.4417, stage1_loss_cls: 0.6001, stage1_pos_acc: 78.0393, stage1_loss_bbox: 0.3366, stage1_loss_iou: 0.8072, stage2_loss_cls: 0.5729, stage2_pos_acc: 80.0946, stage2_loss_bbox: 0.2827, stage2_loss_iou: 0.6767, stage3_loss_cls: 0.5670, stage3_pos_acc: 81.1159, stage3_loss_bbox: 0.2746, stage3_loss_iou: 0.6589, stage4_loss_cls: 0.5616, stage4_pos_acc: 80.8272, stage4_loss_bbox: 0.2730, stage4_loss_iou: 0.6552, stage5_loss_cls: 0.5627, stage5_pos_acc: 80.1476, stage5_loss_bbox: 0.2712, stage5_loss_iou: 0.6518, loss: 10.8399, grad_norm: 49.5602
2023-02-03 03:16:17,875 - mmdet - INFO - Epoch [7][100/834]	lr: 2.500e-05, eta: 0:53:18, time: 0.660, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8410, stage0_pos_acc: 65.4230, stage0_loss_bbox: 0.7633, stage0_loss_iou: 1.4313, stage1_loss_cls: 0.5744, stage1_pos_acc: 79.3891, stage1_loss_bbox: 0.3408, stage1_loss_iou: 0.8057, stage2_loss_cls: 0.5491, stage2_pos_acc: 81.6024, stage2_loss_bbox: 0.2962, stage2_loss_iou: 0.6967, stage3_loss_cls: 0.5437, stage3_pos_acc: 81.9806, stage3_loss_bbox: 0.2885, stage3_loss_iou: 0.6818, stage4_loss_cls: 0.5403, stage4_pos_acc: 82.2432, stage4_loss_bbox: 0.2891, stage4_loss_iou: 0.6812, stage5_loss_cls: 0.5439, stage5_pos_acc: 82.6693, stage5_loss_bbox: 0.2880, stage5_loss_iou: 0.6798, loss: 10.8348, grad_norm: 56.5726
2023-02-03 03:16:50,691 - mmdet - INFO - Epoch [7][150/834]	lr: 2.500e-05, eta: 0:52:45, time: 0.656, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.7971, stage0_pos_acc: 70.2318, stage0_loss_bbox: 0.7437, stage0_loss_iou: 1.4636, stage1_loss_cls: 0.5579, stage1_pos_acc: 80.4032, stage1_loss_bbox: 0.3149, stage1_loss_iou: 0.8173, stage2_loss_cls: 0.5303, stage2_pos_acc: 83.5497, stage2_loss_bbox: 0.2672, stage2_loss_iou: 0.6915, stage3_loss_cls: 0.5214, stage3_pos_acc: 83.9421, stage3_loss_bbox: 0.2601, stage3_loss_iou: 0.6763, stage4_loss_cls: 0.5160, stage4_pos_acc: 84.2852, stage4_loss_bbox: 0.2598, stage4_loss_iou: 0.6715, stage5_loss_cls: 0.5211, stage5_pos_acc: 84.5230, stage5_loss_bbox: 0.2614, stage5_loss_iou: 0.6743, loss: 10.5455, grad_norm: 47.0765
2023-02-03 03:17:23,654 - mmdet - INFO - Epoch [7][200/834]	lr: 2.500e-05, eta: 0:52:13, time: 0.659, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8137, stage0_pos_acc: 66.5867, stage0_loss_bbox: 0.7400, stage0_loss_iou: 1.4208, stage1_loss_cls: 0.5710, stage1_pos_acc: 77.2617, stage1_loss_bbox: 0.3431, stage1_loss_iou: 0.8154, stage2_loss_cls: 0.5463, stage2_pos_acc: 79.7825, stage2_loss_bbox: 0.2938, stage2_loss_iou: 0.7099, stage3_loss_cls: 0.5435, stage3_pos_acc: 80.1818, stage3_loss_bbox: 0.2888, stage3_loss_iou: 0.6968, stage4_loss_cls: 0.5450, stage4_pos_acc: 80.7252, stage4_loss_bbox: 0.2872, stage4_loss_iou: 0.6900, stage5_loss_cls: 0.5448, stage5_pos_acc: 81.4352, stage5_loss_bbox: 0.2871, stage5_loss_iou: 0.6893, loss: 10.8266, grad_norm: 49.8072
2023-02-03 03:17:56,599 - mmdet - INFO - Epoch [7][250/834]	lr: 2.500e-05, eta: 0:51:41, time: 0.659, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8321, stage0_pos_acc: 66.3065, stage0_loss_bbox: 0.7338, stage0_loss_iou: 1.4181, stage1_loss_cls: 0.5808, stage1_pos_acc: 79.0759, stage1_loss_bbox: 0.3243, stage1_loss_iou: 0.7909, stage2_loss_cls: 0.5571, stage2_pos_acc: 81.6203, stage2_loss_bbox: 0.2813, stage2_loss_iou: 0.6834, stage3_loss_cls: 0.5488, stage3_pos_acc: 81.8560, stage3_loss_bbox: 0.2702, stage3_loss_iou: 0.6628, stage4_loss_cls: 0.5427, stage4_pos_acc: 81.8074, stage4_loss_bbox: 0.2702, stage4_loss_iou: 0.6600, stage5_loss_cls: 0.5492, stage5_pos_acc: 80.8477, stage5_loss_bbox: 0.2678, stage5_loss_iou: 0.6580, loss: 10.6314, grad_norm: 48.3199
2023-02-03 03:18:29,423 - mmdet - INFO - Epoch [7][300/834]	lr: 2.500e-05, eta: 0:51:08, time: 0.656, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8522, stage0_pos_acc: 66.8093, stage0_loss_bbox: 0.7418, stage0_loss_iou: 1.4212, stage1_loss_cls: 0.5733, stage1_pos_acc: 80.4974, stage1_loss_bbox: 0.3189, stage1_loss_iou: 0.7766, stage2_loss_cls: 0.5468, stage2_pos_acc: 82.2915, stage2_loss_bbox: 0.2717, stage2_loss_iou: 0.6621, stage3_loss_cls: 0.5419, stage3_pos_acc: 82.6442, stage3_loss_bbox: 0.2646, stage3_loss_iou: 0.6512, stage4_loss_cls: 0.5458, stage4_pos_acc: 82.7679, stage4_loss_bbox: 0.2625, stage4_loss_iou: 0.6470, stage5_loss_cls: 0.5512, stage5_pos_acc: 82.4865, stage5_loss_bbox: 0.2604, stage5_loss_iou: 0.6430, loss: 10.5321, grad_norm: 50.8703
2023-02-03 03:19:02,175 - mmdet - INFO - Epoch [7][350/834]	lr: 2.500e-05, eta: 0:50:36, time: 0.655, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8523, stage0_pos_acc: 67.5992, stage0_loss_bbox: 0.7585, stage0_loss_iou: 1.4164, stage1_loss_cls: 0.6000, stage1_pos_acc: 77.9030, stage1_loss_bbox: 0.3388, stage1_loss_iou: 0.8190, stage2_loss_cls: 0.5697, stage2_pos_acc: 79.7662, stage2_loss_bbox: 0.2868, stage2_loss_iou: 0.6986, stage3_loss_cls: 0.5699, stage3_pos_acc: 80.8576, stage3_loss_bbox: 0.2763, stage3_loss_iou: 0.6763, stage4_loss_cls: 0.5687, stage4_pos_acc: 80.8424, stage4_loss_bbox: 0.2745, stage4_loss_iou: 0.6706, stage5_loss_cls: 0.5739, stage5_pos_acc: 80.0325, stage5_loss_bbox: 0.2752, stage5_loss_iou: 0.6704, loss: 10.8958, grad_norm: 50.9548
2023-02-03 03:19:35,281 - mmdet - INFO - Epoch [7][400/834]	lr: 2.500e-05, eta: 0:50:04, time: 0.662, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8447, stage0_pos_acc: 70.5221, stage0_loss_bbox: 0.7463, stage0_loss_iou: 1.4243, stage1_loss_cls: 0.5906, stage1_pos_acc: 80.2909, stage1_loss_bbox: 0.3232, stage1_loss_iou: 0.7816, stage2_loss_cls: 0.5707, stage2_pos_acc: 83.1070, stage2_loss_bbox: 0.2713, stage2_loss_iou: 0.6575, stage3_loss_cls: 0.5670, stage3_pos_acc: 83.2090, stage3_loss_bbox: 0.2612, stage3_loss_iou: 0.6384, stage4_loss_cls: 0.5649, stage4_pos_acc: 83.0156, stage4_loss_bbox: 0.2619, stage4_loss_iou: 0.6350, stage5_loss_cls: 0.5662, stage5_pos_acc: 83.6650, stage5_loss_bbox: 0.2600, stage5_loss_iou: 0.6344, loss: 10.5992, grad_norm: 51.0800
2023-02-03 03:20:08,376 - mmdet - INFO - Epoch [7][450/834]	lr: 2.500e-05, eta: 0:49:31, time: 0.662, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.8166, stage0_pos_acc: 69.6244, stage0_loss_bbox: 0.7507, stage0_loss_iou: 1.4424, stage1_loss_cls: 0.5855, stage1_pos_acc: 78.2675, stage1_loss_bbox: 0.3266, stage1_loss_iou: 0.8111, stage2_loss_cls: 0.5550, stage2_pos_acc: 81.6580, stage2_loss_bbox: 0.2795, stage2_loss_iou: 0.6991, stage3_loss_cls: 0.5512, stage3_pos_acc: 81.6725, stage3_loss_bbox: 0.2710, stage3_loss_iou: 0.6802, stage4_loss_cls: 0.5484, stage4_pos_acc: 81.6655, stage4_loss_bbox: 0.2708, stage4_loss_iou: 0.6767, stage5_loss_cls: 0.5527, stage5_pos_acc: 81.2212, stage5_loss_bbox: 0.2706, stage5_loss_iou: 0.6776, loss: 10.7657, grad_norm: 48.9117
2023-02-03 03:20:41,740 - mmdet - INFO - Epoch [7][500/834]	lr: 2.500e-05, eta: 0:48:59, time: 0.667, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.8073, stage0_pos_acc: 69.5948, stage0_loss_bbox: 0.7327, stage0_loss_iou: 1.4496, stage1_loss_cls: 0.5698, stage1_pos_acc: 80.3524, stage1_loss_bbox: 0.3321, stage1_loss_iou: 0.8435, stage2_loss_cls: 0.5486, stage2_pos_acc: 82.0814, stage2_loss_bbox: 0.2869, stage2_loss_iou: 0.7290, stage3_loss_cls: 0.5373, stage3_pos_acc: 83.1631, stage3_loss_bbox: 0.2835, stage3_loss_iou: 0.7170, stage4_loss_cls: 0.5320, stage4_pos_acc: 83.7884, stage4_loss_bbox: 0.2831, stage4_loss_iou: 0.7143, stage5_loss_cls: 0.5334, stage5_pos_acc: 82.7510, stage5_loss_bbox: 0.2841, stage5_loss_iou: 0.7169, loss: 10.9011, grad_norm: 46.4568
2023-02-03 03:21:14,755 - mmdet - INFO - Epoch [7][550/834]	lr: 2.500e-05, eta: 0:48:27, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8708, stage0_pos_acc: 68.7380, stage0_loss_bbox: 0.7361, stage0_loss_iou: 1.3912, stage1_loss_cls: 0.5966, stage1_pos_acc: 79.8698, stage1_loss_bbox: 0.3247, stage1_loss_iou: 0.7755, stage2_loss_cls: 0.5704, stage2_pos_acc: 82.3030, stage2_loss_bbox: 0.2805, stage2_loss_iou: 0.6711, stage3_loss_cls: 0.5583, stage3_pos_acc: 83.1206, stage3_loss_bbox: 0.2766, stage3_loss_iou: 0.6613, stage4_loss_cls: 0.5533, stage4_pos_acc: 83.1162, stage4_loss_bbox: 0.2766, stage4_loss_iou: 0.6595, stage5_loss_cls: 0.5574, stage5_pos_acc: 82.9056, stage5_loss_bbox: 0.2764, stage5_loss_iou: 0.6584, loss: 10.6947, grad_norm: 49.5939
2023-02-03 03:21:48,068 - mmdet - INFO - Epoch [7][600/834]	lr: 2.500e-05, eta: 0:47:55, time: 0.666, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8305, stage0_pos_acc: 67.0174, stage0_loss_bbox: 0.7408, stage0_loss_iou: 1.4390, stage1_loss_cls: 0.5773, stage1_pos_acc: 77.9564, stage1_loss_bbox: 0.3282, stage1_loss_iou: 0.8174, stage2_loss_cls: 0.5518, stage2_pos_acc: 80.9231, stage2_loss_bbox: 0.2817, stage2_loss_iou: 0.6986, stage3_loss_cls: 0.5410, stage3_pos_acc: 81.9644, stage3_loss_bbox: 0.2768, stage3_loss_iou: 0.6883, stage4_loss_cls: 0.5390, stage4_pos_acc: 81.5038, stage4_loss_bbox: 0.2764, stage4_loss_iou: 0.6863, stage5_loss_cls: 0.5454, stage5_pos_acc: 81.1876, stage5_loss_bbox: 0.2738, stage5_loss_iou: 0.6829, loss: 10.7750, grad_norm: 49.6526
2023-02-03 03:22:21,183 - mmdet - INFO - Epoch [7][650/834]	lr: 2.500e-05, eta: 0:47:23, time: 0.662, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8342, stage0_pos_acc: 71.0705, stage0_loss_bbox: 0.7524, stage0_loss_iou: 1.4213, stage1_loss_cls: 0.5775, stage1_pos_acc: 79.9523, stage1_loss_bbox: 0.3284, stage1_loss_iou: 0.8068, stage2_loss_cls: 0.5593, stage2_pos_acc: 82.5766, stage2_loss_bbox: 0.2836, stage2_loss_iou: 0.6917, stage3_loss_cls: 0.5509, stage3_pos_acc: 83.6545, stage3_loss_bbox: 0.2745, stage3_loss_iou: 0.6763, stage4_loss_cls: 0.5505, stage4_pos_acc: 83.1723, stage4_loss_bbox: 0.2750, stage4_loss_iou: 0.6763, stage5_loss_cls: 0.5533, stage5_pos_acc: 83.3700, stage5_loss_bbox: 0.2741, stage5_loss_iou: 0.6752, loss: 10.7612, grad_norm: 49.2492
2023-02-03 03:22:54,548 - mmdet - INFO - Epoch [7][700/834]	lr: 2.500e-05, eta: 0:46:51, time: 0.667, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8422, stage0_pos_acc: 68.2120, stage0_loss_bbox: 0.7306, stage0_loss_iou: 1.4095, stage1_loss_cls: 0.5894, stage1_pos_acc: 77.2537, stage1_loss_bbox: 0.3249, stage1_loss_iou: 0.7774, stage2_loss_cls: 0.5633, stage2_pos_acc: 78.8261, stage2_loss_bbox: 0.2755, stage2_loss_iou: 0.6689, stage3_loss_cls: 0.5601, stage3_pos_acc: 78.6750, stage3_loss_bbox: 0.2695, stage3_loss_iou: 0.6535, stage4_loss_cls: 0.5586, stage4_pos_acc: 79.1227, stage4_loss_bbox: 0.2654, stage4_loss_iou: 0.6450, stage5_loss_cls: 0.5610, stage5_pos_acc: 78.1783, stage5_loss_bbox: 0.2663, stage5_loss_iou: 0.6454, loss: 10.6064, grad_norm: 46.9252
2023-02-03 03:23:27,712 - mmdet - INFO - Epoch [7][750/834]	lr: 2.500e-05, eta: 0:46:18, time: 0.663, data_time: 0.008, memory: 5033, stage0_loss_cls: 0.8253, stage0_pos_acc: 67.4147, stage0_loss_bbox: 0.7537, stage0_loss_iou: 1.4435, stage1_loss_cls: 0.5762, stage1_pos_acc: 77.7475, stage1_loss_bbox: 0.3398, stage1_loss_iou: 0.8178, stage2_loss_cls: 0.5516, stage2_pos_acc: 81.0227, stage2_loss_bbox: 0.2905, stage2_loss_iou: 0.7059, stage3_loss_cls: 0.5449, stage3_pos_acc: 80.5800, stage3_loss_bbox: 0.2811, stage3_loss_iou: 0.6905, stage4_loss_cls: 0.5399, stage4_pos_acc: 81.8115, stage4_loss_bbox: 0.2797, stage4_loss_iou: 0.6879, stage5_loss_cls: 0.5420, stage5_pos_acc: 81.6979, stage5_loss_bbox: 0.2800, stage5_loss_iou: 0.6879, loss: 10.8381, grad_norm: 49.6391
2023-02-03 03:24:00,547 - mmdet - INFO - Epoch [7][800/834]	lr: 2.500e-05, eta: 0:45:46, time: 0.657, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8282, stage0_pos_acc: 69.2303, stage0_loss_bbox: 0.7452, stage0_loss_iou: 1.4335, stage1_loss_cls: 0.5801, stage1_pos_acc: 80.1543, stage1_loss_bbox: 0.3288, stage1_loss_iou: 0.8045, stage2_loss_cls: 0.5612, stage2_pos_acc: 81.7937, stage2_loss_bbox: 0.2770, stage2_loss_iou: 0.6824, stage3_loss_cls: 0.5544, stage3_pos_acc: 83.0944, stage3_loss_bbox: 0.2676, stage3_loss_iou: 0.6643, stage4_loss_cls: 0.5554, stage4_pos_acc: 83.0800, stage4_loss_bbox: 0.2665, stage4_loss_iou: 0.6645, stage5_loss_cls: 0.5541, stage5_pos_acc: 82.5024, stage5_loss_bbox: 0.2652, stage5_loss_iou: 0.6601, loss: 10.6932, grad_norm: 49.6994
2023-02-03 03:24:52,091 - mmdet - INFO - Evaluating bbox...
2023-02-03 03:25:05,421 - mmdet - INFO - Exp name: sparse_rcnn_r50_fpn_1x_coco_comparison.py
2023-02-03 03:25:05,422 - mmdet - INFO - Epoch(val) [7][186]	bbox_mAP: 0.1720, bbox_mAP_50: 0.3510, bbox_mAP_75: 0.1470, bbox_mAP_s: 0.0050, bbox_mAP_m: 0.0890, bbox_mAP_l: 0.1800, bbox_mAP_copypaste: 0.172 0.351 0.147 0.005 0.089 0.180
2023-02-03 03:25:52,624 - mmdet - INFO - Epoch [8][50/834]	lr: 2.500e-05, eta: 0:44:46, time: 0.944, data_time: 0.271, memory: 5033, stage0_loss_cls: 0.8145, stage0_pos_acc: 73.3846, stage0_loss_bbox: 0.7447, stage0_loss_iou: 1.4099, stage1_loss_cls: 0.5629, stage1_pos_acc: 81.1242, stage1_loss_bbox: 0.3163, stage1_loss_iou: 0.7700, stage2_loss_cls: 0.5463, stage2_pos_acc: 84.3231, stage2_loss_bbox: 0.2717, stage2_loss_iou: 0.6568, stage3_loss_cls: 0.5365, stage3_pos_acc: 84.6318, stage3_loss_bbox: 0.2610, stage3_loss_iou: 0.6373, stage4_loss_cls: 0.5365, stage4_pos_acc: 84.3247, stage4_loss_bbox: 0.2585, stage4_loss_iou: 0.6325, stage5_loss_cls: 0.5373, stage5_pos_acc: 84.8067, stage5_loss_bbox: 0.2584, stage5_loss_iou: 0.6318, loss: 10.3830, grad_norm: 51.2681
2023-02-03 03:26:26,522 - mmdet - INFO - Epoch [8][100/834]	lr: 2.500e-05, eta: 0:44:14, time: 0.678, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8518, stage0_pos_acc: 68.6012, stage0_loss_bbox: 0.7419, stage0_loss_iou: 1.4062, stage1_loss_cls: 0.5855, stage1_pos_acc: 79.3747, stage1_loss_bbox: 0.3203, stage1_loss_iou: 0.7720, stage2_loss_cls: 0.5618, stage2_pos_acc: 80.9103, stage2_loss_bbox: 0.2707, stage2_loss_iou: 0.6509, stage3_loss_cls: 0.5505, stage3_pos_acc: 82.2250, stage3_loss_bbox: 0.2640, stage3_loss_iou: 0.6375, stage4_loss_cls: 0.5447, stage4_pos_acc: 82.2905, stage4_loss_bbox: 0.2624, stage4_loss_iou: 0.6331, stage5_loss_cls: 0.5463, stage5_pos_acc: 82.0437, stage5_loss_bbox: 0.2631, stage5_loss_iou: 0.6323, loss: 10.4949, grad_norm: 49.4234
2023-02-03 03:26:59,788 - mmdet - INFO - Epoch [8][150/834]	lr: 2.500e-05, eta: 0:43:42, time: 0.665, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8588, stage0_pos_acc: 66.9068, stage0_loss_bbox: 0.7456, stage0_loss_iou: 1.3953, stage1_loss_cls: 0.5786, stage1_pos_acc: 79.8827, stage1_loss_bbox: 0.3248, stage1_loss_iou: 0.7802, stage2_loss_cls: 0.5553, stage2_pos_acc: 81.9524, stage2_loss_bbox: 0.2808, stage2_loss_iou: 0.6740, stage3_loss_cls: 0.5493, stage3_pos_acc: 83.0457, stage3_loss_bbox: 0.2713, stage3_loss_iou: 0.6581, stage4_loss_cls: 0.5468, stage4_pos_acc: 82.7279, stage4_loss_bbox: 0.2730, stage4_loss_iou: 0.6581, stage5_loss_cls: 0.5478, stage5_pos_acc: 83.2893, stage5_loss_bbox: 0.2710, stage5_loss_iou: 0.6531, loss: 10.6219, grad_norm: 50.1411
2023-02-03 03:27:32,834 - mmdet - INFO - Epoch [8][200/834]	lr: 2.500e-05, eta: 0:43:09, time: 0.661, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8175, stage0_pos_acc: 69.7469, stage0_loss_bbox: 0.7395, stage0_loss_iou: 1.4342, stage1_loss_cls: 0.5637, stage1_pos_acc: 79.4622, stage1_loss_bbox: 0.3125, stage1_loss_iou: 0.7831, stage2_loss_cls: 0.5297, stage2_pos_acc: 82.4373, stage2_loss_bbox: 0.2696, stage2_loss_iou: 0.6809, stage3_loss_cls: 0.5192, stage3_pos_acc: 84.7822, stage3_loss_bbox: 0.2648, stage3_loss_iou: 0.6685, stage4_loss_cls: 0.5150, stage4_pos_acc: 83.6828, stage4_loss_bbox: 0.2650, stage4_loss_iou: 0.6654, stage5_loss_cls: 0.5160, stage5_pos_acc: 84.2721, stage5_loss_bbox: 0.2636, stage5_loss_iou: 0.6619, loss: 10.4701, grad_norm: 48.5661
2023-02-03 03:28:06,178 - mmdet - INFO - Epoch [8][250/834]	lr: 2.500e-05, eta: 0:42:37, time: 0.667, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.8055, stage0_pos_acc: 73.2889, stage0_loss_bbox: 0.7613, stage0_loss_iou: 1.4578, stage1_loss_cls: 0.5577, stage1_pos_acc: 81.7424, stage1_loss_bbox: 0.3334, stage1_loss_iou: 0.8190, stage2_loss_cls: 0.5310, stage2_pos_acc: 84.5221, stage2_loss_bbox: 0.2879, stage2_loss_iou: 0.7033, stage3_loss_cls: 0.5193, stage3_pos_acc: 85.8687, stage3_loss_bbox: 0.2818, stage3_loss_iou: 0.6910, stage4_loss_cls: 0.5191, stage4_pos_acc: 86.3597, stage4_loss_bbox: 0.2811, stage4_loss_iou: 0.6866, stage5_loss_cls: 0.5197, stage5_pos_acc: 85.4116, stage5_loss_bbox: 0.2808, stage5_loss_iou: 0.6855, loss: 10.7217, grad_norm: 47.8277
2023-02-03 03:28:39,519 - mmdet - INFO - Epoch [8][300/834]	lr: 2.500e-05, eta: 0:42:05, time: 0.667, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.8131, stage0_pos_acc: 72.4034, stage0_loss_bbox: 0.7266, stage0_loss_iou: 1.4350, stage1_loss_cls: 0.5541, stage1_pos_acc: 81.0921, stage1_loss_bbox: 0.3088, stage1_loss_iou: 0.7761, stage2_loss_cls: 0.5247, stage2_pos_acc: 83.4470, stage2_loss_bbox: 0.2585, stage2_loss_iou: 0.6492, stage3_loss_cls: 0.5213, stage3_pos_acc: 84.2577, stage3_loss_bbox: 0.2529, stage3_loss_iou: 0.6321, stage4_loss_cls: 0.5182, stage4_pos_acc: 83.7790, stage4_loss_bbox: 0.2541, stage4_loss_iou: 0.6335, stage5_loss_cls: 0.5170, stage5_pos_acc: 83.3688, stage5_loss_bbox: 0.2535, stage5_loss_iou: 0.6308, loss: 10.2596, grad_norm: 49.2840
2023-02-03 03:29:12,359 - mmdet - INFO - Epoch [8][350/834]	lr: 2.500e-05, eta: 0:41:33, time: 0.657, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7929, stage0_pos_acc: 72.1267, stage0_loss_bbox: 0.7413, stage0_loss_iou: 1.4193, stage1_loss_cls: 0.5664, stage1_pos_acc: 80.5794, stage1_loss_bbox: 0.3254, stage1_loss_iou: 0.7844, stage2_loss_cls: 0.5365, stage2_pos_acc: 82.2140, stage2_loss_bbox: 0.2794, stage2_loss_iou: 0.6733, stage3_loss_cls: 0.5305, stage3_pos_acc: 81.9257, stage3_loss_bbox: 0.2712, stage3_loss_iou: 0.6576, stage4_loss_cls: 0.5302, stage4_pos_acc: 82.3926, stage4_loss_bbox: 0.2702, stage4_loss_iou: 0.6535, stage5_loss_cls: 0.5323, stage5_pos_acc: 82.9158, stage5_loss_bbox: 0.2680, stage5_loss_iou: 0.6513, loss: 10.4838, grad_norm: 50.2625
2023-02-03 03:29:45,723 - mmdet - INFO - Epoch [8][400/834]	lr: 2.500e-05, eta: 0:41:00, time: 0.667, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8225, stage0_pos_acc: 70.4962, stage0_loss_bbox: 0.7523, stage0_loss_iou: 1.4373, stage1_loss_cls: 0.5796, stage1_pos_acc: 80.4188, stage1_loss_bbox: 0.3217, stage1_loss_iou: 0.7756, stage2_loss_cls: 0.5526, stage2_pos_acc: 83.6099, stage2_loss_bbox: 0.2747, stage2_loss_iou: 0.6696, stage3_loss_cls: 0.5469, stage3_pos_acc: 84.6669, stage3_loss_bbox: 0.2660, stage3_loss_iou: 0.6516, stage4_loss_cls: 0.5434, stage4_pos_acc: 84.1619, stage4_loss_bbox: 0.2640, stage4_loss_iou: 0.6469, stage5_loss_cls: 0.5518, stage5_pos_acc: 83.5338, stage5_loss_bbox: 0.2658, stage5_loss_iou: 0.6481, loss: 10.5703, grad_norm: 50.4103
2023-02-03 03:30:19,114 - mmdet - INFO - Epoch [8][450/834]	lr: 2.500e-05, eta: 0:40:28, time: 0.667, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8005, stage0_pos_acc: 70.4808, stage0_loss_bbox: 0.7203, stage0_loss_iou: 1.4008, stage1_loss_cls: 0.5666, stage1_pos_acc: 78.8901, stage1_loss_bbox: 0.3230, stage1_loss_iou: 0.7656, stage2_loss_cls: 0.5422, stage2_pos_acc: 80.2604, stage2_loss_bbox: 0.2828, stage2_loss_iou: 0.6696, stage3_loss_cls: 0.5371, stage3_pos_acc: 80.3514, stage3_loss_bbox: 0.2742, stage3_loss_iou: 0.6544, stage4_loss_cls: 0.5372, stage4_pos_acc: 80.1251, stage4_loss_bbox: 0.2714, stage4_loss_iou: 0.6474, stage5_loss_cls: 0.5376, stage5_pos_acc: 80.5912, stage5_loss_bbox: 0.2709, stage5_loss_iou: 0.6437, loss: 10.4455, grad_norm: 47.7063
2023-02-03 03:30:52,335 - mmdet - INFO - Epoch [8][500/834]	lr: 2.500e-05, eta: 0:39:56, time: 0.665, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.8360, stage0_pos_acc: 70.7471, stage0_loss_bbox: 0.7438, stage0_loss_iou: 1.3993, stage1_loss_cls: 0.5792, stage1_pos_acc: 80.4862, stage1_loss_bbox: 0.3358, stage1_loss_iou: 0.7947, stage2_loss_cls: 0.5515, stage2_pos_acc: 82.0794, stage2_loss_bbox: 0.2861, stage2_loss_iou: 0.6816, stage3_loss_cls: 0.5433, stage3_pos_acc: 82.5694, stage3_loss_bbox: 0.2794, stage3_loss_iou: 0.6661, stage4_loss_cls: 0.5364, stage4_pos_acc: 82.5887, stage4_loss_bbox: 0.2812, stage4_loss_iou: 0.6681, stage5_loss_cls: 0.5382, stage5_pos_acc: 82.2675, stage5_loss_bbox: 0.2790, stage5_loss_iou: 0.6645, loss: 10.6641, grad_norm: 49.7010
2023-02-03 03:31:25,462 - mmdet - INFO - Epoch [8][550/834]	lr: 2.500e-05, eta: 0:39:23, time: 0.663, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7857, stage0_pos_acc: 71.0858, stage0_loss_bbox: 0.7392, stage0_loss_iou: 1.4464, stage1_loss_cls: 0.5379, stage1_pos_acc: 81.3138, stage1_loss_bbox: 0.3097, stage1_loss_iou: 0.8046, stage2_loss_cls: 0.5150, stage2_pos_acc: 82.9994, stage2_loss_bbox: 0.2640, stage2_loss_iou: 0.6882, stage3_loss_cls: 0.5065, stage3_pos_acc: 83.6664, stage3_loss_bbox: 0.2571, stage3_loss_iou: 0.6726, stage4_loss_cls: 0.5059, stage4_pos_acc: 83.5631, stage4_loss_bbox: 0.2544, stage4_loss_iou: 0.6671, stage5_loss_cls: 0.5089, stage5_pos_acc: 83.6171, stage5_loss_bbox: 0.2532, stage5_loss_iou: 0.6659, loss: 10.3824, grad_norm: 44.7922
2023-02-03 03:31:58,507 - mmdet - INFO - Epoch [8][600/834]	lr: 2.500e-05, eta: 0:38:51, time: 0.661, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.8197, stage0_pos_acc: 71.9950, stage0_loss_bbox: 0.7310, stage0_loss_iou: 1.4092, stage1_loss_cls: 0.5741, stage1_pos_acc: 78.8984, stage1_loss_bbox: 0.3189, stage1_loss_iou: 0.7696, stage2_loss_cls: 0.5452, stage2_pos_acc: 81.3793, stage2_loss_bbox: 0.2724, stage2_loss_iou: 0.6617, stage3_loss_cls: 0.5328, stage3_pos_acc: 82.9176, stage3_loss_bbox: 0.2643, stage3_loss_iou: 0.6463, stage4_loss_cls: 0.5317, stage4_pos_acc: 84.1639, stage4_loss_bbox: 0.2625, stage4_loss_iou: 0.6415, stage5_loss_cls: 0.5311, stage5_pos_acc: 83.5966, stage5_loss_bbox: 0.2599, stage5_loss_iou: 0.6374, loss: 10.4093, grad_norm: 48.7723
2023-02-03 03:32:31,833 - mmdet - INFO - Epoch [8][650/834]	lr: 2.500e-05, eta: 0:38:19, time: 0.667, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7999, stage0_pos_acc: 69.7098, stage0_loss_bbox: 0.7464, stage0_loss_iou: 1.4569, stage1_loss_cls: 0.5778, stage1_pos_acc: 77.4756, stage1_loss_bbox: 0.3243, stage1_loss_iou: 0.8268, stage2_loss_cls: 0.5585, stage2_pos_acc: 78.6351, stage2_loss_bbox: 0.2806, stage2_loss_iou: 0.7152, stage3_loss_cls: 0.5528, stage3_pos_acc: 79.5053, stage3_loss_bbox: 0.2724, stage3_loss_iou: 0.6987, stage4_loss_cls: 0.5482, stage4_pos_acc: 79.1946, stage4_loss_bbox: 0.2738, stage4_loss_iou: 0.6960, stage5_loss_cls: 0.5503, stage5_pos_acc: 79.7233, stage5_loss_bbox: 0.2734, stage5_loss_iou: 0.6939, loss: 10.8461, grad_norm: 49.2193
2023-02-03 03:33:04,822 - mmdet - INFO - Epoch [8][700/834]	lr: 2.500e-05, eta: 0:37:46, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7965, stage0_pos_acc: 70.5532, stage0_loss_bbox: 0.7410, stage0_loss_iou: 1.4223, stage1_loss_cls: 0.5486, stage1_pos_acc: 82.1298, stage1_loss_bbox: 0.3150, stage1_loss_iou: 0.7984, stage2_loss_cls: 0.5300, stage2_pos_acc: 83.1518, stage2_loss_bbox: 0.2685, stage2_loss_iou: 0.6809, stage3_loss_cls: 0.5234, stage3_pos_acc: 84.2112, stage3_loss_bbox: 0.2609, stage3_loss_iou: 0.6640, stage4_loss_cls: 0.5216, stage4_pos_acc: 83.5344, stage4_loss_bbox: 0.2590, stage4_loss_iou: 0.6580, stage5_loss_cls: 0.5236, stage5_pos_acc: 83.8439, stage5_loss_bbox: 0.2603, stage5_loss_iou: 0.6595, loss: 10.4312, grad_norm: 45.5822
2023-02-03 03:33:37,916 - mmdet - INFO - Epoch [8][750/834]	lr: 2.500e-05, eta: 0:37:14, time: 0.662, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7975, stage0_pos_acc: 75.5173, stage0_loss_bbox: 0.7325, stage0_loss_iou: 1.4453, stage1_loss_cls: 0.5410, stage1_pos_acc: 82.4834, stage1_loss_bbox: 0.3054, stage1_loss_iou: 0.7747, stage2_loss_cls: 0.5237, stage2_pos_acc: 84.4871, stage2_loss_bbox: 0.2615, stage2_loss_iou: 0.6638, stage3_loss_cls: 0.5170, stage3_pos_acc: 85.2943, stage3_loss_bbox: 0.2512, stage3_loss_iou: 0.6461, stage4_loss_cls: 0.5102, stage4_pos_acc: 85.6707, stage4_loss_bbox: 0.2536, stage4_loss_iou: 0.6475, stage5_loss_cls: 0.5075, stage5_pos_acc: 85.7082, stage5_loss_bbox: 0.2528, stage5_loss_iou: 0.6469, loss: 10.2782, grad_norm: 50.3065
2023-02-03 03:34:11,211 - mmdet - INFO - Epoch [8][800/834]	lr: 2.500e-05, eta: 0:36:41, time: 0.666, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7783, stage0_pos_acc: 73.8611, stage0_loss_bbox: 0.7468, stage0_loss_iou: 1.4434, stage1_loss_cls: 0.5364, stage1_pos_acc: 83.2383, stage1_loss_bbox: 0.3263, stage1_loss_iou: 0.8048, stage2_loss_cls: 0.5178, stage2_pos_acc: 83.4217, stage2_loss_bbox: 0.2824, stage2_loss_iou: 0.6960, stage3_loss_cls: 0.5146, stage3_pos_acc: 84.2786, stage3_loss_bbox: 0.2734, stage3_loss_iou: 0.6753, stage4_loss_cls: 0.5120, stage4_pos_acc: 84.3194, stage4_loss_bbox: 0.2723, stage4_loss_iou: 0.6729, stage5_loss_cls: 0.5127, stage5_pos_acc: 85.1797, stage5_loss_bbox: 0.2737, stage5_loss_iou: 0.6718, loss: 10.5110, grad_norm: 44.9735
2023-02-03 03:34:34,656 - mmdet - INFO - Saving checkpoint at 8 epochs
2023-02-03 03:35:06,111 - mmdet - INFO - Evaluating bbox...
2023-02-03 03:35:18,798 - mmdet - INFO - Exp name: sparse_rcnn_r50_fpn_1x_coco_comparison.py
2023-02-03 03:35:18,799 - mmdet - INFO - Epoch(val) [8][186]	bbox_mAP: 0.1950, bbox_mAP_50: 0.4030, bbox_mAP_75: 0.1490, bbox_mAP_s: 0.0050, bbox_mAP_m: 0.1240, bbox_mAP_l: 0.2160, bbox_mAP_copypaste: 0.195 0.403 0.149 0.005 0.124 0.216
2023-02-03 03:36:05,408 - mmdet - INFO - Epoch [9][50/834]	lr: 2.500e-06, eta: 0:35:43, time: 0.932, data_time: 0.266, memory: 5033, stage0_loss_cls: 0.7634, stage0_pos_acc: 74.0963, stage0_loss_bbox: 0.7264, stage0_loss_iou: 1.4283, stage1_loss_cls: 0.5194, stage1_pos_acc: 82.6031, stage1_loss_bbox: 0.2992, stage1_loss_iou: 0.7632, stage2_loss_cls: 0.4874, stage2_pos_acc: 84.9382, stage2_loss_bbox: 0.2566, stage2_loss_iou: 0.6617, stage3_loss_cls: 0.4780, stage3_pos_acc: 85.8006, stage3_loss_bbox: 0.2486, stage3_loss_iou: 0.6463, stage4_loss_cls: 0.4765, stage4_pos_acc: 85.9689, stage4_loss_bbox: 0.2459, stage4_loss_iou: 0.6395, stage5_loss_cls: 0.4795, stage5_pos_acc: 86.1974, stage5_loss_bbox: 0.2444, stage5_loss_iou: 0.6388, loss: 10.0032, grad_norm: 42.5723
2023-02-03 03:36:38,793 - mmdet - INFO - Epoch [9][100/834]	lr: 2.500e-06, eta: 0:35:10, time: 0.668, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7974, stage0_pos_acc: 73.7690, stage0_loss_bbox: 0.7083, stage0_loss_iou: 1.4008, stage1_loss_cls: 0.5271, stage1_pos_acc: 83.4321, stage1_loss_bbox: 0.2848, stage1_loss_iou: 0.7292, stage2_loss_cls: 0.4949, stage2_pos_acc: 86.1962, stage2_loss_bbox: 0.2444, stage2_loss_iou: 0.6254, stage3_loss_cls: 0.4884, stage3_pos_acc: 86.7697, stage3_loss_bbox: 0.2373, stage3_loss_iou: 0.6101, stage4_loss_cls: 0.4857, stage4_pos_acc: 86.8451, stage4_loss_bbox: 0.2347, stage4_loss_iou: 0.6035, stage5_loss_cls: 0.4874, stage5_pos_acc: 86.6686, stage5_loss_bbox: 0.2342, stage5_loss_iou: 0.6010, loss: 9.7947, grad_norm: 41.8706
2023-02-03 03:37:11,825 - mmdet - INFO - Epoch [9][150/834]	lr: 2.500e-06, eta: 0:34:38, time: 0.661, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7807, stage0_pos_acc: 73.0506, stage0_loss_bbox: 0.7014, stage0_loss_iou: 1.3836, stage1_loss_cls: 0.4985, stage1_pos_acc: 83.6709, stage1_loss_bbox: 0.2833, stage1_loss_iou: 0.7194, stage2_loss_cls: 0.4747, stage2_pos_acc: 85.3890, stage2_loss_bbox: 0.2453, stage2_loss_iou: 0.6276, stage3_loss_cls: 0.4654, stage3_pos_acc: 85.9942, stage3_loss_bbox: 0.2399, stage3_loss_iou: 0.6168, stage4_loss_cls: 0.4611, stage4_pos_acc: 86.1044, stage4_loss_bbox: 0.2385, stage4_loss_iou: 0.6136, stage5_loss_cls: 0.4630, stage5_pos_acc: 86.5317, stage5_loss_bbox: 0.2373, stage5_loss_iou: 0.6116, loss: 9.6618, grad_norm: 44.4727
2023-02-03 03:37:45,239 - mmdet - INFO - Epoch [9][200/834]	lr: 2.500e-06, eta: 0:34:06, time: 0.668, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7802, stage0_pos_acc: 74.0171, stage0_loss_bbox: 0.6974, stage0_loss_iou: 1.3587, stage1_loss_cls: 0.5152, stage1_pos_acc: 82.0669, stage1_loss_bbox: 0.2786, stage1_loss_iou: 0.7014, stage2_loss_cls: 0.4908, stage2_pos_acc: 83.8360, stage2_loss_bbox: 0.2412, stage2_loss_iou: 0.6097, stage3_loss_cls: 0.4831, stage3_pos_acc: 84.1493, stage3_loss_bbox: 0.2357, stage3_loss_iou: 0.5951, stage4_loss_cls: 0.4808, stage4_pos_acc: 84.2509, stage4_loss_bbox: 0.2338, stage4_loss_iou: 0.5911, stage5_loss_cls: 0.4830, stage5_pos_acc: 84.1188, stage5_loss_bbox: 0.2339, stage5_loss_iou: 0.5904, loss: 9.6000, grad_norm: 43.5498
2023-02-03 03:38:18,300 - mmdet - INFO - Epoch [9][250/834]	lr: 2.500e-06, eta: 0:33:33, time: 0.661, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7747, stage0_pos_acc: 73.7519, stage0_loss_bbox: 0.6999, stage0_loss_iou: 1.3469, stage1_loss_cls: 0.5143, stage1_pos_acc: 84.1473, stage1_loss_bbox: 0.2869, stage1_loss_iou: 0.6906, stage2_loss_cls: 0.4842, stage2_pos_acc: 85.9045, stage2_loss_bbox: 0.2527, stage2_loss_iou: 0.6044, stage3_loss_cls: 0.4777, stage3_pos_acc: 86.4900, stage3_loss_bbox: 0.2458, stage3_loss_iou: 0.5911, stage4_loss_cls: 0.4764, stage4_pos_acc: 86.6105, stage4_loss_bbox: 0.2452, stage4_loss_iou: 0.5872, stage5_loss_cls: 0.4790, stage5_pos_acc: 86.5243, stage5_loss_bbox: 0.2447, stage5_loss_iou: 0.5863, loss: 9.5879, grad_norm: 47.6884
2023-02-03 03:38:51,489 - mmdet - INFO - Epoch [9][300/834]	lr: 2.500e-06, eta: 0:33:01, time: 0.664, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7606, stage0_pos_acc: 75.5495, stage0_loss_bbox: 0.7000, stage0_loss_iou: 1.3775, stage1_loss_cls: 0.5001, stage1_pos_acc: 83.5288, stage1_loss_bbox: 0.2838, stage1_loss_iou: 0.7103, stage2_loss_cls: 0.4786, stage2_pos_acc: 85.6126, stage2_loss_bbox: 0.2481, stage2_loss_iou: 0.6218, stage3_loss_cls: 0.4687, stage3_pos_acc: 86.8969, stage3_loss_bbox: 0.2433, stage3_loss_iou: 0.6121, stage4_loss_cls: 0.4656, stage4_pos_acc: 87.1669, stage4_loss_bbox: 0.2428, stage4_loss_iou: 0.6102, stage5_loss_cls: 0.4641, stage5_pos_acc: 87.7340, stage5_loss_bbox: 0.2449, stage5_loss_iou: 0.6119, loss: 9.6443, grad_norm: 44.6918
2023-02-03 03:39:24,703 - mmdet - INFO - Epoch [9][350/834]	lr: 2.500e-06, eta: 0:32:28, time: 0.664, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7757, stage0_pos_acc: 71.8387, stage0_loss_bbox: 0.6975, stage0_loss_iou: 1.3744, stage1_loss_cls: 0.5096, stage1_pos_acc: 82.8561, stage1_loss_bbox: 0.2843, stage1_loss_iou: 0.7137, stage2_loss_cls: 0.4794, stage2_pos_acc: 83.5834, stage2_loss_bbox: 0.2426, stage2_loss_iou: 0.6168, stage3_loss_cls: 0.4747, stage3_pos_acc: 83.3884, stage3_loss_bbox: 0.2350, stage3_loss_iou: 0.5988, stage4_loss_cls: 0.4695, stage4_pos_acc: 84.0640, stage4_loss_bbox: 0.2340, stage4_loss_iou: 0.5943, stage5_loss_cls: 0.4708, stage5_pos_acc: 84.3383, stage5_loss_bbox: 0.2332, stage5_loss_iou: 0.5932, loss: 9.5976, grad_norm: 42.5983
2023-02-03 03:39:57,891 - mmdet - INFO - Epoch [9][400/834]	lr: 2.500e-06, eta: 0:31:56, time: 0.664, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7815, stage0_pos_acc: 72.4264, stage0_loss_bbox: 0.6786, stage0_loss_iou: 1.3612, stage1_loss_cls: 0.5186, stage1_pos_acc: 81.8887, stage1_loss_bbox: 0.2727, stage1_loss_iou: 0.7042, stage2_loss_cls: 0.4874, stage2_pos_acc: 84.3500, stage2_loss_bbox: 0.2391, stage2_loss_iou: 0.6158, stage3_loss_cls: 0.4810, stage3_pos_acc: 84.6537, stage3_loss_bbox: 0.2318, stage3_loss_iou: 0.6011, stage4_loss_cls: 0.4754, stage4_pos_acc: 84.4797, stage4_loss_bbox: 0.2306, stage4_loss_iou: 0.5986, stage5_loss_cls: 0.4772, stage5_pos_acc: 84.4243, stage5_loss_bbox: 0.2298, stage5_loss_iou: 0.5987, loss: 9.5832, grad_norm: 42.4239
2023-02-03 03:40:30,990 - mmdet - INFO - Epoch [9][450/834]	lr: 2.500e-06, eta: 0:31:24, time: 0.662, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7579, stage0_pos_acc: 75.8008, stage0_loss_bbox: 0.6784, stage0_loss_iou: 1.3759, stage1_loss_cls: 0.4976, stage1_pos_acc: 85.7825, stage1_loss_bbox: 0.2839, stage1_loss_iou: 0.7148, stage2_loss_cls: 0.4698, stage2_pos_acc: 87.6244, stage2_loss_bbox: 0.2479, stage2_loss_iou: 0.6244, stage3_loss_cls: 0.4648, stage3_pos_acc: 87.4756, stage3_loss_bbox: 0.2417, stage3_loss_iou: 0.6108, stage4_loss_cls: 0.4603, stage4_pos_acc: 87.6709, stage4_loss_bbox: 0.2411, stage4_loss_iou: 0.6099, stage5_loss_cls: 0.4615, stage5_pos_acc: 87.3218, stage5_loss_bbox: 0.2398, stage5_loss_iou: 0.6078, loss: 9.5883, grad_norm: 41.2286
2023-02-03 03:41:04,089 - mmdet - INFO - Epoch [9][500/834]	lr: 2.500e-06, eta: 0:30:51, time: 0.662, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7761, stage0_pos_acc: 75.4724, stage0_loss_bbox: 0.6982, stage0_loss_iou: 1.3788, stage1_loss_cls: 0.5002, stage1_pos_acc: 85.4892, stage1_loss_bbox: 0.2756, stage1_loss_iou: 0.7199, stage2_loss_cls: 0.4738, stage2_pos_acc: 88.1666, stage2_loss_bbox: 0.2434, stage2_loss_iou: 0.6304, stage3_loss_cls: 0.4661, stage3_pos_acc: 87.9973, stage3_loss_bbox: 0.2375, stage3_loss_iou: 0.6175, stage4_loss_cls: 0.4639, stage4_pos_acc: 87.9956, stage4_loss_bbox: 0.2376, stage4_loss_iou: 0.6140, stage5_loss_cls: 0.4664, stage5_pos_acc: 87.5658, stage5_loss_bbox: 0.2344, stage5_loss_iou: 0.6112, loss: 9.6451, grad_norm: 41.3595
2023-02-03 03:41:37,154 - mmdet - INFO - Epoch [9][550/834]	lr: 2.500e-06, eta: 0:30:19, time: 0.661, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7716, stage0_pos_acc: 75.0562, stage0_loss_bbox: 0.6832, stage0_loss_iou: 1.3969, stage1_loss_cls: 0.4936, stage1_pos_acc: 85.1365, stage1_loss_bbox: 0.2676, stage1_loss_iou: 0.7292, stage2_loss_cls: 0.4688, stage2_pos_acc: 84.8960, stage2_loss_bbox: 0.2336, stage2_loss_iou: 0.6380, stage3_loss_cls: 0.4601, stage3_pos_acc: 85.2565, stage3_loss_bbox: 0.2288, stage3_loss_iou: 0.6241, stage4_loss_cls: 0.4574, stage4_pos_acc: 85.8207, stage4_loss_bbox: 0.2267, stage4_loss_iou: 0.6204, stage5_loss_cls: 0.4558, stage5_pos_acc: 85.5505, stage5_loss_bbox: 0.2272, stage5_loss_iou: 0.6207, loss: 9.6035, grad_norm: 40.3628
2023-02-03 03:42:10,144 - mmdet - INFO - Epoch [9][600/834]	lr: 2.500e-06, eta: 0:29:46, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7775, stage0_pos_acc: 74.9254, stage0_loss_bbox: 0.6916, stage0_loss_iou: 1.3373, stage1_loss_cls: 0.4943, stage1_pos_acc: 85.4687, stage1_loss_bbox: 0.2822, stage1_loss_iou: 0.6873, stage2_loss_cls: 0.4667, stage2_pos_acc: 87.5029, stage2_loss_bbox: 0.2476, stage2_loss_iou: 0.5987, stage3_loss_cls: 0.4565, stage3_pos_acc: 87.5562, stage3_loss_bbox: 0.2415, stage3_loss_iou: 0.5864, stage4_loss_cls: 0.4526, stage4_pos_acc: 87.8554, stage4_loss_bbox: 0.2385, stage4_loss_iou: 0.5830, stage5_loss_cls: 0.4531, stage5_pos_acc: 87.6769, stage5_loss_bbox: 0.2373, stage5_loss_iou: 0.5820, loss: 9.4143, grad_norm: 42.7427
2023-02-03 03:42:43,083 - mmdet - INFO - Epoch [9][650/834]	lr: 2.500e-06, eta: 0:29:14, time: 0.659, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7858, stage0_pos_acc: 75.3062, stage0_loss_bbox: 0.6757, stage0_loss_iou: 1.3630, stage1_loss_cls: 0.4981, stage1_pos_acc: 85.2244, stage1_loss_bbox: 0.2717, stage1_loss_iou: 0.6944, stage2_loss_cls: 0.4677, stage2_pos_acc: 86.9697, stage2_loss_bbox: 0.2351, stage2_loss_iou: 0.6051, stage3_loss_cls: 0.4624, stage3_pos_acc: 87.3283, stage3_loss_bbox: 0.2276, stage3_loss_iou: 0.5897, stage4_loss_cls: 0.4567, stage4_pos_acc: 87.5698, stage4_loss_bbox: 0.2280, stage4_loss_iou: 0.5861, stage5_loss_cls: 0.4591, stage5_pos_acc: 87.5432, stage5_loss_bbox: 0.2261, stage5_loss_iou: 0.5852, loss: 9.4175, grad_norm: 42.7601
2023-02-03 03:43:16,486 - mmdet - INFO - Epoch [9][700/834]	lr: 2.500e-06, eta: 0:28:41, time: 0.668, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7728, stage0_pos_acc: 77.2697, stage0_loss_bbox: 0.6844, stage0_loss_iou: 1.3835, stage1_loss_cls: 0.4944, stage1_pos_acc: 86.0933, stage1_loss_bbox: 0.2668, stage1_loss_iou: 0.6960, stage2_loss_cls: 0.4675, stage2_pos_acc: 87.1475, stage2_loss_bbox: 0.2360, stage2_loss_iou: 0.6111, stage3_loss_cls: 0.4638, stage3_pos_acc: 87.1520, stage3_loss_bbox: 0.2303, stage3_loss_iou: 0.5948, stage4_loss_cls: 0.4583, stage4_pos_acc: 87.5581, stage4_loss_bbox: 0.2276, stage4_loss_iou: 0.5926, stage5_loss_cls: 0.4590, stage5_pos_acc: 87.5590, stage5_loss_bbox: 0.2260, stage5_loss_iou: 0.5899, loss: 9.4547, grad_norm: 49.3440
2023-02-03 03:43:49,793 - mmdet - INFO - Epoch [9][750/834]	lr: 2.500e-06, eta: 0:28:09, time: 0.666, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8030, stage0_pos_acc: 75.0998, stage0_loss_bbox: 0.6807, stage0_loss_iou: 1.3526, stage1_loss_cls: 0.5056, stage1_pos_acc: 85.4986, stage1_loss_bbox: 0.2754, stage1_loss_iou: 0.7017, stage2_loss_cls: 0.4740, stage2_pos_acc: 87.2265, stage2_loss_bbox: 0.2452, stage2_loss_iou: 0.6219, stage3_loss_cls: 0.4673, stage3_pos_acc: 87.6417, stage3_loss_bbox: 0.2386, stage3_loss_iou: 0.6085, stage4_loss_cls: 0.4655, stage4_pos_acc: 87.8578, stage4_loss_bbox: 0.2378, stage4_loss_iou: 0.6063, stage5_loss_cls: 0.4624, stage5_pos_acc: 88.3428, stage5_loss_bbox: 0.2383, stage5_loss_iou: 0.6040, loss: 9.5888, grad_norm: 44.6299
2023-02-03 03:44:22,931 - mmdet - INFO - Epoch [9][800/834]	lr: 2.500e-06, eta: 0:27:36, time: 0.663, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7989, stage0_pos_acc: 72.5046, stage0_loss_bbox: 0.6916, stage0_loss_iou: 1.3549, stage1_loss_cls: 0.5031, stage1_pos_acc: 83.7107, stage1_loss_bbox: 0.2760, stage1_loss_iou: 0.6923, stage2_loss_cls: 0.4751, stage2_pos_acc: 85.4413, stage2_loss_bbox: 0.2319, stage2_loss_iou: 0.5952, stage3_loss_cls: 0.4713, stage3_pos_acc: 86.2319, stage3_loss_bbox: 0.2247, stage3_loss_iou: 0.5806, stage4_loss_cls: 0.4625, stage4_pos_acc: 86.4953, stage4_loss_bbox: 0.2254, stage4_loss_iou: 0.5788, stage5_loss_cls: 0.4655, stage5_pos_acc: 86.7821, stage5_loss_bbox: 0.2226, stage5_loss_iou: 0.5749, loss: 9.4255, grad_norm: 43.7179
2023-02-03 03:45:14,485 - mmdet - INFO - Evaluating bbox...
2023-02-03 03:45:27,952 - mmdet - INFO - Exp name: sparse_rcnn_r50_fpn_1x_coco_comparison.py
2023-02-03 03:45:27,953 - mmdet - INFO - Epoch(val) [9][186]	bbox_mAP: 0.2210, bbox_mAP_50: 0.4390, bbox_mAP_75: 0.1920, bbox_mAP_s: 0.0080, bbox_mAP_m: 0.1360, bbox_mAP_l: 0.2260, bbox_mAP_copypaste: 0.221 0.439 0.192 0.008 0.136 0.226
2023-02-03 03:46:14,003 - mmdet - INFO - Epoch [10][50/834]	lr: 2.500e-06, eta: 0:26:38, time: 0.920, data_time: 0.267, memory: 5033, stage0_loss_cls: 0.7631, stage0_pos_acc: 77.8270, stage0_loss_bbox: 0.6697, stage0_loss_iou: 1.3687, stage1_loss_cls: 0.4815, stage1_pos_acc: 86.0417, stage1_loss_bbox: 0.2567, stage1_loss_iou: 0.6823, stage2_loss_cls: 0.4593, stage2_pos_acc: 86.2921, stage2_loss_bbox: 0.2200, stage2_loss_iou: 0.5881, stage3_loss_cls: 0.4502, stage3_pos_acc: 86.8736, stage3_loss_bbox: 0.2135, stage3_loss_iou: 0.5731, stage4_loss_cls: 0.4442, stage4_pos_acc: 87.2200, stage4_loss_bbox: 0.2130, stage4_loss_iou: 0.5709, stage5_loss_cls: 0.4441, stage5_pos_acc: 86.5133, stage5_loss_bbox: 0.2131, stage5_loss_iou: 0.5701, loss: 9.1817, grad_norm: 42.6673
2023-02-03 03:46:47,140 - mmdet - INFO - Epoch [10][100/834]	lr: 2.500e-06, eta: 0:26:06, time: 0.663, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7814, stage0_pos_acc: 76.8221, stage0_loss_bbox: 0.6737, stage0_loss_iou: 1.3249, stage1_loss_cls: 0.4985, stage1_pos_acc: 86.2565, stage1_loss_bbox: 0.2808, stage1_loss_iou: 0.6789, stage2_loss_cls: 0.4719, stage2_pos_acc: 87.3148, stage2_loss_bbox: 0.2452, stage2_loss_iou: 0.6019, stage3_loss_cls: 0.4624, stage3_pos_acc: 87.4093, stage3_loss_bbox: 0.2405, stage3_loss_iou: 0.5889, stage4_loss_cls: 0.4571, stage4_pos_acc: 87.7932, stage4_loss_bbox: 0.2389, stage4_loss_iou: 0.5875, stage5_loss_cls: 0.4572, stage5_pos_acc: 88.2760, stage5_loss_bbox: 0.2392, stage5_loss_iou: 0.5861, loss: 9.4150, grad_norm: 46.8709
2023-02-03 03:47:20,157 - mmdet - INFO - Epoch [10][150/834]	lr: 2.500e-06, eta: 0:25:34, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7751, stage0_pos_acc: 76.9788, stage0_loss_bbox: 0.6742, stage0_loss_iou: 1.3534, stage1_loss_cls: 0.4890, stage1_pos_acc: 86.6256, stage1_loss_bbox: 0.2613, stage1_loss_iou: 0.6813, stage2_loss_cls: 0.4658, stage2_pos_acc: 87.3557, stage2_loss_bbox: 0.2272, stage2_loss_iou: 0.5963, stage3_loss_cls: 0.4542, stage3_pos_acc: 87.2984, stage3_loss_bbox: 0.2230, stage3_loss_iou: 0.5840, stage4_loss_cls: 0.4509, stage4_pos_acc: 87.8746, stage4_loss_bbox: 0.2215, stage4_loss_iou: 0.5788, stage5_loss_cls: 0.4505, stage5_pos_acc: 87.6569, stage5_loss_bbox: 0.2210, stage5_loss_iou: 0.5777, loss: 9.2852, grad_norm: 44.2374
2023-02-03 03:47:53,560 - mmdet - INFO - Epoch [10][200/834]	lr: 2.500e-06, eta: 0:25:01, time: 0.668, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7765, stage0_pos_acc: 74.9696, stage0_loss_bbox: 0.6847, stage0_loss_iou: 1.3607, stage1_loss_cls: 0.4917, stage1_pos_acc: 84.1909, stage1_loss_bbox: 0.2770, stage1_loss_iou: 0.7039, stage2_loss_cls: 0.4627, stage2_pos_acc: 86.4228, stage2_loss_bbox: 0.2434, stage2_loss_iou: 0.6182, stage3_loss_cls: 0.4542, stage3_pos_acc: 87.7737, stage3_loss_bbox: 0.2358, stage3_loss_iou: 0.6049, stage4_loss_cls: 0.4531, stage4_pos_acc: 87.6335, stage4_loss_bbox: 0.2342, stage4_loss_iou: 0.6006, stage5_loss_cls: 0.4525, stage5_pos_acc: 87.8764, stage5_loss_bbox: 0.2334, stage5_loss_iou: 0.6000, loss: 9.4873, grad_norm: 43.1444
2023-02-03 03:48:26,708 - mmdet - INFO - Epoch [10][250/834]	lr: 2.500e-06, eta: 0:24:29, time: 0.662, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7509, stage0_pos_acc: 74.0908, stage0_loss_bbox: 0.6728, stage0_loss_iou: 1.3995, stage1_loss_cls: 0.4839, stage1_pos_acc: 82.8679, stage1_loss_bbox: 0.2707, stage1_loss_iou: 0.7213, stage2_loss_cls: 0.4619, stage2_pos_acc: 84.2121, stage2_loss_bbox: 0.2379, stage2_loss_iou: 0.6326, stage3_loss_cls: 0.4519, stage3_pos_acc: 84.4004, stage3_loss_bbox: 0.2334, stage3_loss_iou: 0.6200, stage4_loss_cls: 0.4488, stage4_pos_acc: 84.0562, stage4_loss_bbox: 0.2312, stage4_loss_iou: 0.6154, stage5_loss_cls: 0.4484, stage5_pos_acc: 84.1924, stage5_loss_bbox: 0.2306, stage5_loss_iou: 0.6151, loss: 9.5264, grad_norm: 44.6581
2023-02-03 03:49:00,191 - mmdet - INFO - Epoch [10][300/834]	lr: 2.500e-06, eta: 0:23:56, time: 0.670, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7481, stage0_pos_acc: 77.7750, stage0_loss_bbox: 0.6750, stage0_loss_iou: 1.3605, stage1_loss_cls: 0.4811, stage1_pos_acc: 86.6071, stage1_loss_bbox: 0.2779, stage1_loss_iou: 0.7131, stage2_loss_cls: 0.4485, stage2_pos_acc: 88.0456, stage2_loss_bbox: 0.2475, stage2_loss_iou: 0.6318, stage3_loss_cls: 0.4438, stage3_pos_acc: 88.0720, stage3_loss_bbox: 0.2407, stage3_loss_iou: 0.6191, stage4_loss_cls: 0.4402, stage4_pos_acc: 88.7376, stage4_loss_bbox: 0.2389, stage4_loss_iou: 0.6154, stage5_loss_cls: 0.4410, stage5_pos_acc: 88.0581, stage5_loss_bbox: 0.2376, stage5_loss_iou: 0.6140, loss: 9.4742, grad_norm: 40.0311
2023-02-03 03:49:33,352 - mmdet - INFO - Epoch [10][350/834]	lr: 2.500e-06, eta: 0:23:24, time: 0.663, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7549, stage0_pos_acc: 76.2704, stage0_loss_bbox: 0.6717, stage0_loss_iou: 1.3755, stage1_loss_cls: 0.4747, stage1_pos_acc: 85.3450, stage1_loss_bbox: 0.2611, stage1_loss_iou: 0.6963, stage2_loss_cls: 0.4464, stage2_pos_acc: 86.9424, stage2_loss_bbox: 0.2264, stage2_loss_iou: 0.6024, stage3_loss_cls: 0.4406, stage3_pos_acc: 87.3991, stage3_loss_bbox: 0.2194, stage3_loss_iou: 0.5891, stage4_loss_cls: 0.4382, stage4_pos_acc: 87.5882, stage4_loss_bbox: 0.2171, stage4_loss_iou: 0.5843, stage5_loss_cls: 0.4376, stage5_pos_acc: 87.5330, stage5_loss_bbox: 0.2166, stage5_loss_iou: 0.5827, loss: 9.2350, grad_norm: 42.9281
2023-02-03 03:50:06,220 - mmdet - INFO - Epoch [10][400/834]	lr: 2.500e-06, eta: 0:22:51, time: 0.657, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7836, stage0_pos_acc: 70.8875, stage0_loss_bbox: 0.6678, stage0_loss_iou: 1.3554, stage1_loss_cls: 0.4826, stage1_pos_acc: 82.9479, stage1_loss_bbox: 0.2739, stage1_loss_iou: 0.6964, stage2_loss_cls: 0.4495, stage2_pos_acc: 85.3901, stage2_loss_bbox: 0.2384, stage2_loss_iou: 0.6096, stage3_loss_cls: 0.4395, stage3_pos_acc: 86.0141, stage3_loss_bbox: 0.2335, stage3_loss_iou: 0.5991, stage4_loss_cls: 0.4365, stage4_pos_acc: 86.6539, stage4_loss_bbox: 0.2317, stage4_loss_iou: 0.5959, stage5_loss_cls: 0.4369, stage5_pos_acc: 86.7612, stage5_loss_bbox: 0.2319, stage5_loss_iou: 0.5954, loss: 9.3576, grad_norm: 46.8598
2023-02-03 03:50:39,465 - mmdet - INFO - Epoch [10][450/834]	lr: 2.500e-06, eta: 0:22:19, time: 0.665, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7763, stage0_pos_acc: 74.2557, stage0_loss_bbox: 0.6854, stage0_loss_iou: 1.3659, stage1_loss_cls: 0.4857, stage1_pos_acc: 86.3727, stage1_loss_bbox: 0.2780, stage1_loss_iou: 0.7013, stage2_loss_cls: 0.4502, stage2_pos_acc: 89.0592, stage2_loss_bbox: 0.2457, stage2_loss_iou: 0.6161, stage3_loss_cls: 0.4450, stage3_pos_acc: 89.2102, stage3_loss_bbox: 0.2413, stage3_loss_iou: 0.6062, stage4_loss_cls: 0.4408, stage4_pos_acc: 89.1356, stage4_loss_bbox: 0.2404, stage4_loss_iou: 0.6050, stage5_loss_cls: 0.4401, stage5_pos_acc: 89.4945, stage5_loss_bbox: 0.2403, stage5_loss_iou: 0.6036, loss: 9.4675, grad_norm: 47.1596
2023-02-03 03:51:12,590 - mmdet - INFO - Epoch [10][500/834]	lr: 2.500e-06, eta: 0:21:46, time: 0.663, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7581, stage0_pos_acc: 75.4225, stage0_loss_bbox: 0.6750, stage0_loss_iou: 1.3560, stage1_loss_cls: 0.4796, stage1_pos_acc: 85.9597, stage1_loss_bbox: 0.2655, stage1_loss_iou: 0.6807, stage2_loss_cls: 0.4518, stage2_pos_acc: 88.0427, stage2_loss_bbox: 0.2303, stage2_loss_iou: 0.5983, stage3_loss_cls: 0.4459, stage3_pos_acc: 88.3850, stage3_loss_bbox: 0.2268, stage3_loss_iou: 0.5866, stage4_loss_cls: 0.4416, stage4_pos_acc: 88.6029, stage4_loss_bbox: 0.2250, stage4_loss_iou: 0.5832, stage5_loss_cls: 0.4423, stage5_pos_acc: 88.4423, stage5_loss_bbox: 0.2240, stage5_loss_iou: 0.5805, loss: 9.2513, grad_norm: 44.3196
2023-02-03 03:51:45,662 - mmdet - INFO - Epoch [10][550/834]	lr: 2.500e-06, eta: 0:21:14, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7852, stage0_pos_acc: 74.8670, stage0_loss_bbox: 0.6744, stage0_loss_iou: 1.3229, stage1_loss_cls: 0.4848, stage1_pos_acc: 85.4224, stage1_loss_bbox: 0.2744, stage1_loss_iou: 0.6740, stage2_loss_cls: 0.4569, stage2_pos_acc: 86.5662, stage2_loss_bbox: 0.2352, stage2_loss_iou: 0.5907, stage3_loss_cls: 0.4510, stage3_pos_acc: 86.8005, stage3_loss_bbox: 0.2301, stage3_loss_iou: 0.5776, stage4_loss_cls: 0.4459, stage4_pos_acc: 87.0286, stage4_loss_bbox: 0.2280, stage4_loss_iou: 0.5757, stage5_loss_cls: 0.4491, stage5_pos_acc: 87.5358, stage5_loss_bbox: 0.2256, stage5_loss_iou: 0.5725, loss: 9.2541, grad_norm: 46.2563
2023-02-03 03:52:18,828 - mmdet - INFO - Epoch [10][600/834]	lr: 2.500e-06, eta: 0:20:41, time: 0.665, data_time: 0.011, memory: 5033, stage0_loss_cls: 0.8003, stage0_pos_acc: 71.4652, stage0_loss_bbox: 0.6716, stage0_loss_iou: 1.3181, stage1_loss_cls: 0.4941, stage1_pos_acc: 84.1725, stage1_loss_bbox: 0.2711, stage1_loss_iou: 0.6640, stage2_loss_cls: 0.4638, stage2_pos_acc: 86.5133, stage2_loss_bbox: 0.2381, stage2_loss_iou: 0.5875, stage3_loss_cls: 0.4566, stage3_pos_acc: 86.0714, stage3_loss_bbox: 0.2301, stage3_loss_iou: 0.5707, stage4_loss_cls: 0.4522, stage4_pos_acc: 87.1238, stage4_loss_bbox: 0.2295, stage4_loss_iou: 0.5690, stage5_loss_cls: 0.4564, stage5_pos_acc: 87.3186, stage5_loss_bbox: 0.2280, stage5_loss_iou: 0.5659, loss: 9.2669, grad_norm: 46.7692
2023-02-03 03:52:51,964 - mmdet - INFO - Epoch [10][650/834]	lr: 2.500e-06, eta: 0:20:09, time: 0.663, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7601, stage0_pos_acc: 76.7382, stage0_loss_bbox: 0.6561, stage0_loss_iou: 1.3669, stage1_loss_cls: 0.4728, stage1_pos_acc: 84.0899, stage1_loss_bbox: 0.2598, stage1_loss_iou: 0.6958, stage2_loss_cls: 0.4530, stage2_pos_acc: 86.1700, stage2_loss_bbox: 0.2224, stage2_loss_iou: 0.6042, stage3_loss_cls: 0.4458, stage3_pos_acc: 86.3010, stage3_loss_bbox: 0.2174, stage3_loss_iou: 0.5907, stage4_loss_cls: 0.4448, stage4_pos_acc: 85.6379, stage4_loss_bbox: 0.2155, stage4_loss_iou: 0.5862, stage5_loss_cls: 0.4473, stage5_pos_acc: 85.6515, stage5_loss_bbox: 0.2145, stage5_loss_iou: 0.5836, loss: 9.2371, grad_norm: 41.0777
2023-02-03 03:53:25,450 - mmdet - INFO - Epoch [10][700/834]	lr: 2.500e-06, eta: 0:19:36, time: 0.670, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7501, stage0_pos_acc: 76.4651, stage0_loss_bbox: 0.6588, stage0_loss_iou: 1.3705, stage1_loss_cls: 0.4750, stage1_pos_acc: 85.3318, stage1_loss_bbox: 0.2703, stage1_loss_iou: 0.7132, stage2_loss_cls: 0.4488, stage2_pos_acc: 87.1279, stage2_loss_bbox: 0.2369, stage2_loss_iou: 0.6285, stage3_loss_cls: 0.4435, stage3_pos_acc: 87.0639, stage3_loss_bbox: 0.2309, stage3_loss_iou: 0.6174, stage4_loss_cls: 0.4393, stage4_pos_acc: 87.0522, stage4_loss_bbox: 0.2301, stage4_loss_iou: 0.6139, stage5_loss_cls: 0.4392, stage5_pos_acc: 87.1911, stage5_loss_bbox: 0.2293, stage5_loss_iou: 0.6139, loss: 9.4094, grad_norm: 42.8734
2023-02-03 03:53:58,529 - mmdet - INFO - Epoch [10][750/834]	lr: 2.500e-06, eta: 0:19:04, time: 0.662, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7703, stage0_pos_acc: 77.1672, stage0_loss_bbox: 0.6574, stage0_loss_iou: 1.3563, stage1_loss_cls: 0.4845, stage1_pos_acc: 87.4946, stage1_loss_bbox: 0.2577, stage1_loss_iou: 0.6863, stage2_loss_cls: 0.4562, stage2_pos_acc: 87.9372, stage2_loss_bbox: 0.2215, stage2_loss_iou: 0.5960, stage3_loss_cls: 0.4490, stage3_pos_acc: 88.8844, stage3_loss_bbox: 0.2169, stage3_loss_iou: 0.5841, stage4_loss_cls: 0.4470, stage4_pos_acc: 89.4807, stage4_loss_bbox: 0.2142, stage4_loss_iou: 0.5803, stage5_loss_cls: 0.4462, stage5_pos_acc: 89.5174, stage5_loss_bbox: 0.2134, stage5_loss_iou: 0.5785, loss: 9.2159, grad_norm: 43.7117
2023-02-03 03:54:31,544 - mmdet - INFO - Epoch [10][800/834]	lr: 2.500e-06, eta: 0:18:31, time: 0.660, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7769, stage0_pos_acc: 74.2956, stage0_loss_bbox: 0.6625, stage0_loss_iou: 1.3493, stage1_loss_cls: 0.4885, stage1_pos_acc: 82.4859, stage1_loss_bbox: 0.2625, stage1_loss_iou: 0.6909, stage2_loss_cls: 0.4564, stage2_pos_acc: 85.4834, stage2_loss_bbox: 0.2316, stage2_loss_iou: 0.6031, stage3_loss_cls: 0.4478, stage3_pos_acc: 85.8221, stage3_loss_bbox: 0.2259, stage3_loss_iou: 0.5871, stage4_loss_cls: 0.4462, stage4_pos_acc: 86.1728, stage4_loss_bbox: 0.2215, stage4_loss_iou: 0.5818, stage5_loss_cls: 0.4473, stage5_pos_acc: 86.0456, stage5_loss_bbox: 0.2205, stage5_loss_iou: 0.5806, loss: 9.2806, grad_norm: 44.4224
2023-02-03 03:54:54,541 - mmdet - INFO - Saving checkpoint at 10 epochs
2023-02-03 03:55:25,710 - mmdet - INFO - Evaluating bbox...
2023-02-03 03:55:38,838 - mmdet - INFO - Exp name: sparse_rcnn_r50_fpn_1x_coco_comparison.py
2023-02-03 03:55:38,839 - mmdet - INFO - Epoch(val) [10][186]	bbox_mAP: 0.2330, bbox_mAP_50: 0.4570, bbox_mAP_75: 0.2090, bbox_mAP_s: 0.0070, bbox_mAP_m: 0.1400, bbox_mAP_l: 0.2390, bbox_mAP_copypaste: 0.233 0.457 0.209 0.007 0.140 0.239
2023-02-03 03:56:25,168 - mmdet - INFO - Epoch [11][50/834]	lr: 2.500e-06, eta: 0:17:35, time: 0.926, data_time: 0.270, memory: 5033, stage0_loss_cls: 0.7418, stage0_pos_acc: 76.6502, stage0_loss_bbox: 0.6494, stage0_loss_iou: 1.3530, stage1_loss_cls: 0.4595, stage1_pos_acc: 85.4893, stage1_loss_bbox: 0.2626, stage1_loss_iou: 0.6881, stage2_loss_cls: 0.4334, stage2_pos_acc: 87.8113, stage2_loss_bbox: 0.2276, stage2_loss_iou: 0.5991, stage3_loss_cls: 0.4249, stage3_pos_acc: 88.7848, stage3_loss_bbox: 0.2230, stage3_loss_iou: 0.5867, stage4_loss_cls: 0.4208, stage4_pos_acc: 88.3000, stage4_loss_bbox: 0.2220, stage4_loss_iou: 0.5840, stage5_loss_cls: 0.4217, stage5_pos_acc: 88.3806, stage5_loss_bbox: 0.2212, stage5_loss_iou: 0.5827, loss: 9.1015, grad_norm: 43.6567
2023-02-03 03:56:58,535 - mmdet - INFO - Epoch [11][100/834]	lr: 2.500e-06, eta: 0:17:02, time: 0.667, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7618, stage0_pos_acc: 76.2527, stage0_loss_bbox: 0.6552, stage0_loss_iou: 1.3508, stage1_loss_cls: 0.4757, stage1_pos_acc: 84.2477, stage1_loss_bbox: 0.2666, stage1_loss_iou: 0.6898, stage2_loss_cls: 0.4475, stage2_pos_acc: 86.1907, stage2_loss_bbox: 0.2307, stage2_loss_iou: 0.6016, stage3_loss_cls: 0.4409, stage3_pos_acc: 86.5786, stage3_loss_bbox: 0.2258, stage3_loss_iou: 0.5900, stage4_loss_cls: 0.4390, stage4_pos_acc: 86.8453, stage4_loss_bbox: 0.2229, stage4_loss_iou: 0.5843, stage5_loss_cls: 0.4393, stage5_pos_acc: 86.9207, stage5_loss_bbox: 0.2223, stage5_loss_iou: 0.5817, loss: 9.2257, grad_norm: 43.1960
2023-02-03 03:57:31,803 - mmdet - INFO - Epoch [11][150/834]	lr: 2.500e-06, eta: 0:16:30, time: 0.665, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7669, stage0_pos_acc: 74.7873, stage0_loss_bbox: 0.6562, stage0_loss_iou: 1.3398, stage1_loss_cls: 0.4825, stage1_pos_acc: 84.3179, stage1_loss_bbox: 0.2585, stage1_loss_iou: 0.6723, stage2_loss_cls: 0.4542, stage2_pos_acc: 85.9451, stage2_loss_bbox: 0.2249, stage2_loss_iou: 0.5907, stage3_loss_cls: 0.4480, stage3_pos_acc: 86.1867, stage3_loss_bbox: 0.2190, stage3_loss_iou: 0.5780, stage4_loss_cls: 0.4447, stage4_pos_acc: 86.3241, stage4_loss_bbox: 0.2179, stage4_loss_iou: 0.5752, stage5_loss_cls: 0.4458, stage5_pos_acc: 86.3124, stage5_loss_bbox: 0.2163, stage5_loss_iou: 0.5730, loss: 9.1636, grad_norm: 47.4156
2023-02-03 03:58:04,926 - mmdet - INFO - Epoch [11][200/834]	lr: 2.500e-06, eta: 0:15:57, time: 0.662, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7456, stage0_pos_acc: 78.3846, stage0_loss_bbox: 0.6619, stage0_loss_iou: 1.3549, stage1_loss_cls: 0.4537, stage1_pos_acc: 87.3439, stage1_loss_bbox: 0.2609, stage1_loss_iou: 0.6831, stage2_loss_cls: 0.4228, stage2_pos_acc: 89.8590, stage2_loss_bbox: 0.2281, stage2_loss_iou: 0.6016, stage3_loss_cls: 0.4187, stage3_pos_acc: 89.3806, stage3_loss_bbox: 0.2202, stage3_loss_iou: 0.5855, stage4_loss_cls: 0.4172, stage4_pos_acc: 89.6750, stage4_loss_bbox: 0.2187, stage4_loss_iou: 0.5810, stage5_loss_cls: 0.4161, stage5_pos_acc: 89.9154, stage5_loss_bbox: 0.2183, stage5_loss_iou: 0.5801, loss: 9.0684, grad_norm: 44.6919
2023-02-03 03:58:37,941 - mmdet - INFO - Epoch [11][250/834]	lr: 2.500e-06, eta: 0:15:25, time: 0.661, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7656, stage0_pos_acc: 78.7973, stage0_loss_bbox: 0.6557, stage0_loss_iou: 1.3542, stage1_loss_cls: 0.4724, stage1_pos_acc: 84.7892, stage1_loss_bbox: 0.2552, stage1_loss_iou: 0.6764, stage2_loss_cls: 0.4476, stage2_pos_acc: 87.2504, stage2_loss_bbox: 0.2229, stage2_loss_iou: 0.5907, stage3_loss_cls: 0.4369, stage3_pos_acc: 87.1611, stage3_loss_bbox: 0.2175, stage3_loss_iou: 0.5802, stage4_loss_cls: 0.4351, stage4_pos_acc: 87.4018, stage4_loss_bbox: 0.2160, stage4_loss_iou: 0.5755, stage5_loss_cls: 0.4369, stage5_pos_acc: 87.2525, stage5_loss_bbox: 0.2150, stage5_loss_iou: 0.5729, loss: 9.1269, grad_norm: 43.2721
2023-02-03 03:59:11,097 - mmdet - INFO - Epoch [11][300/834]	lr: 2.500e-06, eta: 0:14:52, time: 0.663, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7605, stage0_pos_acc: 76.1827, stage0_loss_bbox: 0.6599, stage0_loss_iou: 1.3433, stage1_loss_cls: 0.4724, stage1_pos_acc: 84.9385, stage1_loss_bbox: 0.2717, stage1_loss_iou: 0.6884, stage2_loss_cls: 0.4499, stage2_pos_acc: 86.6424, stage2_loss_bbox: 0.2359, stage2_loss_iou: 0.6041, stage3_loss_cls: 0.4450, stage3_pos_acc: 86.8693, stage3_loss_bbox: 0.2277, stage3_loss_iou: 0.5905, stage4_loss_cls: 0.4414, stage4_pos_acc: 87.6011, stage4_loss_bbox: 0.2253, stage4_loss_iou: 0.5864, stage5_loss_cls: 0.4398, stage5_pos_acc: 87.3178, stage5_loss_bbox: 0.2265, stage5_loss_iou: 0.5866, loss: 9.2555, grad_norm: 43.9612
2023-02-03 03:59:44,370 - mmdet - INFO - Epoch [11][350/834]	lr: 2.500e-06, eta: 0:14:20, time: 0.665, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7639, stage0_pos_acc: 75.5933, stage0_loss_bbox: 0.6553, stage0_loss_iou: 1.3472, stage1_loss_cls: 0.4851, stage1_pos_acc: 85.6596, stage1_loss_bbox: 0.2725, stage1_loss_iou: 0.6938, stage2_loss_cls: 0.4609, stage2_pos_acc: 87.6476, stage2_loss_bbox: 0.2380, stage2_loss_iou: 0.6077, stage3_loss_cls: 0.4543, stage3_pos_acc: 87.8856, stage3_loss_bbox: 0.2334, stage3_loss_iou: 0.5979, stage4_loss_cls: 0.4526, stage4_pos_acc: 87.8449, stage4_loss_bbox: 0.2326, stage4_loss_iou: 0.5943, stage5_loss_cls: 0.4513, stage5_pos_acc: 87.8988, stage5_loss_bbox: 0.2313, stage5_loss_iou: 0.5922, loss: 9.3641, grad_norm: 47.7399
2023-02-03 04:00:17,416 - mmdet - INFO - Epoch [11][400/834]	lr: 2.500e-06, eta: 0:13:47, time: 0.661, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7868, stage0_pos_acc: 73.6829, stage0_loss_bbox: 0.6646, stage0_loss_iou: 1.3552, stage1_loss_cls: 0.4793, stage1_pos_acc: 83.8659, stage1_loss_bbox: 0.2628, stage1_loss_iou: 0.6807, stage2_loss_cls: 0.4539, stage2_pos_acc: 85.9373, stage2_loss_bbox: 0.2298, stage2_loss_iou: 0.5975, stage3_loss_cls: 0.4478, stage3_pos_acc: 86.6636, stage3_loss_bbox: 0.2239, stage3_loss_iou: 0.5809, stage4_loss_cls: 0.4448, stage4_pos_acc: 86.6568, stage4_loss_bbox: 0.2223, stage4_loss_iou: 0.5767, stage5_loss_cls: 0.4441, stage5_pos_acc: 86.3315, stage5_loss_bbox: 0.2220, stage5_loss_iou: 0.5768, loss: 9.2498, grad_norm: 47.0409
2023-02-03 04:00:50,393 - mmdet - INFO - Epoch [11][450/834]	lr: 2.500e-06, eta: 0:13:14, time: 0.659, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7882, stage0_pos_acc: 75.4478, stage0_loss_bbox: 0.6681, stage0_loss_iou: 1.3389, stage1_loss_cls: 0.4816, stage1_pos_acc: 85.9627, stage1_loss_bbox: 0.2574, stage1_loss_iou: 0.6519, stage2_loss_cls: 0.4527, stage2_pos_acc: 87.1656, stage2_loss_bbox: 0.2258, stage2_loss_iou: 0.5726, stage3_loss_cls: 0.4426, stage3_pos_acc: 87.8090, stage3_loss_bbox: 0.2185, stage3_loss_iou: 0.5576, stage4_loss_cls: 0.4389, stage4_pos_acc: 88.4601, stage4_loss_bbox: 0.2169, stage4_loss_iou: 0.5542, stage5_loss_cls: 0.4359, stage5_pos_acc: 88.3477, stage5_loss_bbox: 0.2168, stage5_loss_iou: 0.5531, loss: 9.0717, grad_norm: 46.9071
2023-02-03 04:01:23,507 - mmdet - INFO - Epoch [11][500/834]	lr: 2.500e-06, eta: 0:12:42, time: 0.663, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7653, stage0_pos_acc: 72.7091, stage0_loss_bbox: 0.6640, stage0_loss_iou: 1.3616, stage1_loss_cls: 0.4713, stage1_pos_acc: 85.1215, stage1_loss_bbox: 0.2640, stage1_loss_iou: 0.6909, stage2_loss_cls: 0.4460, stage2_pos_acc: 87.7114, stage2_loss_bbox: 0.2296, stage2_loss_iou: 0.6057, stage3_loss_cls: 0.4384, stage3_pos_acc: 88.2026, stage3_loss_bbox: 0.2253, stage3_loss_iou: 0.5959, stage4_loss_cls: 0.4354, stage4_pos_acc: 88.3192, stage4_loss_bbox: 0.2238, stage4_loss_iou: 0.5928, stage5_loss_cls: 0.4341, stage5_pos_acc: 88.5603, stage5_loss_bbox: 0.2231, stage5_loss_iou: 0.5929, loss: 9.2600, grad_norm: 46.7844
2023-02-03 04:01:56,480 - mmdet - INFO - Epoch [11][550/834]	lr: 2.500e-06, eta: 0:12:09, time: 0.659, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7596, stage0_pos_acc: 75.7453, stage0_loss_bbox: 0.6524, stage0_loss_iou: 1.3171, stage1_loss_cls: 0.4831, stage1_pos_acc: 83.5780, stage1_loss_bbox: 0.2703, stage1_loss_iou: 0.6746, stage2_loss_cls: 0.4524, stage2_pos_acc: 85.9782, stage2_loss_bbox: 0.2395, stage2_loss_iou: 0.5962, stage3_loss_cls: 0.4421, stage3_pos_acc: 86.6894, stage3_loss_bbox: 0.2325, stage3_loss_iou: 0.5833, stage4_loss_cls: 0.4379, stage4_pos_acc: 87.1054, stage4_loss_bbox: 0.2313, stage4_loss_iou: 0.5809, stage5_loss_cls: 0.4370, stage5_pos_acc: 87.0968, stage5_loss_bbox: 0.2314, stage5_loss_iou: 0.5798, loss: 9.2016, grad_norm: 47.0268
2023-02-03 04:02:29,776 - mmdet - INFO - Epoch [11][600/834]	lr: 2.500e-06, eta: 0:11:37, time: 0.666, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7739, stage0_pos_acc: 74.1613, stage0_loss_bbox: 0.6696, stage0_loss_iou: 1.3368, stage1_loss_cls: 0.4894, stage1_pos_acc: 84.5074, stage1_loss_bbox: 0.2723, stage1_loss_iou: 0.6918, stage2_loss_cls: 0.4568, stage2_pos_acc: 86.0487, stage2_loss_bbox: 0.2372, stage2_loss_iou: 0.6070, stage3_loss_cls: 0.4463, stage3_pos_acc: 87.1522, stage3_loss_bbox: 0.2346, stage3_loss_iou: 0.5963, stage4_loss_cls: 0.4456, stage4_pos_acc: 86.8467, stage4_loss_bbox: 0.2336, stage4_loss_iou: 0.5949, stage5_loss_cls: 0.4451, stage5_pos_acc: 86.5467, stage5_loss_bbox: 0.2335, stage5_loss_iou: 0.5940, loss: 9.3588, grad_norm: 45.7205
2023-02-03 04:03:02,666 - mmdet - INFO - Epoch [11][650/834]	lr: 2.500e-06, eta: 0:11:04, time: 0.658, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7548, stage0_pos_acc: 77.7850, stage0_loss_bbox: 0.6503, stage0_loss_iou: 1.3191, stage1_loss_cls: 0.4717, stage1_pos_acc: 85.8941, stage1_loss_bbox: 0.2601, stage1_loss_iou: 0.6635, stage2_loss_cls: 0.4503, stage2_pos_acc: 88.2124, stage2_loss_bbox: 0.2267, stage2_loss_iou: 0.5861, stage3_loss_cls: 0.4395, stage3_pos_acc: 88.3851, stage3_loss_bbox: 0.2211, stage3_loss_iou: 0.5741, stage4_loss_cls: 0.4373, stage4_pos_acc: 88.6353, stage4_loss_bbox: 0.2194, stage4_loss_iou: 0.5715, stage5_loss_cls: 0.4367, stage5_pos_acc: 88.5385, stage5_loss_bbox: 0.2187, stage5_loss_iou: 0.5692, loss: 9.0699, grad_norm: 45.3423
2023-02-03 04:03:35,714 - mmdet - INFO - Epoch [11][700/834]	lr: 2.500e-06, eta: 0:10:31, time: 0.661, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7818, stage0_pos_acc: 75.4590, stage0_loss_bbox: 0.6515, stage0_loss_iou: 1.3301, stage1_loss_cls: 0.4895, stage1_pos_acc: 84.5728, stage1_loss_bbox: 0.2650, stage1_loss_iou: 0.6669, stage2_loss_cls: 0.4550, stage2_pos_acc: 86.3052, stage2_loss_bbox: 0.2319, stage2_loss_iou: 0.5905, stage3_loss_cls: 0.4469, stage3_pos_acc: 85.7893, stage3_loss_bbox: 0.2271, stage3_loss_iou: 0.5769, stage4_loss_cls: 0.4428, stage4_pos_acc: 85.6818, stage4_loss_bbox: 0.2248, stage4_loss_iou: 0.5747, stage5_loss_cls: 0.4422, stage5_pos_acc: 86.5310, stage5_loss_bbox: 0.2253, stage5_loss_iou: 0.5749, loss: 9.1977, grad_norm: 44.7011
2023-02-03 04:04:09,273 - mmdet - INFO - Epoch [11][750/834]	lr: 2.500e-06, eta: 0:09:59, time: 0.671, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7502, stage0_pos_acc: 77.9003, stage0_loss_bbox: 0.6562, stage0_loss_iou: 1.3710, stage1_loss_cls: 0.4652, stage1_pos_acc: 85.9657, stage1_loss_bbox: 0.2544, stage1_loss_iou: 0.6825, stage2_loss_cls: 0.4325, stage2_pos_acc: 87.4718, stage2_loss_bbox: 0.2216, stage2_loss_iou: 0.5988, stage3_loss_cls: 0.4259, stage3_pos_acc: 88.1700, stage3_loss_bbox: 0.2183, stage3_loss_iou: 0.5873, stage4_loss_cls: 0.4224, stage4_pos_acc: 88.6747, stage4_loss_bbox: 0.2168, stage4_loss_iou: 0.5805, stage5_loss_cls: 0.4218, stage5_pos_acc: 88.6126, stage5_loss_bbox: 0.2162, stage5_loss_iou: 0.5796, loss: 9.1012, grad_norm: 45.4577
2023-02-03 04:04:42,194 - mmdet - INFO - Epoch [11][800/834]	lr: 2.500e-06, eta: 0:09:26, time: 0.659, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.8024, stage0_pos_acc: 74.6666, stage0_loss_bbox: 0.6772, stage0_loss_iou: 1.3479, stage1_loss_cls: 0.4905, stage1_pos_acc: 84.5056, stage1_loss_bbox: 0.2724, stage1_loss_iou: 0.6821, stage2_loss_cls: 0.4631, stage2_pos_acc: 87.2310, stage2_loss_bbox: 0.2366, stage2_loss_iou: 0.5925, stage3_loss_cls: 0.4549, stage3_pos_acc: 88.5201, stage3_loss_bbox: 0.2301, stage3_loss_iou: 0.5764, stage4_loss_cls: 0.4495, stage4_pos_acc: 88.1975, stage4_loss_bbox: 0.2289, stage4_loss_iou: 0.5730, stage5_loss_cls: 0.4509, stage5_pos_acc: 88.4510, stage5_loss_bbox: 0.2273, stage5_loss_iou: 0.5706, loss: 9.3263, grad_norm: 46.2787
2023-02-03 04:05:33,831 - mmdet - INFO - Evaluating bbox...
2023-02-03 04:05:47,346 - mmdet - INFO - Exp name: sparse_rcnn_r50_fpn_1x_coco_comparison.py
2023-02-03 04:05:47,347 - mmdet - INFO - Epoch(val) [11][186]	bbox_mAP: 0.2430, bbox_mAP_50: 0.4650, bbox_mAP_75: 0.2140, bbox_mAP_s: 0.0110, bbox_mAP_m: 0.1490, bbox_mAP_l: 0.2480, bbox_mAP_copypaste: 0.243 0.465 0.214 0.011 0.149 0.248
2023-02-03 04:06:33,533 - mmdet - INFO - Epoch [12][50/834]	lr: 2.500e-07, eta: 0:08:31, time: 0.923, data_time: 0.270, memory: 5033, stage0_loss_cls: 0.7530, stage0_pos_acc: 76.5862, stage0_loss_bbox: 0.6586, stage0_loss_iou: 1.3839, stage1_loss_cls: 0.4622, stage1_pos_acc: 85.9290, stage1_loss_bbox: 0.2497, stage1_loss_iou: 0.6859, stage2_loss_cls: 0.4334, stage2_pos_acc: 87.3665, stage2_loss_bbox: 0.2179, stage2_loss_iou: 0.6008, stage3_loss_cls: 0.4260, stage3_pos_acc: 87.5429, stage3_loss_bbox: 0.2137, stage3_loss_iou: 0.5879, stage4_loss_cls: 0.4227, stage4_pos_acc: 87.7610, stage4_loss_bbox: 0.2122, stage4_loss_iou: 0.5840, stage5_loss_cls: 0.4214, stage5_pos_acc: 87.8132, stage5_loss_bbox: 0.2116, stage5_loss_iou: 0.5828, loss: 9.1077, grad_norm: 44.7989
2023-02-03 04:07:06,438 - mmdet - INFO - Epoch [12][100/834]	lr: 2.500e-07, eta: 0:07:58, time: 0.658, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7564, stage0_pos_acc: 74.9026, stage0_loss_bbox: 0.6649, stage0_loss_iou: 1.3503, stage1_loss_cls: 0.4749, stage1_pos_acc: 84.6676, stage1_loss_bbox: 0.2734, stage1_loss_iou: 0.6982, stage2_loss_cls: 0.4497, stage2_pos_acc: 86.8332, stage2_loss_bbox: 0.2369, stage2_loss_iou: 0.6087, stage3_loss_cls: 0.4422, stage3_pos_acc: 86.8456, stage3_loss_bbox: 0.2321, stage3_loss_iou: 0.5992, stage4_loss_cls: 0.4392, stage4_pos_acc: 86.8808, stage4_loss_bbox: 0.2306, stage4_loss_iou: 0.5971, stage5_loss_cls: 0.4380, stage5_pos_acc: 86.7194, stage5_loss_bbox: 0.2301, stage5_loss_iou: 0.5952, loss: 9.3170, grad_norm: 43.5197
2023-02-03 04:07:39,696 - mmdet - INFO - Epoch [12][150/834]	lr: 2.500e-07, eta: 0:07:26, time: 0.665, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7432, stage0_pos_acc: 78.5258, stage0_loss_bbox: 0.6513, stage0_loss_iou: 1.3561, stage1_loss_cls: 0.4615, stage1_pos_acc: 87.6186, stage1_loss_bbox: 0.2613, stage1_loss_iou: 0.6948, stage2_loss_cls: 0.4358, stage2_pos_acc: 89.6471, stage2_loss_bbox: 0.2319, stage2_loss_iou: 0.6136, stage3_loss_cls: 0.4294, stage3_pos_acc: 90.4253, stage3_loss_bbox: 0.2245, stage3_loss_iou: 0.5972, stage4_loss_cls: 0.4245, stage4_pos_acc: 90.3033, stage4_loss_bbox: 0.2234, stage4_loss_iou: 0.5960, stage5_loss_cls: 0.4266, stage5_pos_acc: 90.2715, stage5_loss_bbox: 0.2218, stage5_loss_iou: 0.5932, loss: 9.1862, grad_norm: 44.2196
2023-02-03 04:08:12,740 - mmdet - INFO - Epoch [12][200/834]	lr: 2.500e-07, eta: 0:06:53, time: 0.661, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7638, stage0_pos_acc: 78.3683, stage0_loss_bbox: 0.6517, stage0_loss_iou: 1.3167, stage1_loss_cls: 0.4652, stage1_pos_acc: 87.5493, stage1_loss_bbox: 0.2606, stage1_loss_iou: 0.6560, stage2_loss_cls: 0.4361, stage2_pos_acc: 88.2429, stage2_loss_bbox: 0.2293, stage2_loss_iou: 0.5771, stage3_loss_cls: 0.4280, stage3_pos_acc: 88.4092, stage3_loss_bbox: 0.2250, stage3_loss_iou: 0.5651, stage4_loss_cls: 0.4234, stage4_pos_acc: 88.8828, stage4_loss_bbox: 0.2242, stage4_loss_iou: 0.5630, stage5_loss_cls: 0.4230, stage5_pos_acc: 88.3724, stage5_loss_bbox: 0.2245, stage5_loss_iou: 0.5629, loss: 8.9955, grad_norm: 46.4452
2023-02-03 04:08:45,730 - mmdet - INFO - Epoch [12][250/834]	lr: 2.500e-07, eta: 0:06:20, time: 0.660, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7558, stage0_pos_acc: 76.1828, stage0_loss_bbox: 0.6475, stage0_loss_iou: 1.3290, stage1_loss_cls: 0.4657, stage1_pos_acc: 85.8172, stage1_loss_bbox: 0.2622, stage1_loss_iou: 0.6731, stage2_loss_cls: 0.4372, stage2_pos_acc: 87.6948, stage2_loss_bbox: 0.2309, stage2_loss_iou: 0.5924, stage3_loss_cls: 0.4329, stage3_pos_acc: 87.8775, stage3_loss_bbox: 0.2233, stage3_loss_iou: 0.5794, stage4_loss_cls: 0.4287, stage4_pos_acc: 87.9274, stage4_loss_bbox: 0.2215, stage4_loss_iou: 0.5748, stage5_loss_cls: 0.4289, stage5_pos_acc: 87.9835, stage5_loss_bbox: 0.2202, stage5_loss_iou: 0.5735, loss: 9.0770, grad_norm: 43.6149
2023-02-03 04:09:18,725 - mmdet - INFO - Epoch [12][300/834]	lr: 2.500e-07, eta: 0:05:48, time: 0.660, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7663, stage0_pos_acc: 75.1206, stage0_loss_bbox: 0.6484, stage0_loss_iou: 1.3490, stage1_loss_cls: 0.4652, stage1_pos_acc: 86.7054, stage1_loss_bbox: 0.2550, stage1_loss_iou: 0.6798, stage2_loss_cls: 0.4335, stage2_pos_acc: 88.3293, stage2_loss_bbox: 0.2211, stage2_loss_iou: 0.5951, stage3_loss_cls: 0.4253, stage3_pos_acc: 88.8557, stage3_loss_bbox: 0.2161, stage3_loss_iou: 0.5830, stage4_loss_cls: 0.4194, stage4_pos_acc: 89.4064, stage4_loss_bbox: 0.2153, stage4_loss_iou: 0.5818, stage5_loss_cls: 0.4206, stage5_pos_acc: 89.1106, stage5_loss_bbox: 0.2143, stage5_loss_iou: 0.5800, loss: 9.0690, grad_norm: 43.6842
2023-02-03 04:09:52,056 - mmdet - INFO - Epoch [12][350/834]	lr: 2.500e-07, eta: 0:05:15, time: 0.667, data_time: 0.009, memory: 5033, stage0_loss_cls: 0.7480, stage0_pos_acc: 74.7304, stage0_loss_bbox: 0.6421, stage0_loss_iou: 1.3281, stage1_loss_cls: 0.4625, stage1_pos_acc: 84.6580, stage1_loss_bbox: 0.2600, stage1_loss_iou: 0.6687, stage2_loss_cls: 0.4364, stage2_pos_acc: 85.7544, stage2_loss_bbox: 0.2263, stage2_loss_iou: 0.5886, stage3_loss_cls: 0.4308, stage3_pos_acc: 86.4362, stage3_loss_bbox: 0.2207, stage3_loss_iou: 0.5733, stage4_loss_cls: 0.4248, stage4_pos_acc: 85.9043, stage4_loss_bbox: 0.2185, stage4_loss_iou: 0.5695, stage5_loss_cls: 0.4264, stage5_pos_acc: 86.7746, stage5_loss_bbox: 0.2174, stage5_loss_iou: 0.5666, loss: 9.0087, grad_norm: 42.1008
2023-02-03 04:10:25,473 - mmdet - INFO - Epoch [12][400/834]	lr: 2.500e-07, eta: 0:04:43, time: 0.668, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7469, stage0_pos_acc: 77.4029, stage0_loss_bbox: 0.6562, stage0_loss_iou: 1.3280, stage1_loss_cls: 0.4622, stage1_pos_acc: 86.5370, stage1_loss_bbox: 0.2694, stage1_loss_iou: 0.6749, stage2_loss_cls: 0.4367, stage2_pos_acc: 87.5852, stage2_loss_bbox: 0.2407, stage2_loss_iou: 0.6028, stage3_loss_cls: 0.4272, stage3_pos_acc: 88.5828, stage3_loss_bbox: 0.2356, stage3_loss_iou: 0.5923, stage4_loss_cls: 0.4273, stage4_pos_acc: 88.3506, stage4_loss_bbox: 0.2340, stage4_loss_iou: 0.5885, stage5_loss_cls: 0.4279, stage5_pos_acc: 88.3528, stage5_loss_bbox: 0.2335, stage5_loss_iou: 0.5877, loss: 9.1717, grad_norm: 44.6876
2023-02-03 04:10:58,684 - mmdet - INFO - Epoch [12][450/834]	lr: 2.500e-07, eta: 0:04:10, time: 0.665, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7697, stage0_pos_acc: 72.2379, stage0_loss_bbox: 0.6504, stage0_loss_iou: 1.3345, stage1_loss_cls: 0.4754, stage1_pos_acc: 83.0242, stage1_loss_bbox: 0.2662, stage1_loss_iou: 0.6879, stage2_loss_cls: 0.4445, stage2_pos_acc: 85.9652, stage2_loss_bbox: 0.2341, stage2_loss_iou: 0.6066, stage3_loss_cls: 0.4385, stage3_pos_acc: 86.6952, stage3_loss_bbox: 0.2270, stage3_loss_iou: 0.5954, stage4_loss_cls: 0.4323, stage4_pos_acc: 87.1062, stage4_loss_bbox: 0.2249, stage4_loss_iou: 0.5919, stage5_loss_cls: 0.4333, stage5_pos_acc: 86.9007, stage5_loss_bbox: 0.2236, stage5_loss_iou: 0.5896, loss: 9.2258, grad_norm: 42.6260
2023-02-03 04:11:31,737 - mmdet - INFO - Epoch [12][500/834]	lr: 2.500e-07, eta: 0:03:37, time: 0.661, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7836, stage0_pos_acc: 77.0704, stage0_loss_bbox: 0.6497, stage0_loss_iou: 1.3356, stage1_loss_cls: 0.4646, stage1_pos_acc: 87.7188, stage1_loss_bbox: 0.2547, stage1_loss_iou: 0.6622, stage2_loss_cls: 0.4300, stage2_pos_acc: 89.2327, stage2_loss_bbox: 0.2203, stage2_loss_iou: 0.5765, stage3_loss_cls: 0.4230, stage3_pos_acc: 89.7281, stage3_loss_bbox: 0.2123, stage3_loss_iou: 0.5633, stage4_loss_cls: 0.4190, stage4_pos_acc: 89.9550, stage4_loss_bbox: 0.2101, stage4_loss_iou: 0.5588, stage5_loss_cls: 0.4182, stage5_pos_acc: 89.9869, stage5_loss_bbox: 0.2095, stage5_loss_iou: 0.5571, loss: 8.9487, grad_norm: 45.6958
2023-02-03 04:12:05,158 - mmdet - INFO - Epoch [12][550/834]	lr: 2.500e-07, eta: 0:03:05, time: 0.668, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7754, stage0_pos_acc: 75.3730, stage0_loss_bbox: 0.6516, stage0_loss_iou: 1.3363, stage1_loss_cls: 0.4677, stage1_pos_acc: 85.4037, stage1_loss_bbox: 0.2560, stage1_loss_iou: 0.6659, stage2_loss_cls: 0.4424, stage2_pos_acc: 86.7740, stage2_loss_bbox: 0.2238, stage2_loss_iou: 0.5899, stage3_loss_cls: 0.4350, stage3_pos_acc: 87.1297, stage3_loss_bbox: 0.2198, stage3_loss_iou: 0.5825, stage4_loss_cls: 0.4314, stage4_pos_acc: 87.0478, stage4_loss_bbox: 0.2196, stage4_loss_iou: 0.5807, stage5_loss_cls: 0.4341, stage5_pos_acc: 87.3971, stage5_loss_bbox: 0.2186, stage5_loss_iou: 0.5783, loss: 9.1088, grad_norm: 45.2468
2023-02-03 04:12:38,244 - mmdet - INFO - Epoch [12][600/834]	lr: 2.500e-07, eta: 0:02:32, time: 0.662, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7693, stage0_pos_acc: 72.0649, stage0_loss_bbox: 0.6370, stage0_loss_iou: 1.3099, stage1_loss_cls: 0.4651, stage1_pos_acc: 83.6512, stage1_loss_bbox: 0.2557, stage1_loss_iou: 0.6472, stage2_loss_cls: 0.4358, stage2_pos_acc: 86.3942, stage2_loss_bbox: 0.2209, stage2_loss_iou: 0.5650, stage3_loss_cls: 0.4244, stage3_pos_acc: 87.4702, stage3_loss_bbox: 0.2160, stage3_loss_iou: 0.5549, stage4_loss_cls: 0.4218, stage4_pos_acc: 87.6104, stage4_loss_bbox: 0.2136, stage4_loss_iou: 0.5519, stage5_loss_cls: 0.4205, stage5_pos_acc: 87.5772, stage5_loss_bbox: 0.2133, stage5_loss_iou: 0.5524, loss: 8.8747, grad_norm: 45.4877
2023-02-03 04:13:11,401 - mmdet - INFO - Epoch [12][650/834]	lr: 2.500e-07, eta: 0:02:00, time: 0.663, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7748, stage0_pos_acc: 74.7564, stage0_loss_bbox: 0.6483, stage0_loss_iou: 1.3186, stage1_loss_cls: 0.4735, stage1_pos_acc: 85.0879, stage1_loss_bbox: 0.2596, stage1_loss_iou: 0.6564, stage2_loss_cls: 0.4453, stage2_pos_acc: 86.3856, stage2_loss_bbox: 0.2305, stage2_loss_iou: 0.5855, stage3_loss_cls: 0.4391, stage3_pos_acc: 86.5048, stage3_loss_bbox: 0.2241, stage3_loss_iou: 0.5723, stage4_loss_cls: 0.4346, stage4_pos_acc: 87.4180, stage4_loss_bbox: 0.2226, stage4_loss_iou: 0.5686, stage5_loss_cls: 0.4358, stage5_pos_acc: 86.7346, stage5_loss_bbox: 0.2208, stage5_loss_iou: 0.5650, loss: 9.0752, grad_norm: 46.0526
2023-02-03 04:13:44,583 - mmdet - INFO - Epoch [12][700/834]	lr: 2.500e-07, eta: 0:01:27, time: 0.664, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7565, stage0_pos_acc: 74.9871, stage0_loss_bbox: 0.6351, stage0_loss_iou: 1.3318, stage1_loss_cls: 0.4636, stage1_pos_acc: 84.0325, stage1_loss_bbox: 0.2548, stage1_loss_iou: 0.6670, stage2_loss_cls: 0.4367, stage2_pos_acc: 86.4494, stage2_loss_bbox: 0.2218, stage2_loss_iou: 0.5896, stage3_loss_cls: 0.4312, stage3_pos_acc: 86.0154, stage3_loss_bbox: 0.2158, stage3_loss_iou: 0.5757, stage4_loss_cls: 0.4266, stage4_pos_acc: 86.3996, stage4_loss_bbox: 0.2150, stage4_loss_iou: 0.5733, stage5_loss_cls: 0.4268, stage5_pos_acc: 86.4804, stage5_loss_bbox: 0.2134, stage5_loss_iou: 0.5700, loss: 9.0048, grad_norm: 44.0547
2023-02-03 04:14:17,733 - mmdet - INFO - Epoch [12][750/834]	lr: 2.500e-07, eta: 0:00:54, time: 0.663, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7568, stage0_pos_acc: 77.9396, stage0_loss_bbox: 0.6471, stage0_loss_iou: 1.3218, stage1_loss_cls: 0.4627, stage1_pos_acc: 86.1842, stage1_loss_bbox: 0.2550, stage1_loss_iou: 0.6585, stage2_loss_cls: 0.4316, stage2_pos_acc: 87.9056, stage2_loss_bbox: 0.2258, stage2_loss_iou: 0.5803, stage3_loss_cls: 0.4262, stage3_pos_acc: 88.3560, stage3_loss_bbox: 0.2210, stage3_loss_iou: 0.5669, stage4_loss_cls: 0.4232, stage4_pos_acc: 88.6491, stage4_loss_bbox: 0.2191, stage4_loss_iou: 0.5624, stage5_loss_cls: 0.4227, stage5_pos_acc: 88.3917, stage5_loss_bbox: 0.2184, stage5_loss_iou: 0.5606, loss: 8.9603, grad_norm: 44.8296
2023-02-03 04:14:51,173 - mmdet - INFO - Epoch [12][800/834]	lr: 2.500e-07, eta: 0:00:22, time: 0.669, data_time: 0.010, memory: 5033, stage0_loss_cls: 0.7569, stage0_pos_acc: 75.5626, stage0_loss_bbox: 0.6425, stage0_loss_iou: 1.3345, stage1_loss_cls: 0.4453, stage1_pos_acc: 86.9289, stage1_loss_bbox: 0.2541, stage1_loss_iou: 0.6715, stage2_loss_cls: 0.4194, stage2_pos_acc: 88.0125, stage2_loss_bbox: 0.2217, stage2_loss_iou: 0.5879, stage3_loss_cls: 0.4085, stage3_pos_acc: 88.1755, stage3_loss_bbox: 0.2184, stage3_loss_iou: 0.5816, stage4_loss_cls: 0.4026, stage4_pos_acc: 88.3596, stage4_loss_bbox: 0.2189, stage4_loss_iou: 0.5810, stage5_loss_cls: 0.4047, stage5_pos_acc: 88.6544, stage5_loss_bbox: 0.2181, stage5_loss_iou: 0.5801, loss: 8.9475, grad_norm: 44.2327
2023-02-03 04:15:14,209 - mmdet - INFO - Saving checkpoint at 12 epochs
2023-02-03 04:15:45,366 - mmdet - INFO - Evaluating bbox...
2023-02-03 04:15:59,023 - mmdet - INFO - Exp name: sparse_rcnn_r50_fpn_1x_coco_comparison.py
2023-02-03 04:15:59,024 - mmdet - INFO - Epoch(val) [12][186]	bbox_mAP: 0.2430, bbox_mAP_50: 0.4730, bbox_mAP_75: 0.2150, bbox_mAP_s: 0.0080, bbox_mAP_m: 0.1430, bbox_mAP_l: 0.2510, bbox_mAP_copypaste: 0.243 0.473 0.215 0.008 0.143 0.251
